{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b4bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "110af1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import os\n",
    "\n",
    "def image_to_minimal_line_drawing(image_path, output_path):\n",
    "    \"\"\"\n",
    "    Converts an image of a person to a highly minimal, thick-lined drawing \n",
    "    by adjusting blur, Canny thresholds, and applying dilation.\n",
    "    \"\"\"\n",
    "    # 1. Initialize MediaPipe Selfie Segmentation\n",
    "    mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "    \n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=0) as segmentor:\n",
    "        \n",
    "        # Read the image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Error: Could not read image at {image_path}\")\n",
    "            return\n",
    "\n",
    "        # --- Background Removal (Same as before) ---\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_rgb.flags.writeable = False\n",
    "        results = segmentor.process(img_rgb)\n",
    "        img_rgb.flags.writeable = True\n",
    "\n",
    "        if results.segmentation_mask is None:\n",
    "            print(\"Warning: No person detected. Processing entire image...\")\n",
    "            processed_img = img \n",
    "        else:\n",
    "            threshold = 0.1\n",
    "            binary_mask = results.segmentation_mask > threshold\n",
    "            mask_3channel = np.stack((binary_mask,) * 3, axis=-1)\n",
    "            \n",
    "            # Use white background for contrast\n",
    "            background_color = [255, 255, 255] \n",
    "            solid_background = np.full(img.shape, background_color, dtype=np.uint8)\n",
    "            processed_img = np.where(mask_3channel, img, solid_background)\n",
    "        \n",
    "        # --- Edge Detection with Minimal Settings ---\n",
    "        \n",
    "        gray_img = cv2.cvtColor(processed_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # 1. STRONGER GAUSSIAN BLUR (Increased kernel size from 5 to 11)\n",
    "        # Larger kernel simplifies features, reducing small, noisy lines.\n",
    "        blurred_img = cv2.GaussianBlur(gray_img, (11, 11), 0)\n",
    "\n",
    "        # 2. TIGHTER CANNY THRESHOLDS (Increased both high and low values)\n",
    "        # Only the absolute strongest edges will be kept.\n",
    "        low_threshold = 100\n",
    "        high_threshold = 200 \n",
    "        edges = cv2.Canny(blurred_img, low_threshold, high_threshold)\n",
    "\n",
    "        # 3. POST-PROCESSING: DILATION to Thicken and Connect Lines\n",
    "        # Dilation expands the white pixels (edges), making lines thicker and bridging small gaps.\n",
    "        kernel = np.ones((3, 3), np.uint8) # Defines the size/shape of the thickening area\n",
    "        dilated_edges = cv2.dilate(edges, kernel, iterations=1) # Increase iterations for max thickness\n",
    "\n",
    "        # Invert the edges to get black lines on a white background\n",
    "        line_drawing = cv2.bitwise_not(dilated_edges)\n",
    "\n",
    "        # Save the result\n",
    "        cv2.imwrite(output_path, line_drawing)\n",
    "        print(f\"Minimal line drawing saved to: {output_path}\")\n",
    "\n",
    "# Example Usage:\n",
    "# Note: You need an image named 'person_image.jpg' in the same directory.\n",
    "# image_to_line_drawing_with_segmentation('person_image.jpg', 'line_drawing_output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6d1ba4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760825571.235488 48011750 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760825571.237021 48018601 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal line drawing saved to: line_drawing_output_minimal.png\n"
     ]
    }
   ],
   "source": [
    "image_to_minimal_line_drawing('divija.png', 'line_drawing_output_minimal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73efa7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_minimal_line_drawing_fixed(image_path, output_path):\n",
    "    \"\"\"\n",
    "    Adjusts the blur and Canny thresholds to create minimal lines while \n",
    "    retaining essential facial features.\n",
    "    \"\"\"\n",
    "    # 1. Initialize MediaPipe Selfie Segmentation\n",
    "    mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "    \n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=0) as segmentor:\n",
    "        \n",
    "        # Read the image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Error: Could not read image at {image_path}\")\n",
    "            return\n",
    "\n",
    "        # --- Background Removal --- (Using the same method)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_rgb.flags.writeable = False\n",
    "        results = segmentor.process(img_rgb)\n",
    "        img_rgb.flags.writeable = True\n",
    "\n",
    "        if results.segmentation_mask is None:\n",
    "            processed_img = img \n",
    "        else:\n",
    "            threshold = 0.1\n",
    "            binary_mask = results.segmentation_mask > threshold\n",
    "            mask_3channel = np.stack((binary_mask,) * 3, axis=-1)\n",
    "            background_color = [255, 255, 255] \n",
    "            solid_background = np.full(img.shape, background_color, dtype=np.uint8)\n",
    "            processed_img = np.where(mask_3channel, img, solid_background)\n",
    "        \n",
    "        # --- Edge Detection with Detail Preservation ---\n",
    "        \n",
    "        gray_img = cv2.cvtColor(processed_img, cv2.COLOR_BGR2GRAY)\n",
    "        high_contrast_img = cv2.equalizeHist(gray_img)\n",
    "        # 1. MILDER GAUSSIAN BLUR (Reduced kernel size from 11 to 7 or 9)\n",
    "        # Less blur preserves finer gradients needed for facial features.\n",
    "        blurred_img = cv2.GaussianBlur(high_contrast_img, (11, 11), 0) # Try (9, 9) if features are still too faint\n",
    "\n",
    "        # 2. LOOSER CANNY THRESHOLDS (Lowered thresholds slightly)\n",
    "        # This allows slightly weaker gradients (like those for a mouth or eyebrow) to be detected.\n",
    "        # Original minimal: (100, 200) -> New balanced: (80, 180)\n",
    "        low_threshold = 20\n",
    "        high_threshold = 180 \n",
    "        edges = cv2.Canny(blurred_img, low_threshold, high_threshold)\n",
    "\n",
    "        # 3. POST-PROCESSING: DILATION to Thicken and Connect Lines\n",
    "        # Dilation is kept to maintain the desired thick, continuous look.\n",
    "        kernel = np.ones((3, 3), np.uint8) \n",
    "        dilated_edges = cv2.dilate(edges, kernel, iterations=1) \n",
    "\n",
    "        # Invert to get black lines on a white background\n",
    "        line_drawing = cv2.bitwise_not(dilated_edges)\n",
    "\n",
    "        # Save the result\n",
    "        cv2.imwrite(output_path, blurred_img)\n",
    "        print(f\"Minimal line drawing with features saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84ae5fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760825745.588507 48011750 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760825745.589607 48021489 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal line drawing with features saved to: line_drawing_output_minimal_fixed.png\n"
     ]
    }
   ],
   "source": [
    "image_to_minimal_line_drawing_fixed('divija.png', 'line_drawing_output_minimal_fixed.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57667aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760839269.756599 48084326 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760839269.758616 48142685 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final minimal line drawing saved to: minimal_line_drawing_final.png\n"
     ]
    }
   ],
   "source": [
    "def image_to_minimal_line_drawing_final(image_path, output_path):\n",
    "    \"\"\"\n",
    "    Uses CLAHE (Adaptive Contrast) to boost facial features, then applies a strong \n",
    "    blur and minimal Canny settings for a highly stylized, clean line drawing.\n",
    "    \"\"\"\n",
    "    # 1. Initialize MediaPipe Selfie Segmentation\n",
    "    mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "    \n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=0) as segmentor:\n",
    "        \n",
    "        # Read the image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Error: Could not read image at {image_path}\")\n",
    "            return\n",
    "\n",
    "        # --- Background Removal ---\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_rgb.flags.writeable = False\n",
    "        results = segmentor.process(img_rgb)\n",
    "        img_rgb.flags.writeable = True\n",
    "\n",
    "        processed_img = img \n",
    "        if results.segmentation_mask is not None:\n",
    "            threshold = 0.1\n",
    "            binary_mask = results.segmentation_mask > threshold\n",
    "            mask_3channel = np.stack((binary_mask,) * 3, axis=-1)\n",
    "            background_color = [255, 255, 255] \n",
    "            solid_background = np.full(img.shape, background_color, dtype=np.uint8)\n",
    "            processed_img = np.where(mask_3channel, img, solid_background)\n",
    "        \n",
    "        # --- Edge Detection with CLAHE Contrast Enhancement ---\n",
    "        \n",
    "        gray_img = cv2.cvtColor(processed_img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # 1. ADAPTIVE CONTRAST (CLAHE): Aggressively boost local features\n",
    "        # Increased clipLimit (e.g., to 3.0) for stronger detail enhancement \n",
    "        # (simulates your \"making details more clearer\" setting).\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "        high_contrast_img = clahe.apply(gray_img) \n",
    "        \n",
    "        # 2. STRONG GAUSSIAN BLUR (Increased kernel size to eliminate insignificant stuff)\n",
    "        # This acts like your \"washing out insignificant stuff\" setting.\n",
    "        # It removes noise and minor texture, but the high-contrast features survive.\n",
    "        blurred_img = cv2.GaussianBlur(high_contrast_img, (13, 13), 0)\n",
    "\n",
    "        # 3. TIGHT CANNY THRESHOLDS (Only grab the strongest, essential lines)\n",
    "        low_threshold = 100\n",
    "        high_threshold = 200 \n",
    "        edges = cv2.Canny(blurred_img, 0, 100)\n",
    "\n",
    "        # 4. DILATION to Thicken and Connect Lines\n",
    "        kernel = np.ones((5, 5), np.uint8) \n",
    "        dilated_edges = cv2.dilate(edges, kernel, iterations=2) \n",
    "\n",
    "        # Invert to get black lines on a white background\n",
    "        line_drawing = cv2.bitwise_not(dilated_edges)\n",
    "\n",
    "        # Save the result\n",
    "        cv2.imwrite(output_path, line_drawing)\n",
    "        print(f\"Final minimal line drawing saved to: {output_path}\")\n",
    "\n",
    "# Example Usage:\n",
    "image_to_minimal_line_drawing_final('divija.png', 'minimal_line_drawing_final.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3b5c33c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MediaPipe/CLAHE line drawing for: divija.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760842821.649733 48084326 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760842821.652119 48204329 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final minimal line drawing saved to: minimal_line_drawing_final.png\n"
     ]
    }
   ],
   "source": [
    "# --- NEW MINIMAL LINE DRAWING WITH MEDIAPIPE AND SELECTIVE CLOTHING BLUR ---\n",
    "def image_to_minimal_line_drawing_final(image_path, output_path):\n",
    "    \"\"\"\n",
    "    Uses MediaPipe Selfie Segmentation to remove the background, \n",
    "    then applies CLAHE (Adaptive Contrast) to boost facial features, \n",
    "    followed by strong blur and minimal Canny settings for a highly \n",
    "    stylized, clean line drawing.\n",
    "    \"\"\"\n",
    "    print(f\"Starting MediaPipe/CLAHE line drawing for: {image_path}\")\n",
    "    \n",
    "    # 1. Initialize MediaPipe Selfie Segmentation\n",
    "    mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "    \n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=0) as segmentor:\n",
    "        \n",
    "        # Read the image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Error: Could not read image at {image_path}\")\n",
    "            return\n",
    "\n",
    "        # --- Background Removal using MediaPipe ---\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_rgb.flags.writeable = False\n",
    "        # MediaPipe requires an RGB input\n",
    "        results = segmentor.process(img_rgb) \n",
    "        img_rgb.flags.writeable = True\n",
    "\n",
    "        processed_img = img \n",
    "        if results.segmentation_mask is not None:\n",
    "            # Create a mask for the foreground (person)\n",
    "            threshold = 0.1\n",
    "            binary_mask = results.segmentation_mask > threshold\n",
    "            # Stack the 1-channel mask to 3 channels to apply it to the BGR image\n",
    "            mask_3channel = np.stack((binary_mask,) * 3, axis=-1)\n",
    "            \n",
    "            # Create a solid white background (BGR format)\n",
    "            background_color = [255, 255, 255] \n",
    "            solid_background = np.full(img.shape, background_color, dtype=np.uint8)\n",
    "            \n",
    "            # Use the mask to place the person (img) on the white background\n",
    "            processed_img = np.where(mask_3channel, img, solid_background)\n",
    "        \n",
    "        # --- Edge Detection with CLAHE Contrast Enhancement ---\n",
    "        \n",
    "        gray_img = cv2.cvtColor(processed_img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # 1. ADAPTIVE CONTRAST (CLAHE): Boost local features \n",
    "        # Increased clipLimit for stronger detail enhancement in the face area\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "        high_contrast_img = clahe.apply(gray_img) \n",
    "        \n",
    "        # 2. STRONG GAUSSIAN BLUR: Removes noise and texture (like fine hair details)\n",
    "        # (13, 13) is a strong blur to eliminate insignificant stuff.\n",
    "        blurred_img = cv2.GaussianBlur(high_contrast_img, (41, 41), 0)\n",
    "\n",
    "        # 3. TIGHT CANNY THRESHOLDS: Only grab the strongest, essential lines\n",
    "        # Using 0 as the lower threshold here can sometimes capture more detail, \n",
    "        # but combined with the strong blur, it should still result in clean lines.\n",
    "        # We will use the original thresholds from the provided logic (100/200 on the blurred image).\n",
    "        low_threshold = 100\n",
    "        high_threshold = 200 \n",
    "        edges = cv2.Canny(blurred_img, 0, 30)\n",
    "\n",
    "        # 4. DILATION to Thicken and Connect Lines\n",
    "        kernel = np.ones((5, 5), np.uint8) \n",
    "        dilated_edges = cv2.dilate(edges, kernel, iterations=2) \n",
    "\n",
    "        # Invert to get black lines on a white background\n",
    "        line_drawing = cv2.bitwise_not(dilated_edges)\n",
    "\n",
    "        # Save the result\n",
    "        cv2.imwrite(output_path, line_drawing)\n",
    "        print(f\"Final minimal line drawing saved to: {output_path}\")\n",
    "\n",
    "image_to_minimal_line_drawing_final('divija.png', 'minimal_line_drawing_final.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7947f754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting post-processing for: minimal_line_drawing_final.png\n",
      "Cleaned line drawing saved to: cleaned_line_drawing.png\n"
     ]
    }
   ],
   "source": [
    "def post_process_line_drawing(input_line_drawing_path, output_path=\"cleaned_line_drawing.png\"):\n",
    "    \"\"\"\n",
    "    Applies morphological operations and skeletonization to an existing line drawing\n",
    "    to create a cleaner, more single-line type of drawing.\n",
    "    \"\"\"\n",
    "    print(f\"Starting post-processing for: {input_line_drawing_path}\")\n",
    "\n",
    "    # Load the line drawing (assuming it's black lines on a white background)\n",
    "    img = cv2.imread(input_line_drawing_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read line drawing at {input_line_drawing_path}\")\n",
    "        return\n",
    "\n",
    "    # Invert to make lines white on a black background for morphological operations\n",
    "    # (Many CV operations work better on white foreground)\n",
    "    inverted_img = cv2.bitwise_not(img)\n",
    "\n",
    "    # Convert to binary (ensure values are strictly 0 or 255)\n",
    "    _, binary_img = cv2.threshold(inverted_img, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # 1. Closing: Dilation followed by Erosion\n",
    "    # Helps close small gaps and merge nearby lines.\n",
    "    # Kernel size will influence how much \"merging\" happens.\n",
    "    close_kernel = np.ones((3, 3), np.uint8) # Adjust kernel size as needed\n",
    "    closed_img = cv2.morphologyEx(binary_img, cv2.MORPH_CLOSE, close_kernel, iterations=1)\n",
    "\n",
    "    # 2. Opening: Erosion followed by Dilation\n",
    "    # Helps remove small isolated noise pixels and thin out lines slightly.\n",
    "    open_kernel = np.ones((3, 3), np.uint8) # Adjust kernel size as needed\n",
    "    opened_img = cv2.morphologyEx(closed_img, cv2.MORPH_OPEN, open_kernel, iterations=1)\n",
    "    \n",
    "    # 3. Skeletonization (Medial Axis Transform)\n",
    "    # This algorithm reduces binary objects (lines) to a single-pixel wide skeleton.\n",
    "    # OpenCV doesn't have a direct `skeletonize` function, so we implement a common algorithm.\n",
    "    skeleton = np.zeros(opened_img.shape, dtype=np.uint8)\n",
    "    eroded = opened_img.copy()\n",
    "    temp = np.zeros(opened_img.shape, dtype=np.uint8)\n",
    "    \n",
    "    # Define a cross-shaped structuring element for thinning\n",
    "    kernel_thin = cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))\n",
    "\n",
    "    while True:\n",
    "        eroded = cv2.erode(eroded, kernel_thin)\n",
    "        temp = cv2.dilate(eroded, kernel_thin)\n",
    "        temp = cv2.subtract(opened_img, temp)\n",
    "        skeleton = cv2.bitwise_or(skeleton, temp)\n",
    "        opened_img = eroded.copy()\n",
    "        if cv2.countNonZero(opened_img) == 0:\n",
    "            break\n",
    "            \n",
    "    # Optional: Remove very small contours (remaining noise/specks)\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(skeleton, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cleaned_skeleton = np.zeros_like(skeleton)\n",
    "    min_area = 10 # Adjust this value: smaller values keep more detail, larger remove more noise\n",
    "\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > min_area:\n",
    "            cv2.drawContours(cleaned_skeleton, [contour], -1, 255, 1) # Draw white lines\n",
    "\n",
    "    # Invert back to black lines on a white background\n",
    "    final_drawing = cv2.bitwise_not(cleaned_skeleton)\n",
    "\n",
    "    cv2.imwrite(output_path, final_drawing)\n",
    "    print(f\"Cleaned line drawing saved to: {output_path}\")\n",
    "\n",
    "post_process_line_drawing('minimal_line_drawing_final.png', 'cleaned_line_drawing.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "119b1a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Contour-Based Stylized Drawing for: divija.png\n",
      "Contour-based stylized drawing saved to: cartoon_stylized_output.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760841226.770654 48084326 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760841226.771394 48177902 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760841226.773054 48177901 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1760841226.796524 48084326 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760841226.797405 48177916 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "def image_to_cartoon_stylized_drawing(image_path, output_path=\"cartoon_stylized_output.png\"):\n",
    "    \"\"\"\n",
    "    Creates a very simple, stylized, almost cartoon-like drawing of a human \n",
    "    face and silhouette by treating both the facial features and the body \n",
    "    as contours (objects) to ensure smooth, continuous lines.\n",
    "    \"\"\"\n",
    "    print(f\"Starting Contour-Based Stylized Drawing for: {image_path}\")\n",
    "\n",
    "    # Initialize MediaPipe solutions\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "    \n",
    "    # MODIFICATION: Separate the Outer Face Oval from Internal Features\n",
    "    face_oval_set = [mp_face_mesh.FACEMESH_FACE_OVAL]\n",
    "    \n",
    "    # Internal features (Lips, Eyes, Nose, Eyebrows)\n",
    "    internal_feature_sets = [\n",
    "        mp_face_mesh.FACEMESH_LIPS,\n",
    "        mp_face_mesh.FACEMESH_LEFT_EYE,\n",
    "        mp_face_mesh.FACEMESH_RIGHT_EYE,\n",
    "        mp_face_mesh.FACEMESH_NOSE,\n",
    "        mp_face_mesh.FACEMESH_LEFT_EYEBROW,\n",
    "        mp_face_mesh.FACEMESH_RIGHT_EYEBROW\n",
    "    ]\n",
    "    \n",
    "    # BGR Black color for lines\n",
    "    line_color = (0, 0, 0)\n",
    "    line_thickness = 2 \n",
    "    \n",
    "    # Read the image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    H, W, _ = img.shape\n",
    "    \n",
    "    # Create a blank white canvas for the output drawing\n",
    "    drawing = np.full((H, W, 3), 255, dtype=np.uint8) \n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # 1. --- Contour-Based Drawing for Facial Features ---\n",
    "    # We create a temporary mask for the features and then find contours\n",
    "    \n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True, \n",
    "        max_num_faces=1, \n",
    "        min_detection_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        \n",
    "        results = face_mesh.process(img_rgb)\n",
    "        \n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                \n",
    "                # --- A. Draw Internal Features (Eyes, Lips, Nose) ---\n",
    "                for feature_set in internal_feature_sets:\n",
    "                    \n",
    "                    # Create a fresh mask for each internal feature to ensure separation\n",
    "                    feature_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "                    \n",
    "                    points = []\n",
    "                    for connection in feature_set:\n",
    "                        p = face_landmarks.landmark[connection[0]]\n",
    "                        points.append((int(p.x * W), int(p.y * H)))\n",
    "\n",
    "                    # Use convex hull to define the shape and fill it solid white in the mask\n",
    "                    points = np.array(points, np.int32)\n",
    "                    \n",
    "                    # We must check if we have enough points for convexHull and contour finding\n",
    "                    if len(points) >= 3: \n",
    "                        hull = cv2.convexHull(points)\n",
    "                        cv2.fillPoly(feature_mask, [hull], 255)\n",
    "\n",
    "                        # Find and Draw Contours for this single internal feature\n",
    "                        feature_contours, _ = cv2.findContours(feature_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                        \n",
    "                        for contour in feature_contours:\n",
    "                            # Low epsilon for minimal simplification\n",
    "                            epsilon = 0.0005 * cv2.arcLength(contour, True) \n",
    "                            approx_contour = cv2.approxPolyDP(contour, epsilon, True)\n",
    "                            \n",
    "                            cv2.drawContours(drawing, [approx_contour], 0, line_color, line_thickness)\n",
    "\n",
    "                # --- B. Draw the Face Oval Contour Separately ---\n",
    "                for feature_set in face_oval_set:\n",
    "                    \n",
    "                    # Create a mask for the oval only\n",
    "                    oval_mask = np.zeros((H, W), dtype=np.uint8) \n",
    "                    \n",
    "                    points = []\n",
    "                    for connection in feature_set:\n",
    "                        p = face_landmarks.landmark[connection[0]]\n",
    "                        points.append((int(p.x * W), int(p.y * H)))\n",
    "\n",
    "                    points = np.array(points, np.int32)\n",
    "                    if len(points) >= 3:\n",
    "                        hull = cv2.convexHull(points)\n",
    "                        cv2.fillPoly(oval_mask, [hull], 255)\n",
    "\n",
    "                        # Find and Draw Contours for the oval\n",
    "                        oval_contours, _ = cv2.findContours(oval_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                        \n",
    "                        for contour in oval_contours:\n",
    "                            epsilon = 0.0005 * cv2.arcLength(contour, True) \n",
    "                            approx_contour = cv2.approxPolyDP(contour, epsilon, True)\n",
    "                            \n",
    "                            # Draw the simplified contour boundary\n",
    "                            cv2.drawContours(drawing, [approx_contour], 0, line_color, line_thickness)\n",
    "\n",
    "\n",
    "    # 2. --- Draw Outer Silhouette (Hair, Head, Shirt) using Segmentation ---\n",
    "    # This is the same logic as before, ensuring a clean outer line.\n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=0) as segmentor:\n",
    "        \n",
    "        segmentation_results = segmentor.process(img_rgb)\n",
    "        \n",
    "        if segmentation_results.segmentation_mask is not None:\n",
    "            \n",
    "            mask = segmentation_results.segmentation_mask\n",
    "            mask_smooth = cv2.GaussianBlur(mask, (7, 7), 0)\n",
    "            threshold = 0.5\n",
    "            binary_mask = (mask_smooth * 255).astype(np.uint8)\n",
    "            _, binary_mask = cv2.threshold(binary_mask, int(threshold * 255), 255, cv2.THRESH_BINARY)\n",
    "            \n",
    "            contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            if contours:\n",
    "                largest_contour = max(contours, key=cv2.contourArea)\n",
    "                \n",
    "                # Epsilon for smooth silhouette (0.001 retains decent shape)\n",
    "                epsilon = 0.001 * cv2.arcLength(largest_contour, True) \n",
    "                approx_contour = cv2.approxPolyDP(largest_contour, epsilon, True)\n",
    "                \n",
    "                # Draw the bold outer boundary\n",
    "                cv2.drawContours(drawing, [approx_contour], 0, line_color, line_thickness + 2) \n",
    "\n",
    "    # Save the final result\n",
    "    cv2.imwrite(output_path, drawing)\n",
    "    print(f\"Contour-based stylized drawing saved to: {output_path}\")\n",
    "\n",
    "image_to_cartoon_stylized_drawing('divija.png', 'cartoon_stylized_output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "701b180c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hybrid Stylized Drawing for: divija.png\n",
      "Hybrid stylized drawing saved to: hybrid_stylized_output.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760841372.010085 48084326 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760841372.010845 48180215 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760841372.012195 48180220 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1760841372.052445 48084326 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760841372.053122 48180226 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "def image_to_hybrid_stylized_drawing(image_path, output_path=\"hybrid_stylized_output.png\"):\n",
    "    \"\"\"\n",
    "    Creates a hybrid stylized drawing by strictly limiting the CLAHE/Canny \n",
    "    edge detection ONLY to the facial region defined by the MediaPipe Face Mesh, \n",
    "    and combining it with a clean outer silhouette from Segmentation.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    import mediapipe as mp\n",
    "    import os\n",
    "\n",
    "    print(f\"Starting Hybrid Stylized Drawing for: {image_path}\")\n",
    "\n",
    "    # Initialize MediaPipe solutions\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "    \n",
    "    # BGR Black color for lines\n",
    "    line_color = (0, 0, 0)\n",
    "    line_thickness = 2 \n",
    "    \n",
    "    # Read the image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    H, W, _ = img.shape\n",
    "    \n",
    "    # Create a blank white canvas for the output drawing\n",
    "    drawing = np.full((H, W, 3), 255, dtype=np.uint8) \n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # 0. --- Generate Face Mask (Region of Interest) ---\n",
    "    face_mask_255 = np.zeros((H, W), dtype=np.uint8) # Mask starts black (0)\n",
    "    \n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True, \n",
    "        max_num_faces=1, \n",
    "        min_detection_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        \n",
    "        results = face_mesh.process(img_rgb)\n",
    "        \n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                \n",
    "                # Get the points for the face oval contour\n",
    "                points = []\n",
    "                for connection in mp_face_mesh.FACEMESH_FACE_OVAL:\n",
    "                    p = face_landmarks.landmark[connection[0]]\n",
    "                    points.append((int(p.x * W), int(p.y * H)))\n",
    "                \n",
    "                # Use convex hull to define the face shape\n",
    "                points = np.array(points, np.int32)\n",
    "                if len(points) >= 3:\n",
    "                    # Fill the inside of the face shape white (255)\n",
    "                    hull = cv2.convexHull(points)\n",
    "                    cv2.fillPoly(face_mask_255, [hull], 255)\n",
    "\n",
    "    # 1. --- Edge Detection using CLAHE + Canny (Masked for Internal Detail) ---\n",
    "    \n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # A. ADAPTIVE CONTRAST (CLAHE): Aggressively boost local features\n",
    "    clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8)) \n",
    "    high_contrast_img = clahe.apply(gray_img) \n",
    "    \n",
    "    # B. LIGHT GAUSSIAN BLUR\n",
    "    blurred_img = cv2.GaussianBlur(high_contrast_img, (5, 5), 0)\n",
    "\n",
    "    # C. CANNY EDGE DETECTION: Find essential lines globally\n",
    "    low_threshold = 20\n",
    "    high_threshold = 80 \n",
    "    internal_edges = cv2.Canny(blurred_img, low_threshold, high_threshold)\n",
    "\n",
    "    # D. MASKING STEP: Apply the Face Mask to the Canny edges\n",
    "    # Only keep the edges that are inside the white facial region (255)\n",
    "    if np.any(face_mask_255):\n",
    "        # Bitwise AND ensures edges are only kept where the mask is white (255)\n",
    "        internal_edges_masked = cv2.bitwise_and(internal_edges, internal_edges, mask=face_mask_255)\n",
    "    else:\n",
    "        # If no face is detected, we fallback to just the global Canny edges (which might be noisy)\n",
    "        internal_edges_masked = internal_edges\n",
    "\n",
    "    # E. Draw the black Canny lines directly onto the white canvas\n",
    "    line_mask_3ch = cv2.cvtColor(internal_edges_masked, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # Wherever line_mask_3ch has a black line (0), the drawing canvas will be overwritten by 0 (black)\n",
    "    drawing = np.minimum(drawing, line_mask_3ch)\n",
    "\n",
    "    # 2. --- Draw Outer Silhouette (Hair, Head, Shirt) using Segmentation ---\n",
    "    # This remains the same as before, ensuring a clean outer line.\n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=0) as segmentor:\n",
    "        \n",
    "        segmentation_results = segmentor.process(img_rgb)\n",
    "        \n",
    "        if segmentation_results.segmentation_mask is not None:\n",
    "            \n",
    "            mask = segmentation_results.segmentation_mask\n",
    "            mask_smooth = cv2.GaussianBlur(mask, (7, 7), 0)\n",
    "            threshold = 0.5\n",
    "            binary_mask = (mask_smooth * 255).astype(np.uint8)\n",
    "            _, binary_mask = cv2.threshold(binary_mask, int(threshold * 255), 255, cv2.THRESH_BINARY)\n",
    "            \n",
    "            contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            if contours:\n",
    "                largest_contour = max(contours, key=cv2.contourArea)\n",
    "                \n",
    "                # Epsilon for smooth silhouette (0.001 retains decent shape)\n",
    "                epsilon = 0.001 * cv2.arcLength(largest_contour, True) \n",
    "                approx_contour = cv2.approxPolyDP(largest_contour, epsilon, True)\n",
    "                \n",
    "                # Draw the bold outer boundary (thickness + 2)\n",
    "                # Note: This is drawn *over* the Canny lines, ensuring the final silhouette is clean.\n",
    "                cv2.drawContours(drawing, [approx_contour], 0, line_color, line_thickness + 2) \n",
    "\n",
    "    # Save the final result\n",
    "    cv2.imwrite(output_path, drawing)\n",
    "    print(f\"Hybrid stylized drawing saved to: {output_path}\")\n",
    "\n",
    "image_to_hybrid_stylized_drawing('divija.png', 'hybrid_stylized_output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc0fdf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting selective line drawing for: divija.png\n",
      "Final selective line drawing saved to: selective_line_drawing.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760841869.270815 48084326 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760841869.271533 48187720 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760841869.277457 48187719 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/Users/divija/Divi Drive/workplace/Princeton/Sem 5/Carlab/carlab_2025/final_project/carlab/lib/python3.12/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "I0000 00:00:1760841869.290394 48084326 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760841869.291631 48187732 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "def image_to_minimal_line_drawing_enhanced(image_path, output_path):\n",
    "    \"\"\"\n",
    "    Creates a highly stylized line drawing by selectively applying contrast enhancement \n",
    "    (CLAHE) and high-detail Canny edge detection only to the facial features (ROI defined \n",
    "    by Face Mesh), and then adding a minimal, low-detail outline for the hair and clothing \n",
    "    using the Selfie Segmentation mask.\n",
    "    \"\"\"\n",
    "    print(f\"Starting selective line drawing for: {image_path}\")\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    H, W, _ = img.shape\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "    # Initialize the final line drawing canvas: A white background (255)\n",
    "    final_line_drawing = np.ones((H, W), dtype=np.uint8) * 255 \n",
    "    \n",
    "    # --- 1. Face Mesh for Feature ROI and CLAHE ---\n",
    "    # The 'refine_landmarks=True' helps get better eye and lip contour details\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True, \n",
    "        max_num_faces=1, \n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        \n",
    "        results_face = face_mesh.process(img_rgb)\n",
    "        \n",
    "        if results_face.multi_face_landmarks:\n",
    "            \n",
    "            # Use the first detected face\n",
    "            face_landmarks = results_face.multi_face_landmarks[0]\n",
    "            \n",
    "            # Calculate a tight bounding box around all face landmarks\n",
    "            x_coords = [lm.x * W for lm in face_landmarks.landmark]\n",
    "            y_coords = [lm.y * H for lm in face_landmarks.landmark]\n",
    "            \n",
    "            min_x, max_x = int(min(x_coords)), int(max(x_coords))\n",
    "            min_y, max_y = int(min(y_coords)), int(max(y_coords))\n",
    "\n",
    "            # Add padding and clamp to image boundaries to ensure smooth edges\n",
    "            padding = int(W * 0.05)\n",
    "            x1 = max(0, min_x - padding)\n",
    "            y1 = max(0, min_y - padding)\n",
    "            x2 = min(W, max_x + padding)\n",
    "            y2 = min(H, max_y + padding)\n",
    "            \n",
    "            # Extract the ROI from the original image\n",
    "            face_roi_area = img[y1:y2, x1:x2].copy()\n",
    "            face_roi_gray = cv2.cvtColor(face_roi_area, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # --- Selective CLAHE and Canny for High-Detail Face ---\n",
    "            \n",
    "            # Apply ADAPTIVE CONTRAST (CLAHE): Increased clipLimit for stronger detail\n",
    "            clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))\n",
    "            high_contrast_face = clahe.apply(face_roi_gray)\n",
    "            \n",
    "            # Apply Gaussian Blur (small kernel to retain facial detail, but remove fine noise)\n",
    "            blurred_face = cv2.GaussianBlur(high_contrast_face, (5, 5), 0)\n",
    "            \n",
    "            # TIGHT CANNY THRESHOLDS for capturing essential lines like eyes, nose, mouth\n",
    "            face_edges = cv2.Canny(blurred_face, 50, 150)\n",
    "            \n",
    "            # Dilate the face lines slightly to make them more visible\n",
    "            kernel_face = np.ones((3, 3), np.uint8) \n",
    "            dilated_face_edges = cv2.dilate(face_edges, kernel_face, iterations=1) \n",
    "            \n",
    "            # Invert the lines (black lines on white)\n",
    "            inverted_face_edges = cv2.bitwise_not(dilated_face_edges)\n",
    "\n",
    "            # Combine face edges onto the final canvas (black lines (0) overwrite white (255))\n",
    "            current_canvas_roi = final_line_drawing[y1:y2, x1:x2]\n",
    "            final_line_drawing[y1:y2, x1:x2] = np.minimum(current_canvas_roi, inverted_face_edges)\n",
    "\n",
    "    # --- 2. Selfie Segmentation for Hair and Clothing Outline (Minimal Detail) ---\n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as segmentor:\n",
    "        results_segmentation = segmentor.process(img_rgb)\n",
    "        \n",
    "        if results_segmentation.segmentation_mask is not None:\n",
    "            \n",
    "            # Create a clean binary mask for the whole person (including hair and clothing)\n",
    "            threshold = 0.5\n",
    "            person_mask = (results_segmentation.segmentation_mask > threshold).astype(np.uint8)\n",
    "            \n",
    "            # Convert the mask to an image (white person shape on black background)\n",
    "            person_outline_img = person_mask * 255 \n",
    "\n",
    "            # Strong blur to simplify the outline (this only captures the gross shape)\n",
    "            blurred_outline = cv2.GaussianBlur(person_outline_img, (61, 61), 0)\n",
    "\n",
    "            # Very broad Canny thresholds for minimal, loose outline lines\n",
    "            low_threshold_outline = 10\n",
    "            high_threshold_outline = 50 \n",
    "            \n",
    "            # Run Canny on the strongly blurred mask\n",
    "            outline_edges = cv2.Canny(blurred_outline, low_threshold_outline, high_threshold_outline)\n",
    "            \n",
    "            # Dilate to thicken the overall outline, making it the dominant silhouette\n",
    "            kernel_outline = np.ones((7, 7), np.uint8) \n",
    "            dilated_outline_edges = cv2.dilate(outline_edges, kernel_outline, iterations=2) \n",
    "            \n",
    "            # Invert the outline edges (black lines on white)\n",
    "            inverted_outline = cv2.bitwise_not(dilated_outline_edges)\n",
    "            \n",
    "            # Combine the outline onto the final canvas using a bitwise AND.\n",
    "            # This ensures that the low-detail outline complements the high-detail face,\n",
    "            # and that lines from both are preserved.\n",
    "            final_line_drawing = cv2.bitwise_and(final_line_drawing, inverted_outline)\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(output_path, final_line_drawing)\n",
    "    print(f\"Final selective line drawing saved to: {output_path}\")\n",
    "\n",
    "# Example call (make sure you have an image named 'divija.png' in the same directory)\n",
    "image_to_minimal_line_drawing_enhanced('divija.png', 'selective_line_drawing.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21bae46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting selective line drawing for: divija.png\n",
      "Final selective line drawing saved to: selective_line_drawing.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760842077.552652 48084326 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760842077.553956 48191226 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760842077.557301 48191229 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/Users/divija/Divi Drive/workplace/Princeton/Sem 5/Carlab/carlab_2025/final_project/carlab/lib/python3.12/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "I0000 00:00:1760842077.567557 48084326 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760842077.568195 48191238 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "# Indices for Eyes, Nose, and Mouth only (excluding the outer face contour/jawline)\n",
    "# This list is used to define a tight bounding box around the key features.\n",
    "FEATURE_INDICES = [\n",
    "    # Nose (Nose bridge, tip)\n",
    "    1, 6, 197,\n",
    "    # Mouth (Upper lip, lower lip, corners)\n",
    "    0, 17, 61, 291, 14, 308, 375,\n",
    "    # Left Eye (Inner corner, outer corner, central points)\n",
    "    33, 133, 163, 144, 246,\n",
    "    # Right Eye (Inner corner, outer corner, central points)\n",
    "    362, 263, 390, 373, 466\n",
    "]\n",
    "\n",
    "def image_to_minimal_line_drawing_enhanced(image_path, output_path):\n",
    "    \"\"\"\n",
    "    Creates a highly stylized line drawing by selectively applying contrast enhancement \n",
    "    (CLAHE) and high-detail Canny edge detection only to the key facial features (ROI \n",
    "    defined by a subset of Face Mesh landmarks), and then adding a minimal, low-detail \n",
    "    outline for the hair and clothing using the Selfie Segmentation mask.\n",
    "    \"\"\"\n",
    "    print(f\"Starting selective line drawing for: {image_path}\")\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    H, W, _ = img.shape\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Initialize the final line drawing canvas: A white background (255)\n",
    "    final_line_drawing = np.ones((H, W), dtype=np.uint8) * 255 \n",
    "    \n",
    "    # --- 1. Face Mesh for Feature ROI and CLAHE ---\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True, \n",
    "        max_num_faces=1, \n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        \n",
    "        results_face = face_mesh.process(img_rgb)\n",
    "        \n",
    "        if results_face.multi_face_landmarks:\n",
    "            \n",
    "            # Use the first detected face\n",
    "            face_landmarks = results_face.multi_face_landmarks[0]\n",
    "            \n",
    "            # Calculate a tight bounding box *only* around the selected features\n",
    "            x_coords = []\n",
    "            y_coords = []\n",
    "            \n",
    "            for index in FEATURE_INDICES:\n",
    "                lm = face_landmarks.landmark[index]\n",
    "                x_coords.append(lm.x * W)\n",
    "                y_coords.append(lm.y * H)\n",
    "            \n",
    "            min_x, max_x = int(min(x_coords)), int(max(x_coords))\n",
    "            min_y, max_y = int(min(y_coords)), int(max(y_coords))\n",
    "\n",
    "            # Add padding and clamp to image boundaries to ensure smooth edges\n",
    "            padding = int(W * 0.05)\n",
    "            x1 = max(0, min_x - padding)\n",
    "            y1 = max(0, min_y - padding)\n",
    "            x2 = min(W, max_x + padding)\n",
    "            y2 = min(H, max_y + padding)\n",
    "            \n",
    "            # Extract the ROI from the original image\n",
    "            face_roi_area = img[y1:y2, x1:x2].copy()\n",
    "            face_roi_gray = cv2.cvtColor(face_roi_area, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # --- Selective CLAHE and Canny for High-Detail Features ---\n",
    "            \n",
    "            # Apply ADAPTIVE CONTRAST (CLAHE): Increased clipLimit for stronger detail\n",
    "            clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))\n",
    "            high_contrast_face = clahe.apply(face_roi_gray)\n",
    "            \n",
    "            # Apply Gaussian Blur (small kernel to retain facial detail, but remove fine noise)\n",
    "            blurred_face = cv2.GaussianBlur(high_contrast_face, (5, 5), 0)\n",
    "            \n",
    "            # TIGHT CANNY THRESHOLDS for capturing essential lines like eyes, nose, mouth\n",
    "            face_edges = cv2.Canny(blurred_face, 50, 150)\n",
    "            \n",
    "            # Dilate the face lines slightly to make them more visible\n",
    "            kernel_face = np.ones((3, 3), np.uint8) \n",
    "            dilated_face_edges = cv2.dilate(face_edges, kernel_face, iterations=1) \n",
    "            \n",
    "            # Invert the lines (black lines on white)\n",
    "            inverted_face_edges = cv2.bitwise_not(dilated_face_edges)\n",
    "\n",
    "            # Combine face edges onto the final canvas (black lines (0) overwrite white (255))\n",
    "            current_canvas_roi = final_line_drawing[y1:y2, x1:x2]\n",
    "            final_line_drawing[y1:y2, x1:x2] = np.minimum(current_canvas_roi, inverted_face_edges)\n",
    "\n",
    "    # --- 2. Selfie Segmentation for Hair and Clothing Outline (Minimal Detail) ---\n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as segmentor:\n",
    "        results_segmentation = segmentor.process(img_rgb)\n",
    "        \n",
    "        if results_segmentation.segmentation_mask is not None:\n",
    "            \n",
    "            # Create a clean binary mask for the whole person (including hair and clothing)\n",
    "            threshold = 0.5\n",
    "            person_mask = (results_segmentation.segmentation_mask > threshold).astype(np.uint8)\n",
    "            \n",
    "            # Convert the mask to an image (white person shape on black background)\n",
    "            person_outline_img = person_mask * 255 \n",
    "\n",
    "            # Strong blur to simplify the outline (this only captures the gross shape)\n",
    "            blurred_outline = cv2.GaussianBlur(person_outline_img, (61, 61), 0)\n",
    "\n",
    "            # Very broad Canny thresholds for minimal, loose outline lines\n",
    "            low_threshold_outline = 10\n",
    "            high_threshold_outline = 50 \n",
    "            \n",
    "            # Run Canny on the strongly blurred mask\n",
    "            outline_edges = cv2.Canny(blurred_outline, low_threshold_outline, high_threshold_outline)\n",
    "            \n",
    "            # Dilate to thicken the overall outline, making it the dominant silhouette\n",
    "            kernel_outline = np.ones((7, 7), np.uint8) \n",
    "            dilated_outline_edges = cv2.dilate(outline_edges, kernel_outline, iterations=2) \n",
    "            \n",
    "            # Invert the outline edges (black lines on white)\n",
    "            inverted_outline = cv2.bitwise_not(dilated_outline_edges)\n",
    "            \n",
    "            # Combine the outline onto the final canvas using a bitwise AND.\n",
    "            # This ensures that the low-detail outline complements the high-detail face,\n",
    "            # and that lines from both are preserved.\n",
    "            final_line_drawing = cv2.bitwise_and(final_line_drawing, inverted_outline)\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(output_path, final_line_drawing)\n",
    "    print(f\"Final selective line drawing saved to: {output_path}\")\n",
    "\n",
    "image_to_minimal_line_drawing_enhanced('divija.png', 'selective_line_drawing.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39f2e16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting selective line drawing for: divija.png\n",
      "DEBUG MODE: ROI extracted to: selective_line_drawing_roi_only.png\n",
      "DEBUG MODE: ROI coordinates (x1, y1, x2, y2): (1320, 1246, 2312, 2138)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760842167.834645 48084326 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760842167.835684 48192795 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760842167.838968 48192797 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize MediaPipe solutions (all required imports in one place)\n",
    "# We need Face Mesh for precise feature ROI, and Selfie Segmentation for the body/hair outline.\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "# Indices for Eyes, Nose, and Mouth only (excluding the outer face contour/jawline)\n",
    "# This list is used to define a tight bounding box around the key features.\n",
    "FEATURE_INDICES = [\n",
    "    # Nose (Nose bridge, tip)\n",
    "    1, 6, 197,\n",
    "    # Mouth (Upper lip, lower lip, corners)\n",
    "    0, 17, 61, 291, 14, 308, 375,\n",
    "    # Left Eye (Inner corner, outer corner, central points)\n",
    "    33, 133, 163, 144, 246,\n",
    "    # Right Eye (Inner corner, outer corner, central points)\n",
    "    362, 263, 390, 373, 466\n",
    "]\n",
    "\n",
    "def image_to_minimal_line_drawing_enhanced(image_path, output_path, debug_mode=False):\n",
    "    \"\"\"\n",
    "    Creates a highly stylized line drawing by selectively applying contrast enhancement \n",
    "    (CLAHE) and high-detail Canny edge detection only to the key facial features (ROI \n",
    "    defined by a subset of Face Mesh landmarks), and then adding a minimal, low-detail \n",
    "    outline for the hair and clothing using the Selfie Segmentation mask.\n",
    "    \n",
    "    If debug_mode is True, it saves the cropped ROI image and stops execution.\n",
    "    \"\"\"\n",
    "    print(f\"Starting selective line drawing for: {image_path}\")\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    H, W, _ = img.shape\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Initialize the final line drawing canvas: A white background (255)\n",
    "    final_line_drawing = np.ones((H, W), dtype=np.uint8) * 255 \n",
    "    \n",
    "    # --- 1. Face Mesh for Feature ROI and CLAHE ---\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True, \n",
    "        max_num_faces=1, \n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        \n",
    "        results_face = face_mesh.process(img_rgb)\n",
    "        \n",
    "        if results_face.multi_face_landmarks:\n",
    "            \n",
    "            # Use the first detected face\n",
    "            face_landmarks = results_face.multi_face_landmarks[0]\n",
    "            \n",
    "            # Calculate a tight bounding box *only* around the selected features\n",
    "            x_coords = []\n",
    "            y_coords = []\n",
    "            \n",
    "            for index in FEATURE_INDICES:\n",
    "                lm = face_landmarks.landmark[index]\n",
    "                x_coords.append(lm.x * W)\n",
    "                y_coords.append(lm.y * H)\n",
    "            \n",
    "            min_x, max_x = int(min(x_coords)), int(max(x_coords))\n",
    "            min_y, max_y = int(min(y_coords)), int(max(y_coords))\n",
    "\n",
    "            # Add padding and clamp to image boundaries to ensure smooth edges\n",
    "            padding = int(W * 0.05)\n",
    "            x1 = max(0, min_x - padding)\n",
    "            y1 = max(0, min_y - padding)\n",
    "            x2 = min(W, max_x + padding)\n",
    "            y2 = min(H, max_y + padding)\n",
    "            \n",
    "            # Extract the ROI from the original image\n",
    "            face_roi_area = img[y1:y2, x1:x2].copy()\n",
    "            \n",
    "            # --- DEBUG MODE: Save ROI and stop ---\n",
    "            if debug_mode:\n",
    "                debug_path = output_path.replace('.png', '_roi_only.png')\n",
    "                cv2.imwrite(debug_path, face_roi_area)\n",
    "                print(f\"DEBUG MODE: ROI extracted to: {debug_path}\")\n",
    "                print(f\"DEBUG MODE: ROI coordinates (x1, y1, x2, y2): ({x1}, {y1}, {x2}, {y2})\")\n",
    "                return # Stop further processing\n",
    "\n",
    "            # --- Selective CLAHE and Canny for High-Detail Features ---\n",
    "            face_roi_gray = cv2.cvtColor(face_roi_area, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Apply ADAPTIVE CONTRAST (CLAHE): Increased clipLimit for stronger detail\n",
    "            clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))\n",
    "            high_contrast_face = clahe.apply(face_roi_gray)\n",
    "            \n",
    "            # Apply Gaussian Blur (small kernel to retain facial detail, but remove fine noise)\n",
    "            blurred_face = cv2.GaussianBlur(high_contrast_face, (5, 5), 0)\n",
    "            \n",
    "            # TIGHT CANNY THRESHOLDS for capturing essential lines like eyes, nose, mouth\n",
    "            face_edges = cv2.Canny(blurred_face, 50, 150)\n",
    "            \n",
    "            # Dilate the face lines slightly to make them more visible\n",
    "            kernel_face = np.ones((3, 3), np.uint8) \n",
    "            dilated_face_edges = cv2.dilate(face_edges, kernel_face, iterations=1) \n",
    "            \n",
    "            # Invert the lines (black lines on white)\n",
    "            inverted_face_edges = cv2.bitwise_not(dilated_face_edges)\n",
    "\n",
    "            # Combine face edges onto the final canvas (black lines (0) overwrite white (255))\n",
    "            current_canvas_roi = final_line_drawing[y1:y2, x1:x2]\n",
    "            final_line_drawing[y1:y2, x1:x2] = np.minimum(current_canvas_roi, inverted_face_edges)\n",
    "\n",
    "    # --- 2. Selfie Segmentation for Hair and Clothing Outline (Minimal Detail) ---\n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as segmentor:\n",
    "        results_segmentation = segmentor.process(img_rgb)\n",
    "        \n",
    "        if results_segmentation.segmentation_mask is not None:\n",
    "            \n",
    "            # Create a clean binary mask for the whole person (including hair and clothing)\n",
    "            threshold = 0.5\n",
    "            person_mask = (results_segmentation.segmentation_mask > threshold).astype(np.uint8)\n",
    "            \n",
    "            # Convert the mask to an image (white person shape on black background)\n",
    "            person_outline_img = person_mask * 255 \n",
    "\n",
    "            # Strong blur to simplify the outline (this only captures the gross shape)\n",
    "            blurred_outline = cv2.GaussianBlur(person_outline_img, (61, 61), 0)\n",
    "\n",
    "            # Very broad Canny thresholds for minimal, loose outline lines\n",
    "            low_threshold_outline = 10\n",
    "            high_threshold_outline = 50 \n",
    "            \n",
    "            # Run Canny on the strongly blurred mask\n",
    "            outline_edges = cv2.Canny(blurred_outline, low_threshold_outline, high_threshold_outline)\n",
    "            \n",
    "            # Dilate to thicken the overall outline, making it the dominant silhouette\n",
    "            kernel_outline = np.ones((7, 7), np.uint8) \n",
    "            dilated_outline_edges = cv2.dilate(outline_edges, kernel_outline, iterations=2) \n",
    "            \n",
    "            # Invert the outline edges (black lines on white)\n",
    "            inverted_outline = cv2.bitwise_not(dilated_outline_edges)\n",
    "            \n",
    "            # Combine the outline onto the final canvas using a bitwise AND.\n",
    "            # This ensures that the low-detail outline complements the high-detail face,\n",
    "            # and that lines from both are preserved.\n",
    "            final_line_drawing = cv2.bitwise_and(final_line_drawing, inverted_outline)\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(output_path, final_line_drawing)\n",
    "    print(f\"Final selective line drawing saved to: {output_path}\")\n",
    "\n",
    "# Example call for normal execution\n",
    "# image_to_minimal_line_drawing_enhanced('divija.png', 'selective_line_drawing.png')\n",
    "\n",
    "# Example call for DEBUG MODE (Uncomment this line to test the ROI)\n",
    "image_to_minimal_line_drawing_enhanced('divija.png', 'selective_line_drawing.png', debug_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b346a916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting selective line drawing for: divija.png\n",
      "Final selective line drawing saved to: selective_line_drawing.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760843085.202883 48084326 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760843085.203591 48208784 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760843085.207236 48208783 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1760843085.215987 48084326 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760843085.216621 48208796 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Initialize MediaPipe solutions (all required imports in one place)\n",
    "# We need Face Mesh for precise feature ROI, and Selfie Segmentation for the body/hair outline.\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "# Feature groups for distinct processing (Eyes, Nose, Mouth).\n",
    "# Each group will be processed with its own tight bounding box.\n",
    "FEATURE_GROUPS = {\n",
    "    # Indices covering both eyes, excluding the face contour\n",
    "    \"eyes\": [33, 133, 163, 144, 246, 362, 263, 390, 373, 466, 130, 160, 243, 359, 387, 463], \n",
    "    \n",
    "    # Indices covering the nose bridge, tip, and base\n",
    "    \"nose\": [1, 6, 197, 2, 4, 5, 278, 275, 420], \n",
    "    \n",
    "    # Indices covering the lips and corners of the mouth\n",
    "    \"mouth\": [0, 17, 61, 291, 14, 308, 375, 40, 81, 311, 317, 415] ,\n",
    "    \"eyebrows\": [46, 53, 52, 65, 55, 70, 63, 105, 107, 276, 283, 282, 295, 285, 336, 296, 334, 293, 300],\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "def image_to_minimal_line_drawing_enhanced(image_path, output_path):\n",
    "    \"\"\"\n",
    "    Creates a highly stylized line drawing by applying CLAHE and Canny edge detection \n",
    "    independently to multiple small facial feature ROIs, then adding a minimal \n",
    "    outline for the hair and clothing.\n",
    "    \"\"\"\n",
    "    print(f\"Starting selective line drawing for: {image_path}\")\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    H, W, _ = img.shape\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Initialize the final line drawing canvas: A white background (255)\n",
    "    final_line_drawing = np.ones((H, W), dtype=np.uint8) * 255 \n",
    "    \n",
    "    # --- 1. Face Mesh for Multiple Feature ROIs and CLAHE ---\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True, \n",
    "        max_num_faces=1, \n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        \n",
    "        results_face = face_mesh.process(img_rgb)\n",
    "        \n",
    "        if results_face.multi_face_landmarks:\n",
    "            \n",
    "            # Use the first detected face\n",
    "            face_landmarks = results_face.multi_face_landmarks[0]\n",
    "            \n",
    "            # Loop over each feature group to process them individually\n",
    "            for feature_name, indices in FEATURE_GROUPS.items():\n",
    "                x_coords = []\n",
    "                y_coords = []\n",
    "                \n",
    "                # Calculate a tight bounding box *only* around the selected feature group\n",
    "                for index in indices:\n",
    "                    lm = face_landmarks.landmark[index]\n",
    "                    x_coords.append(lm.x * W)\n",
    "                    y_coords.append(lm.y * H)\n",
    "                \n",
    "                if not x_coords: continue # Skip if no landmarks are found\n",
    "                \n",
    "                min_x, max_x = int(min(x_coords)), int(max(x_coords))\n",
    "                min_y, max_y = int(min(y_coords)), int(max(y_coords))\n",
    "\n",
    "                # Use a small fixed padding for these tiny ROIs\n",
    "                padding = 15 \n",
    "                x1 = max(0, min_x - padding)\n",
    "                y1 = max(0, min_y - padding)\n",
    "                x2 = min(W, max_x + padding)\n",
    "                y2 = min(H, max_y + padding)\n",
    "                \n",
    "                # Ensure ROI is valid\n",
    "                if x2 <= x1 or y2 <= y1: continue\n",
    "\n",
    "                # Extract the ROI from the original image\n",
    "                face_roi_area = img[y1:y2, x1:x2].copy()\n",
    "                face_roi_gray = cv2.cvtColor(face_roi_area, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # --- Selective CLAHE and Canny for High-Detail Features ---\n",
    "                \n",
    "                # Apply ADAPTIVE CONTRAST (CLAHE)\n",
    "                clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))\n",
    "                high_contrast_feature = clahe.apply(face_roi_gray)\n",
    "                \n",
    "                # Apply Gaussian Blur (kernel size adjusted slightly per feature type if needed, or stick to small (5, 5))\n",
    "                blurred_feature = cv2.GaussianBlur(high_contrast_feature, (11, 11), 0)\n",
    "                \n",
    "                # TIGHT CANNY THRESHOLDS\n",
    "                feature_edges = cv2.Canny(blurred_feature, 0, 100)\n",
    "                \n",
    "                # Dilate the feature lines slightly\n",
    "                kernel_feature = np.ones((3, 3), np.uint8) \n",
    "                dilated_feature_edges = cv2.dilate(feature_edges, kernel_feature, iterations=1) \n",
    "                \n",
    "                # Invert the lines (black lines on white)\n",
    "                inverted_feature_edges = cv2.bitwise_not(dilated_feature_edges)\n",
    "\n",
    "                # Combine feature edges onto the final canvas (black lines (0) overwrite white (255))\n",
    "                current_canvas_roi = final_line_drawing[y1:y2, x1:x2]\n",
    "                final_line_drawing[y1:y2, x1:x2] = np.minimum(current_canvas_roi, inverted_feature_edges)\n",
    "\n",
    "    # --- 2. Selfie Segmentation for Hair and Clothing Outline (Minimal Detail) ---\n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as segmentor:\n",
    "        results_segmentation = segmentor.process(img_rgb)\n",
    "        \n",
    "        if results_segmentation.segmentation_mask is not None:\n",
    "            \n",
    "            # Create a clean binary mask for the whole person (including hair and clothing)\n",
    "            threshold = 0.5\n",
    "            person_mask = (results_segmentation.segmentation_mask > threshold).astype(np.uint8)\n",
    "            \n",
    "            # Convert the mask to an image (white person shape on black background)\n",
    "            person_outline_img = person_mask * 255 \n",
    "\n",
    "            # Strong blur to simplify the outline (this only captures the gross shape)\n",
    "            blurred_outline = cv2.GaussianBlur(person_outline_img, (61, 61), 0)\n",
    "\n",
    "            # Very broad Canny thresholds for minimal, loose outline lines\n",
    "            low_threshold_outline = 10\n",
    "            high_threshold_outline = 50 \n",
    "            \n",
    "            # Run Canny on the strongly blurred mask\n",
    "            outline_edges = cv2.Canny(blurred_outline, low_threshold_outline, high_threshold_outline)\n",
    "            \n",
    "            # Dilate to thicken the overall outline, making it the dominant silhouette\n",
    "            kernel_outline = np.ones((7, 7), np.uint8) \n",
    "            dilated_outline_edges = cv2.dilate(outline_edges, kernel_outline, iterations=2) \n",
    "            \n",
    "            # Invert the outline edges (black lines on white)\n",
    "            inverted_outline = cv2.bitwise_not(dilated_outline_edges)\n",
    "            \n",
    "            # Combine the outline onto the final canvas using a bitwise AND.\n",
    "            # This ensures that the low-detail outline complements the high-detail face,\n",
    "            # and that lines from both are preserved.\n",
    "            final_line_drawing = cv2.bitwise_and(final_line_drawing, inverted_outline)\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(output_path, final_line_drawing)\n",
    "    print(f\"Final selective line drawing saved to: {output_path}\")\n",
    "\n",
    "# Example call for normal execution\n",
    "image_to_minimal_line_drawing_enhanced('divija.png', 'selective_line_drawing.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6a87df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting selective line drawing for: divija.png\n",
      "DEBUG: Saved ROI for eyes to: selective_line_drawing_eyes_roi.png\n",
      "DEBUG: Saved ROI for nose to: selective_line_drawing_nose_roi.png\n",
      "DEBUG: Saved ROI for mouth to: selective_line_drawing_mouth_roi.png\n",
      "DEBUG: Saved ROI for philtrum to: selective_line_drawing_philtrum_roi.png\n",
      "DEBUG: Saved ROI for eyebrows to: selective_line_drawing_eyebrows_roi.png\n",
      "Final selective line drawing saved to: selective_line_drawing.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760845883.300894 48225941 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760845883.301598 48257597 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760845883.305178 48257596 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1760845883.330986 48225941 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760845883.331650 48257610 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "# Feature groups for distinct processing (Eyes, Nose, Mouth).\n",
    "# Each group will be processed with its own tight bounding box.\n",
    "FEATURE_GROUPS = {\n",
    "    # Indices covering both eyes, excluding the face contour\n",
    "    \"eyes\": [33, 133, 163, 144, 246, 362, 263, 390, 373, 466, 130, 160, 243, 359, 387, 463], \n",
    "    \n",
    "    # INDICES UPDATED: Added points for better coverage of nostrils and the nose base.\n",
    "    \"nose\": [1, 6, 197, 2, 4, 5, 278, 275, 420, 129, 358, 64, 294, 98, 327, 118, 347], \n",
    "    \n",
    "    # Indices covering the lips and corners of the mouth\n",
    "    \"mouth\": [0, 17, 61, 291, 14, 308, 375, 40, 81, 311, 317, 415],\n",
    "    \n",
    "    # UPDATED GROUP: Simplified indices to reliably anchor the region between nose (2) and lips (13, 14).\n",
    "    # This creates a vertical bounding box for the philtrum.\n",
    "    \"philtrum\": [2, 13, 14, 172, 406],\n",
    "    \n",
    "    # Indices covering both left and right eyebrows\n",
    "    \"eyebrows\": [46, 53, 52, 65, 55, 70, 63, 105, 107, 276, 283, 282, 295, 285, 336, 296, 334, 293, 300],\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "# Define Canny thresholds based on feature importance/noise level\n",
    "# Lower thresholds (e.g., 50, 150) capture more detail (high sensitivity)\n",
    "# Higher thresholds (e.g., 80, 200) capture only strong, clean edges (low sensitivity/less noise)\n",
    "CANNY_THRESHOLDS = {\n",
    "    \"eyes\": (50, 120),       # High detail for eyes\n",
    "    \"nose\": (50, 120),       # High detail for nose\n",
    "    \"mouth\": (50, 120),      # High detail for lips\n",
    "    \"philtrum\": (60, 120),   # Moderate detail for philtrum/creases\n",
    "    \"eyebrows\": (0, 40),   # Low detail/Less noise for eyebrows\n",
    "}\n",
    "GAUSSIAN_BLUR_SIZES = {\n",
    "    \"eyes\": (13, 13),\n",
    "    \"nose\": (11, 11),\n",
    "    \"mouth\": (11, 11),\n",
    "    \"philtrum\": (31, 31),\n",
    "    \"eyebrows\": (31, 31),\n",
    "}\n",
    "\n",
    "def image_to_minimal_line_drawing_enhanced(image_path, output_path):\n",
    "    \"\"\"\n",
    "    Creates a highly stylized line drawing by applying CLAHE and Canny edge detection \n",
    "    independently to multiple small facial feature ROIs, then adding a minimal \n",
    "    outline for the hair and clothing.\n",
    "    \"\"\"\n",
    "    print(f\"Starting selective line drawing for: {image_path}\")\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    H, W, _ = img.shape\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Initialize the final line drawing canvas: A white background (255)\n",
    "    final_line_drawing = np.ones((H, W), dtype=np.uint8) * 255 \n",
    "    \n",
    "    # --- 1. Face Mesh for Multiple Feature ROIs and CLAHE ---\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True, \n",
    "        max_num_faces=1, \n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        \n",
    "        results_face = face_mesh.process(img_rgb)\n",
    "        \n",
    "        if results_face.multi_face_landmarks:\n",
    "            \n",
    "            # Use the first detected face\n",
    "            face_landmarks = results_face.multi_face_landmarks[0]\n",
    "            \n",
    "            # Loop over each feature group to process them individually\n",
    "            for feature_name, indices in FEATURE_GROUPS.items():\n",
    "                x_coords = []\n",
    "                y_coords = []\n",
    "                \n",
    "                # Calculate a tight bounding box *only* around the selected feature group\n",
    "                for index in indices:\n",
    "                    lm = face_landmarks.landmark[index]\n",
    "                    x_coords.append(lm.x * W)\n",
    "                    y_coords.append(lm.y * H)\n",
    "                \n",
    "                if not x_coords: continue # Skip if no landmarks are found\n",
    "                \n",
    "                min_x, max_x = int(min(x_coords)), int(max(x_coords))\n",
    "                min_y, max_y = int(min(y_coords)), int(max(y_coords))\n",
    "\n",
    "                # Use a small fixed padding for these tiny ROIs\n",
    "                padding = 15\n",
    "                if feature_name == \"nose\" or feature_name == \"philtrum\":\n",
    "                    # Increase padding for the nose and philtrum to prevent harsh cutoffs and ensure better blending\n",
    "                    padding = 25 \n",
    "\n",
    "                x1 = max(0, min_x - padding)\n",
    "                y1 = max(0, min_y - padding)\n",
    "                x2 = min(W, max_x + padding)\n",
    "                y2 = min(H, max_y + padding)\n",
    "                \n",
    "                # Ensure ROI is valid\n",
    "                if x2 <= x1 or y2 <= y1: continue\n",
    "\n",
    "                # Extract the ROI from the original image\n",
    "                face_roi_area = img[y1:y2, x1:x2].copy()\n",
    "                \n",
    "                # --- DEBUG: Save the cropped ROI image for inspection ---\n",
    "                roi_debug_path = output_path.replace('.png', f'_{feature_name}_roi.png')\n",
    "                cv2.imwrite(roi_debug_path, face_roi_area)\n",
    "                print(f\"DEBUG: Saved ROI for {feature_name} to: {roi_debug_path}\")\n",
    "                # --- END DEBUG ---\n",
    "\n",
    "                face_roi_gray = cv2.cvtColor(face_roi_area, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # --- Selective CLAHE and Canny for High-Detail Features ---\n",
    "                \n",
    "                # Apply ADAPTIVE CONTRAST (CLAHE)\n",
    "                clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))\n",
    "                high_contrast_feature = clahe.apply(face_roi_gray)\n",
    "                \n",
    "                # Apply Gaussian Blur (kernel size adjusted slightly per feature type if needed, or stick to small (5, 5))\n",
    "                blurred_feature = cv2.GaussianBlur(high_contrast_feature, GAUSSIAN_BLUR_SIZES.get(feature_name, (5, 5)), 0)\n",
    "                \n",
    "                # --- APPLY DYNAMIC CANNY THRESHOLDS ---\n",
    "                low_thresh, high_thresh = CANNY_THRESHOLDS.get(feature_name, (50, 150))\n",
    "                feature_edges = cv2.Canny(blurred_feature, low_thresh, high_thresh)\n",
    "                # --- END DYNAMIC CANNY THRESHOLDS ---\n",
    "                \n",
    "                # Dilate the feature lines slightly\n",
    "                kernel_feature = np.ones((7, 7), np.uint8) \n",
    "                dilated_feature_edges = cv2.dilate(feature_edges, kernel_feature, iterations=1) \n",
    "                \n",
    "                # Invert the lines (black lines on white)\n",
    "                inverted_feature_edges = cv2.bitwise_not(dilated_feature_edges)\n",
    "\n",
    "                # Combine feature edges onto the final canvas (black lines (0) overwrite white (255))\n",
    "                current_canvas_roi = final_line_drawing[y1:y2, x1:x2]\n",
    "                final_line_drawing[y1:y2, x1:x2] = np.minimum(current_canvas_roi, inverted_feature_edges)\n",
    "\n",
    "     # --- 2. Selfie Segmentation for Hair and Clothing Outline (Minimal Detail) ---\n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as segmentor:\n",
    "        results_segmentation = segmentor.process(img_rgb)\n",
    "        \n",
    "        if results_segmentation.segmentation_mask is not None:\n",
    "            \n",
    "            # Create a clean binary mask for the whole person (including hair and clothing)\n",
    "            threshold = 0.5\n",
    "            person_mask = (results_segmentation.segmentation_mask > threshold).astype(np.uint8)\n",
    "            \n",
    "            # Convert the mask to an image (white person shape on black background)\n",
    "            person_outline_img = person_mask * 255 \n",
    "\n",
    "            # Strong blur to simplify the outline (this only captures the gross shape)\n",
    "            blurred_outline = cv2.GaussianBlur(person_outline_img, (61, 61), 0)\n",
    "\n",
    "            # Very broad Canny thresholds for minimal, loose outline lines\n",
    "            low_threshold_outline = 10\n",
    "            high_threshold_outline = 50\n",
    "         \n",
    "            \n",
    "            # Run Canny on the strongly blurred mask\n",
    "            outline_edges = cv2.Canny(blurred_outline, low_threshold_outline, high_threshold_outline)\n",
    "            \n",
    "            # Dilate to thicken the overall outline, making it the dominant silhouette\n",
    "            kernel_outline = np.ones((7, 7), np.uint8) \n",
    "            dilated_outline_edges = cv2.dilate(outline_edges, kernel_outline, iterations=2) \n",
    "            \n",
    "            # Invert the outline edges (black lines on white)\n",
    "            inverted_outline = cv2.bitwise_not(dilated_outline_edges)\n",
    "            \n",
    "            # Combine the outline onto the final canvas using a bitwise AND.\n",
    "            # This ensures that the low-detail outline complements the high-detail face,\n",
    "            # and that lines from both are preserved.\n",
    "            final_line_drawing = cv2.bitwise_and(final_line_drawing, inverted_outline)\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(output_path, final_line_drawing)\n",
    "    print(f\"Final selective line drawing saved to: {output_path}\")\n",
    "\n",
    "# Example call for normal execution\n",
    "image_to_minimal_line_drawing_enhanced('divija.png', 'selective_line_drawing.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "115a02dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting selective line drawing for: divija.png\n",
      "DEBUG: Saved ROI for eyes to: selective_line_drawing_eyes_roi.png\n",
      "DEBUG: Saved ROI for nose to: selective_line_drawing_nose_roi.png\n",
      "DEBUG: Saved ROI for mouth to: selective_line_drawing_mouth_roi.png\n",
      "DEBUG: Saved ROI for philtrum to: selective_line_drawing_philtrum_roi.png\n",
      "DEBUG: Saved ROI for eyebrows to: selective_line_drawing_eyebrows_roi.png\n",
      "Final selective line drawing saved to: selective_line_drawing.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760845705.839927 48225941 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760845705.840647 48254751 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760845705.844306 48254753 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1760845705.867318 48225941 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760845705.868016 48254771 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize MediaPipe solutions (all required imports in one place)\n",
    "# We need Face Mesh for precise feature ROI, and Selfie Segmentation for the body/hair outline.\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "# Feature groups for distinct processing (Eyes, Nose, Mouth).\n",
    "# Each group will be processed with its own tight bounding box.\n",
    "FEATURE_GROUPS = {\n",
    "    # Indices covering both eyes, excluding the face contour\n",
    "    \"eyes\": [33, 133, 163, 144, 246, 362, 263, 390, 373, 466, 130, 160, 243, 359, 387, 463], \n",
    "    \n",
    "    # INDICES UPDATED: Added points for better coverage of nostrils and the nose base.\n",
    "    \"nose\": [1, 6, 197, 2, 4, 5, 278, 275, 420, 129, 358, 64, 294, 98, 327, 118, 347], \n",
    "    \n",
    "    # Indices covering the lips and corners of the mouth\n",
    "    \"mouth\": [0, 17, 61, 291, 14, 308, 375, 40, 81, 311, 317, 415],\n",
    "    \n",
    "    # UPDATED GROUP: Simplified indices to reliably anchor the region between nose (2) and lips (13, 14).\n",
    "    # This creates a vertical bounding box for the philtrum.\n",
    "    \"philtrum\": [2, 13, 14, 172, 406],\n",
    "    \n",
    "    # Indices covering both left and right eyebrows\n",
    "    \"eyebrows\": [46, 53, 52, 65, 55, 70, 63, 105, 107, 276, 283, 282, 295, 285, 336, 296, 334, 293, 300],\n",
    "    \n",
    "    # NOTE: The 'ears' group has been removed per your request.\n",
    "}\n",
    "\n",
    "# Define Canny thresholds based on feature importance/noise level\n",
    "# Lower thresholds (e.g., 50, 120) capture more detail (high sensitivity)\n",
    "# Higher thresholds (e.g., 80, 200) capture only strong, clean edges (low sensitivity/less noise)\n",
    "CANNY_THRESHOLDS = {\n",
    "    \"eyes\": (50, 120),       # High detail for eyes\n",
    "    \"nose\": (50, 120),       # High detail for nose\n",
    "    \"mouth\": (50, 120),      # High detail for lips\n",
    "    \"philtrum\": (60, 120),   # Moderate detail for philtrum/creases\n",
    "    \"eyebrows\": (0, 40),     # Aggressive/Low-end settings to detect smooth outlines after heavy blur\n",
    "}\n",
    "\n",
    "# Define Gaussian Blur kernel sizes (must be odd numbers) based on required detail\n",
    "# Larger kernels (e.g., 31) remove more texture/noise, retaining only gross shape.\n",
    "# Smaller kernels (e.g., 11) retain finer detail.\n",
    "GAUSSIAN_BLUR_SIZES = {\n",
    "    \"eyes\": (13, 13),\n",
    "    \"nose\": (11, 11),\n",
    "    \"mouth\": (11, 11),\n",
    "    \"philtrum\": (31, 31),\n",
    "    \"eyebrows\": (31, 31),\n",
    "}\n",
    "\n",
    "\n",
    "def image_to_minimal_line_drawing_enhanced(image_path, output_path):\n",
    "    \"\"\"\n",
    "    Creates a highly stylized line drawing by applying CLAHE and Canny edge detection \n",
    "    independently to multiple small facial feature ROIs, then adding a minimal \n",
    "    outline for the hair and clothing.\n",
    "    \"\"\"\n",
    "    print(f\"Starting selective line drawing for: {image_path}\")\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    H, W, _ = img.shape\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Initialize the final line drawing canvas: A white background (255)\n",
    "    final_line_drawing = np.ones((H, W), dtype=np.uint8) * 255 \n",
    "    \n",
    "    # --- 1. Face Mesh for Multiple Feature ROIs and CLAHE ---\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True, \n",
    "        max_num_faces=1, \n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        \n",
    "        results_face = face_mesh.process(img_rgb)\n",
    "        \n",
    "        if results_face.multi_face_landmarks:\n",
    "            \n",
    "            # Use the first detected face\n",
    "            face_landmarks = results_face.multi_face_landmarks[0]\n",
    "            \n",
    "            # Loop over each feature group to process them individually\n",
    "            for feature_name, indices in FEATURE_GROUPS.items():\n",
    "                x_coords = []\n",
    "                y_coords = []\n",
    "                \n",
    "                # Calculate a tight bounding box *only* around the selected feature group\n",
    "                for index in indices:\n",
    "                    lm = face_landmarks.landmark[index]\n",
    "                    x_coords.append(lm.x * W)\n",
    "                    y_coords.append(lm.y * H)\n",
    "                \n",
    "                if not x_coords: continue # Skip if no landmarks are found\n",
    "                \n",
    "                min_x, max_x = int(min(x_coords)), int(max(x_coords))\n",
    "                min_y, max_y = int(min(y_coords)), int(max(y_coords))\n",
    "\n",
    "                # Use a small fixed padding for these tiny ROIs\n",
    "                padding = 15\n",
    "                if feature_name == \"nose\" or feature_name == \"philtrum\":\n",
    "                    # Increase padding for the nose and philtrum to prevent harsh cutoffs and ensure better blending\n",
    "                    padding = 25 \n",
    "\n",
    "                x1 = max(0, min_x - padding)\n",
    "                y1 = max(0, min_y - padding)\n",
    "                x2 = min(W, max_x + padding)\n",
    "                y2 = min(H, max_y + padding)\n",
    "                \n",
    "                # Ensure ROI is valid\n",
    "                if x2 <= x1 or y2 <= y1: continue\n",
    "\n",
    "                # Extract the ROI from the original image\n",
    "                face_roi_area = img[y1:y2, x1:x2].copy()\n",
    "                \n",
    "                # --- DEBUG: Save the cropped ROI image for inspection ---\n",
    "                roi_debug_path = output_path.replace('.png', f'_{feature_name}_roi.png')\n",
    "                cv2.imwrite(roi_debug_path, face_roi_area)\n",
    "                print(f\"DEBUG: Saved ROI for {feature_name} to: {roi_debug_path}\")\n",
    "                # --- END DEBUG ---\n",
    "\n",
    "                face_roi_gray = cv2.cvtColor(face_roi_area, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # --- Selective CLAHE and Canny for High-Detail Features ---\n",
    "                \n",
    "                # Apply ADAPTIVE CONTRAST (CLAHE)\n",
    "                clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))\n",
    "                high_contrast_feature = clahe.apply(face_roi_gray)\n",
    "                \n",
    "                # Apply Dynamic Gaussian Blur\n",
    "                blur_size = GAUSSIAN_BLUR_SIZES.get(feature_name, (5, 5))\n",
    "                blurred_feature = cv2.GaussianBlur(high_contrast_feature, blur_size, 0)\n",
    "                \n",
    "                # --- APPLY DYNAMIC CANNY THRESHOLDS ---\n",
    "                low_thresh, high_thresh = CANNY_THRESHOLDS.get(feature_name, (50, 150))\n",
    "                feature_edges = cv2.Canny(blurred_feature, low_thresh, high_thresh)\n",
    "                # --- END DYNAMIC CANNY THRESHOLDS ---\n",
    "                \n",
    "                # Dilate the feature lines slightly\n",
    "                kernel_feature = np.ones((3, 3), np.uint8) \n",
    "                dilated_feature_edges = cv2.dilate(feature_edges, kernel_feature, iterations=1) \n",
    "                \n",
    "                # Invert the lines (black lines on white)\n",
    "                inverted_feature_edges = cv2.bitwise_not(dilated_feature_edges)\n",
    "\n",
    "                # Combine feature edges onto the final canvas (black lines (0) overwrite white (255))\n",
    "                current_canvas_roi = final_line_drawing[y1:y2, x1:x2]\n",
    "                final_line_drawing[y1:y2, x1:x2] = np.minimum(current_canvas_roi, inverted_feature_edges)\n",
    "\n",
    "    # --- 2. Selfie Segmentation for Hair, Clothing, and Hair/Face Boundary Outline (Inner Edges) ---\n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as segmentor:\n",
    "        results_segmentation = segmentor.process(img_rgb)\n",
    "        \n",
    "        if results_segmentation.segmentation_mask is not None:\n",
    "            \n",
    "            # Create a clean version of the person on a white background (BGR)\n",
    "            threshold = 0.5\n",
    "            person_mask = results_segmentation.segmentation_mask > threshold\n",
    "            mask_3channel = np.stack((person_mask,) * 3, axis=-1)\n",
    "            \n",
    "            background_color = [255, 255, 255] \n",
    "            solid_background = np.full(img.shape, background_color, dtype=np.uint8)\n",
    "            # Isolate the original image of the person against a white background\n",
    "            person_on_white = np.where(mask_3channel, img, solid_background)\n",
    "            \n",
    "            # --- COLOR-BASED EDGE DETECTION (HSV Saturation Channel) ---\n",
    "            # Convert to HSV to better isolate hair/skin color contrast\n",
    "            person_hsv = cv2.cvtColor(person_on_white, cv2.COLOR_BGR2HSV)\n",
    "            \n",
    "            # Use the Saturation channel (index 1), which typically has low values for skin \n",
    "            # and higher values for hair/clothing/background, providing a strong boundary contrast.\n",
    "            person_saturation = person_hsv[:, :, 1]\n",
    "\n",
    "            # Apply a lighter blur (9, 9) to smooth noise while retaining the hair boundary edge.\n",
    "            blurred_person = cv2.GaussianBlur(person_saturation, (9, 9), 0)\n",
    "\n",
    "            # Use Canny on the Saturation channel with thresholds tuned for color contrast\n",
    "            low_threshold_outline = 40 \n",
    "            high_threshold_outline = 120 \n",
    "            \n",
    "            outline_edges = cv2.Canny(blurred_person, low_threshold_outline, high_threshold_outline)\n",
    "            \n",
    "            # --- CONTOUR FILTERING HEURISTIC (NEW) ---\n",
    "            # Invert the Canny output temporarily for contour finding (contours look for white on black)\n",
    "            # We don't need a full inversion here since Canny output is effectively a binary image\n",
    "            \n",
    "            # Find contours (lines)\n",
    "            contours, _ = cv2.findContours(outline_edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            # Create a blank white canvas for the filtered outline\n",
    "            filtered_outline = np.ones((H, W), dtype=np.uint8) * 255\n",
    "            \n",
    "            # Heuristic: Filter by area to keep only the prominent, continuous lines \n",
    "            # (main silhouette, hair edge, clothing folds) and discard small noise.\n",
    "            min_outline_area = 100 # Adjust this threshold based on image resolution if needed\n",
    "\n",
    "            for contour in contours:\n",
    "                if cv2.contourArea(contour) > min_outline_area:\n",
    "                    # Draw the long continuous lines onto the filtered canvas (in black, thickness 1 for thin lines)\n",
    "                    cv2.drawContours(filtered_outline, [contour], -1, 0, 1) \n",
    "            \n",
    "            # The filtered_outline now contains the continuous hair/clothing lines (black on white)\n",
    "            \n",
    "            # Combine the outline onto the final canvas, preserving all lines.\n",
    "            final_line_drawing = np.minimum(final_line_drawing, filtered_outline)\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(output_path, final_line_drawing)\n",
    "    print(f\"Final selective line drawing saved to: {output_path}\")\n",
    "\n",
    "# Example call for normal execution\n",
    "image_to_minimal_line_drawing_enhanced('divija.png', 'selective_line_drawing.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "865dd9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting selective line drawing for: divija.png\n",
      "DEBUG: Saved ROI for eyes to: selective_line_drawing_eyes_roi.png\n",
      "DEBUG: Saved ROI for nose to: selective_line_drawing_nose_roi.png\n",
      "DEBUG: Saved ROI for mouth to: selective_line_drawing_mouth_roi.png\n",
      "DEBUG: Saved ROI for philtrum to: selective_line_drawing_philtrum_roi.png\n",
      "DEBUG: Saved ROI for eyebrows to: selective_line_drawing_eyebrows_roi.png\n",
      "Final selective line drawing saved to: selective_line_drawing.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760846697.619743 48225941 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760846697.621959 48271852 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760846697.625715 48271854 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1760846697.652031 48225941 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760846697.653263 48271866 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "# Feature groups for distinct processing (Eyes, Nose, Mouth).\n",
    "# Each group will be processed with its own tight bounding box.\n",
    "FEATURE_GROUPS = {\n",
    "    # Indices covering both eyes, excluding the face contour\n",
    "    \"eyes\": [33, 133, 163, 144, 246, 362, 263, 390, 373, 466, 130, 160, 243, 359, 387, 463], \n",
    "    \n",
    "    # INDICES UPDATED: Added points for better coverage of nostrils and the nose base.\n",
    "    \"nose\": [1, 6, 197, 2, 4, 5, 278, 275, 420, 129, 358, 64, 294, 98, 327, 118, 347], \n",
    "    \n",
    "    # Indices covering the lips and corners of the mouth\n",
    "    \"mouth\": [0, 17, 61, 291, 14, 308, 375, 40, 81, 311, 317, 415],\n",
    "    \n",
    "    # UPDATED GROUP: Simplified indices to reliably anchor the region between nose (2) and lips (13, 14).\n",
    "    # This creates a vertical bounding box for the philtrum.\n",
    "    \"philtrum\": [2, 13, 14, 172, 406],\n",
    "    \n",
    "    # Indices covering both left and right eyebrows\n",
    "    \"eyebrows\": [46, 53, 52, 65, 55, 70, 63, 105, 107, 276, 283, 282, 295, 285, 336, 296, 334, 293, 300],\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "# Define Canny thresholds based on feature importance/noise level\n",
    "# Lower thresholds (e.g., 50, 150) capture more detail (high sensitivity)\n",
    "# Higher thresholds (e.g., 80, 200) capture only strong, clean edges (low sensitivity/less noise)\n",
    "CANNY_THRESHOLDS = {\n",
    "    \"eyes\": (50, 120),       # High detail for eyes\n",
    "    \"nose\": (50, 120),       # High detail for nose\n",
    "    \"mouth\": (50, 120),      # High detail for lips\n",
    "    \"philtrum\": (60, 120),   # Moderate detail for philtrum/creases\n",
    "    \"eyebrows\": (0, 40),   # Low detail/Less noise for eyebrows\n",
    "}\n",
    "GAUSSIAN_BLUR_SIZES = {\n",
    "    \"eyes\": (13, 13),\n",
    "    \"nose\": (11, 11),\n",
    "    \"mouth\": (11, 11),\n",
    "    \"philtrum\": (31, 31),\n",
    "    \"eyebrows\": (61, 61),\n",
    "}\n",
    "\n",
    "def image_to_minimal_line_drawing_enhanced(image_path, output_path):\n",
    "    \"\"\"\n",
    "    Creates a highly stylized line drawing by applying CLAHE and Canny edge detection \n",
    "    independently to multiple small facial feature ROIs, then adding a minimal \n",
    "    outline for the hair and clothing.\n",
    "    \"\"\"\n",
    "    print(f\"Starting selective line drawing for: {image_path}\")\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    H, W, _ = img.shape\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Initialize the final line drawing canvas: A white background (255)\n",
    "    final_line_drawing = np.ones((H, W), dtype=np.uint8) * 255 \n",
    "    \n",
    "    # --- 1. Face Mesh for Multiple Feature ROIs and CLAHE ---\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True, \n",
    "        max_num_faces=1, \n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        \n",
    "        results_face = face_mesh.process(img_rgb)\n",
    "        \n",
    "        if results_face.multi_face_landmarks:\n",
    "            \n",
    "            # Use the first detected face\n",
    "            face_landmarks = results_face.multi_face_landmarks[0]\n",
    "            \n",
    "            # Loop over each feature group to process them individually\n",
    "            for feature_name, indices in FEATURE_GROUPS.items():\n",
    "                x_coords = []\n",
    "                y_coords = []\n",
    "                \n",
    "                # Calculate a tight bounding box *only* around the selected feature group\n",
    "                for index in indices:\n",
    "                    lm = face_landmarks.landmark[index]\n",
    "                    x_coords.append(lm.x * W)\n",
    "                    y_coords.append(lm.y * H)\n",
    "                \n",
    "                if not x_coords: continue # Skip if no landmarks are found\n",
    "                \n",
    "                min_x, max_x = int(min(x_coords)), int(max(x_coords))\n",
    "                min_y, max_y = int(min(y_coords)), int(max(y_coords))\n",
    "\n",
    "                # Use a small fixed padding for these tiny ROIs\n",
    "                padding = 15\n",
    "                if feature_name == \"nose\" or feature_name == \"philtrum\":\n",
    "                    # Increase padding for the nose and philtrum to prevent harsh cutoffs and ensure better blending\n",
    "                    padding = 25 \n",
    "\n",
    "                x1 = max(0, min_x - padding)\n",
    "                y1 = max(0, min_y - padding)\n",
    "                x2 = min(W, max_x + padding)\n",
    "                y2 = min(H, max_y + padding)\n",
    "                \n",
    "                # Ensure ROI is valid\n",
    "                if x2 <= x1 or y2 <= y1: continue\n",
    "\n",
    "                # Extract the ROI from the original image\n",
    "                face_roi_area = img[y1:y2, x1:x2].copy()\n",
    "                \n",
    "                # --- DEBUG: Save the cropped ROI image for inspection ---\n",
    "                roi_debug_path = output_path.replace('.png', f'_{feature_name}_roi.png')\n",
    "                cv2.imwrite(roi_debug_path, face_roi_area)\n",
    "                print(f\"DEBUG: Saved ROI for {feature_name} to: {roi_debug_path}\")\n",
    "                # --- END DEBUG ---\n",
    "\n",
    "                face_roi_gray = cv2.cvtColor(face_roi_area, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # --- Selective CLAHE and Canny for High-Detail Features ---\n",
    "                \n",
    "                # Apply ADAPTIVE CONTRAST (CLAHE)\n",
    "                clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))\n",
    "                high_contrast_feature = clahe.apply(face_roi_gray)\n",
    "                \n",
    "                # Apply Gaussian Blur (kernel size adjusted slightly per feature type if needed, or stick to small (5, 5))\n",
    "                blurred_feature = cv2.GaussianBlur(high_contrast_feature, GAUSSIAN_BLUR_SIZES.get(feature_name, (5, 5)), 0)\n",
    "                \n",
    "                # --- APPLY DYNAMIC CANNY THRESHOLDS ---\n",
    "                low_thresh, high_thresh = CANNY_THRESHOLDS.get(feature_name, (50, 150))\n",
    "                feature_edges = cv2.Canny(blurred_feature, low_thresh, high_thresh)\n",
    "                # --- END DYNAMIC CANNY THRESHOLDS ---\n",
    "                \n",
    "                # Dilate the feature lines slightly\n",
    "                kernel_feature = np.ones((7, 7), np.uint8) \n",
    "                dilated_feature_edges = cv2.dilate(feature_edges, kernel_feature, iterations=1) \n",
    "                \n",
    "                # Invert the lines (black lines on white)\n",
    "                inverted_feature_edges = cv2.bitwise_not(dilated_feature_edges)\n",
    "\n",
    "                # Combine feature edges onto the final canvas (black lines (0) overwrite white (255))\n",
    "                current_canvas_roi = final_line_drawing[y1:y2, x1:x2]\n",
    "                final_line_drawing[y1:y2, x1:x2] = np.minimum(current_canvas_roi, inverted_feature_edges)\n",
    "\n",
    " # --- 2. Selfie Segmentation for Hair and Clothing Outline ---\n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as segmentor:\n",
    "        results_segmentation = segmentor.process(img_rgb)\n",
    "        \n",
    "        if results_segmentation.segmentation_mask is not None:\n",
    "            \n",
    "            # Create a clean version of the person on a white background (BGR)\n",
    "            threshold = 0.5\n",
    "            person_mask = results_segmentation.segmentation_mask > threshold\n",
    "            mask_3channel = np.stack((person_mask,) * 3, axis=-1)\n",
    "            \n",
    "            background_color = [255, 255, 255] \n",
    "            solid_background = np.full(img.shape, background_color, dtype=np.uint8)\n",
    "            # Isolate the original image of the person against a white background\n",
    "            person_on_white = np.where(mask_3channel, img, solid_background)\n",
    "            \n",
    "            # ----------------------------------------------------------------------\n",
    "            # 2A. OUTERMOST SILHOUETTE (Simple Shape Detection - Grey Channel Logic)\n",
    "            # ----------------------------------------------------------------------\n",
    "            \n",
    "            # Use the segmentation mask image (white on black)\n",
    "            person_outline_img = person_mask.astype(np.uint8) * 255 \n",
    "\n",
    "            # Strong blur to simplify the outline (this only captures the gross shape)\n",
    "            blurred_outline_simple = cv2.GaussianBlur(person_outline_img, (61, 61), 0)\n",
    "\n",
    "            # Very broad Canny thresholds for minimal, loose outline lines\n",
    "            low_threshold_outline_simple = 10\n",
    "            high_threshold_outline_simple = 50 \n",
    "            \n",
    "            # Run Canny on the strongly blurred mask\n",
    "            outline_edges_simple = cv2.Canny(blurred_outline_simple, low_threshold_outline_simple, high_threshold_outline_simple)\n",
    "            \n",
    "            # Light dilation to ensure the line is continuous\n",
    "            kernel_outline_simple = np.ones((5, 5), np.uint8) \n",
    "            dilated_outline_edges_simple = cv2.dilate(outline_edges_simple, kernel_outline_simple, iterations=1) \n",
    "            \n",
    "            # Invert the outline edges (black lines on white)\n",
    "            inverted_outline_simple = cv2.bitwise_not(dilated_outline_edges_simple)\n",
    "            \n",
    "            # Combine the simple outline onto the final canvas\n",
    "            final_line_drawing = np.minimum(final_line_drawing, inverted_outline_simple)\n",
    "            \n",
    "            # ----------------------------------------------------------------------\n",
    "            # 2B. INNER DETAILS (Hair/Face Boundary & Clothing Folds - HSV Saturation/Contour Logic)\n",
    "            # ----------------------------------------------------------------------\n",
    "            \n",
    "            # Convert to HSV to better isolate hair/skin color contrast\n",
    "            person_hsv = cv2.cvtColor(person_on_white, cv2.COLOR_BGR2HSV)\n",
    "            \n",
    "            # Use the Saturation channel (index 1) for strong color boundaries\n",
    "            person_saturation = person_hsv[:, :, 1]\n",
    "\n",
    "            # Apply a lighter blur to smooth noise while retaining the hair boundary edge.\n",
    "            blurred_person_inner = cv2.GaussianBlur(person_saturation, (9, 9), 0)\n",
    "\n",
    "            # Use Canny on the Saturation channel with thresholds tuned for color contrast\n",
    "            low_threshold_inner = 40 \n",
    "            high_threshold_inner = 120 \n",
    "            \n",
    "            outline_edges_inner = cv2.Canny(blurred_person_inner, low_threshold_inner, high_threshold_inner)\n",
    "            \n",
    "            # --- CONTOUR FILTERING HEURISTIC ---\n",
    "            # Find contours (lines)\n",
    "            contours_inner, _ = cv2.findContours(outline_edges_inner, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            # Create a blank white canvas for the filtered outline\n",
    "            filtered_outline_inner = np.ones((H, W), dtype=np.uint8) * 255\n",
    "            \n",
    "            # Heuristic: Filter by area to keep only the prominent, continuous lines \n",
    "            min_outline_area = 100 \n",
    "\n",
    "            for contour in contours_inner:\n",
    "                if cv2.contourArea(contour) > min_outline_area:\n",
    "                    # Draw the long continuous lines onto the filtered canvas (in black, thickness 1 for thin lines)\n",
    "                    cv2.drawContours(filtered_outline_inner, [contour], -1, 0, 1) \n",
    "            \n",
    "            # Combine the inner details onto the final canvas, preserving all lines.\n",
    "            final_line_drawing = np.minimum(final_line_drawing, filtered_outline_inner)\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(output_path, final_line_drawing)\n",
    "    print(f\"Final selective line drawing saved to: {output_path}\")\n",
    "\n",
    "# Example call for normal execution\n",
    "image_to_minimal_line_drawing_enhanced('divija.png', 'selective_line_drawing.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a4699b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting selective line drawing for: divija.png\n",
      "DEBUG: Saved ROI for eyes to: selective_line_drawing_eyes_roi.png\n",
      "DEBUG: Saved ROI for nose to: selective_line_drawing_nose_roi.png\n",
      "DEBUG: Saved ROI for mouth to: selective_line_drawing_mouth_roi.png\n",
      "DEBUG: Saved ROI for philtrum to: selective_line_drawing_philtrum_roi.png\n",
      "DEBUG: Saved ROI for eyebrows to: selective_line_drawing_eyebrows_roi.png\n",
      "Final selective line drawing saved to: selective_line_drawing.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760848373.956629 48225941 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760848373.957371 48300509 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760848373.961134 48300506 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1760848373.986556 48225941 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760848373.987805 48300520 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "# Feature groups for distinct processing (Eyes, Nose, Mouth).\n",
    "# Each group will be processed with its own tight bounding box.\n",
    "FEATURE_GROUPS = {\n",
    "    # Indices covering both eyes, excluding the face contour\n",
    "    \"eyes\": [33, 133, 163, 144, 246, 362, 263, 390, 373, 466, 130, 160, 243, 359, 387, 463], \n",
    "    \n",
    "    # INDICES UPDATED: Added points for better coverage of nostrils and the nose base.\n",
    "    \"nose\": [1, 6, 197, 2, 4, 5, 278, 275, 420, 129, 358, 64, 294, 98, 327, 118, 347], \n",
    "    \n",
    "    # Indices covering the lips and corners of the mouth\n",
    "    \"mouth\": [0, 17, 61, 291, 14, 308, 375, 40, 81, 311, 317, 415],\n",
    "    \n",
    "    # UPDATED GROUP: Simplified indices to reliably anchor the region between nose (2) and lips (13, 14).\n",
    "    # This creates a vertical bounding box for the philtrum.\n",
    "    \"philtrum\": [2, 13, 14, 172, 406, ],\n",
    "    \n",
    "    # Indices covering both left and right eyebrows\n",
    "    \"eyebrows\": [46, 53, 52, 65, 55, 70, 63, 105, 107, 276, 283, 282, 295, 285, 336, 296, 334, 293, 300],\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "# Define Canny thresholds based on feature importance/noise level\n",
    "# Lower thresholds (e.g., 50, 150) capture more detail (high sensitivity)\n",
    "# Higher thresholds (e.g., 80, 200) capture only strong, clean edges (low sensitivity/less noise)\n",
    "CANNY_THRESHOLDS = {\n",
    "    \"eyes\": (50, 120),       # High detail for eyes\n",
    "    \"nose\": (50, 120),       # High detail for nose\n",
    "    \"mouth\": (50, 120),      # High detail for lips\n",
    "    \"philtrum\": (60, 120),   # Moderate detail for philtrum/creases\n",
    "    \"eyebrows\": (10, 30),   # Low detail/Less noise for eyebrows\n",
    "}\n",
    "GAUSSIAN_BLUR_SIZES = {\n",
    "    \"eyes\": (13, 13),\n",
    "    \"nose\": (11, 11),\n",
    "    \"mouth\": (11, 11),\n",
    "    \"philtrum\": (31, 31),\n",
    "    \"eyebrows\": (31, 31),\n",
    "}\n",
    "\n",
    "def image_to_minimal_line_drawing_enhanced(image_path, output_path):\n",
    "    \"\"\"\n",
    "    Creates a highly stylized line drawing by applying CLAHE and Canny edge detection \n",
    "    independently to multiple small facial feature ROIs, then adding a minimal \n",
    "    outline for the hair and clothing.\n",
    "    \"\"\"\n",
    "    print(f\"Starting selective line drawing for: {image_path}\")\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    H, W, _ = img.shape\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Initialize the final line drawing canvas: A white background (255)\n",
    "    final_line_drawing = np.ones((H, W), dtype=np.uint8) * 255 \n",
    "    \n",
    "    # --- 1. Face Mesh for Multiple Feature ROIs and CLAHE ---\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True, \n",
    "        max_num_faces=1, \n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        \n",
    "        results_face = face_mesh.process(img_rgb)\n",
    "        \n",
    "        if results_face.multi_face_landmarks:\n",
    "            \n",
    "            # Use the first detected face\n",
    "            face_landmarks = results_face.multi_face_landmarks[0]\n",
    "            \n",
    "            # Loop over each feature group to process them individually\n",
    "            for feature_name, indices in FEATURE_GROUPS.items():\n",
    "                x_coords = []\n",
    "                y_coords = []\n",
    "                \n",
    "                # Calculate a tight bounding box *only* around the selected feature group\n",
    "                for index in indices:\n",
    "                    lm = face_landmarks.landmark[index]\n",
    "                    x_coords.append(lm.x * W)\n",
    "                    y_coords.append(lm.y * H)\n",
    "                \n",
    "                if not x_coords: continue # Skip if no landmarks are found\n",
    "                \n",
    "                min_x, max_x = int(min(x_coords)), int(max(x_coords))\n",
    "                min_y, max_y = int(min(y_coords)), int(max(y_coords))\n",
    "\n",
    "                # Use a small fixed padding for these tiny ROIs\n",
    "                padding = 15\n",
    "                if feature_name == \"nose\" or feature_name == \"philtrum\":\n",
    "                    # Increase padding for the nose and philtrum to prevent harsh cutoffs and ensure better blending\n",
    "                    padding = 25 \n",
    "\n",
    "                x1 = max(0, min_x - padding)\n",
    "                y1 = max(0, min_y - padding)\n",
    "                x2 = min(W, max_x + padding)\n",
    "                y2 = min(H, max_y + padding)\n",
    "                \n",
    "                # Ensure ROI is valid\n",
    "                if x2 <= x1 or y2 <= y1: continue\n",
    "\n",
    "                # Extract the ROI from the original image\n",
    "                face_roi_area = img[y1:y2, x1:x2].copy()\n",
    "                \n",
    "                # --- DEBUG: Save the cropped ROI image for inspection ---\n",
    "                roi_debug_path = output_path.replace('.png', f'_{feature_name}_roi.png')\n",
    "                cv2.imwrite(roi_debug_path, face_roi_area)\n",
    "                print(f\"DEBUG: Saved ROI for {feature_name} to: {roi_debug_path}\")\n",
    "                # --- END DEBUG ---\n",
    "\n",
    "                face_roi_gray = cv2.cvtColor(face_roi_area, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # --- Selective CLAHE and Canny for High-Detail Features ---\n",
    "                \n",
    "                # Apply ADAPTIVE CONTRAST (CLAHE)\n",
    "                clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))\n",
    "                high_contrast_feature = clahe.apply(face_roi_gray)\n",
    "                \n",
    "                # Apply Gaussian Blur (kernel size adjusted slightly per feature type if needed, or stick to small (5, 5))\n",
    "                blurred_feature = cv2.GaussianBlur(high_contrast_feature, GAUSSIAN_BLUR_SIZES.get(feature_name, (5, 5)), 0)\n",
    "                \n",
    "                # --- APPLY DYNAMIC CANNY THRESHOLDS ---\n",
    "                low_thresh, high_thresh = CANNY_THRESHOLDS.get(feature_name, (50, 150))\n",
    "                feature_edges = cv2.Canny(blurred_feature, low_thresh, high_thresh)\n",
    "                # --- END DYNAMIC CANNY THRESHOLDS ---\n",
    "                \n",
    "                # Dilate the feature lines slightly\n",
    "                kernel_feature = np.ones((7, 7), np.uint8) \n",
    "                dilated_feature_edges = cv2.dilate(feature_edges, kernel_feature, iterations=1) \n",
    "                \n",
    "                # Invert the lines (black lines on white)\n",
    "                inverted_feature_edges = cv2.bitwise_not(dilated_feature_edges)\n",
    "\n",
    "                # Combine feature edges onto the final canvas (black lines (0) overwrite white (255))\n",
    "                current_canvas_roi = final_line_drawing[y1:y2, x1:x2]\n",
    "                final_line_drawing[y1:y2, x1:x2] = np.minimum(current_canvas_roi, inverted_feature_edges)\n",
    "\n",
    "    # --- 2. Selfie Segmentation for Hair and Clothing Outline (Simplified Mask Approach) ---\n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as segmentor:\n",
    "        results_segmentation = segmentor.process(img_rgb)\n",
    "        \n",
    "        if results_segmentation.segmentation_mask is not None:\n",
    "            \n",
    "            # --- SIMPLIFIED OUTER OUTLINE (Based on Mask Only) ---\n",
    "            threshold = 0.5\n",
    "            person_mask = results_segmentation.segmentation_mask > threshold\n",
    "            \n",
    "            # 1. Isolate the mask image (white person shape on black background)\n",
    "            person_outline_img = person_mask.astype(np.uint8) * 255 \n",
    "\n",
    "            # 2. Strong blur to greatly simplify the outline (captures only the gross shape)\n",
    "            blurred_outline = cv2.GaussianBlur(person_outline_img, (61, 61), 0)\n",
    "\n",
    "            # 3. Broad Canny thresholds to find the edge of the highly blurred shape\n",
    "            low_threshold_outline = 10\n",
    "            high_threshold_outline = 50 \n",
    "            \n",
    "            outline_edges = cv2.Canny(blurred_outline, low_threshold_outline, high_threshold_outline)\n",
    "            \n",
    "            # 4. Dilate to thicken and ensure continuity\n",
    "            kernel_outline = np.ones((5, 5), np.uint8) \n",
    "            dilated_outline_edges = cv2.dilate(outline_edges, kernel_outline, iterations=1) \n",
    "            \n",
    "            # 5. Invert and combine\n",
    "            inverted_outline = cv2.bitwise_not(dilated_outline_edges)\n",
    "            \n",
    "            # Combine the simple outline onto the final canvas\n",
    "            final_line_drawing = np.minimum(final_line_drawing, inverted_outline)\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(output_path, final_line_drawing)\n",
    "    print(f\"Final selective line drawing saved to: {output_path}\")\n",
    "\n",
    "# Example call for normal execution\n",
    "image_to_minimal_line_drawing_enhanced('divija.png', 'selective_line_drawing.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c84edbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ALL landmark visualization for: divija.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760848369.921656 48225941 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760848369.923357 48300438 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760848369.927472 48300441 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/Users/divija/Divi Drive/workplace/Princeton/Sem 5/Carlab/carlab_2025/final_project/carlab/lib/python3.12/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full landmark visualization saved to: divija_all_features_debug.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe solutions (only Face Mesh is needed for visualization)\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "\n",
    "def visualize_all_landmarks(image_path, output_path):\n",
    "    \"\"\"\n",
    "    Loads an image, processes it with MediaPipe Face Mesh, and draws colored \n",
    "    markers and their indices for ALL 468 detected landmarks.\n",
    "    \"\"\"\n",
    "    print(f\"Starting ALL landmark visualization for: {image_path}\")\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    H, W, _ = img.shape\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create a copy of the original image to draw on\n",
    "    debug_image = img.copy()\n",
    "\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True, \n",
    "        max_num_faces=1, \n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        \n",
    "        results = face_mesh.process(img_rgb)\n",
    "        \n",
    "        if results.multi_face_landmarks:\n",
    "            face_landmarks = results.multi_face_landmarks[0]\n",
    "            \n",
    "            # Iterate through ALL 468 landmarks\n",
    "            for i, lm in enumerate(face_landmarks.landmark):\n",
    "                # Convert normalized coordinates to pixel coordinates\n",
    "                x = int(lm.x * W)\n",
    "                y = int(lm.y * H)\n",
    "                \n",
    "                # Draw a small filled circle (landmark point) in white\n",
    "                cv2.circle(debug_image, (x, y), 1, (255, 255, 255), -1)\n",
    "                \n",
    "                # Draw the index number in yellow for visibility\n",
    "                cv2.putText(debug_image, str(i), (x + 2, y + 2), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 255), 1)\n",
    "                    \n",
    "        else:\n",
    "            print(\"Warning: Face landmarks not detected in the image.\")\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(output_path, debug_image)\n",
    "    print(f\"Full landmark visualization saved to: {output_path}\")\n",
    "\n",
    "# Example call\n",
    "visualize_all_landmarks('divija.png', 'divija_all_features_debug.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "837f7508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image from: selective_line_drawing_eyebrows_roi.png\n",
      "Preprocessed output saved to: eyebrow_preprocessed_output.png\n"
     ]
    }
   ],
   "source": [
    "def preprocess_eyebrow_roi(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Loads the eyebrow ROI, converts it to grayscale, applies CLAHE for contrast, \n",
    "    and strong Gaussian blur for smoothing, preparing it for clean Canny detection.\n",
    "    \"\"\"\n",
    "    # 1. Load the image\n",
    "    img = cv2.imread(input_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image at {input_path}. Ensure you have run the main script once to generate this file.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loaded image from: {input_path}\")\n",
    "    \n",
    "    # 2. Convert to grayscale (required for CLAHE/Canny)\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 3. Apply CLAHE (Adaptive Contrast)\n",
    "    # This boosts local contrast, helping separate the eyebrow outline from the skin.\n",
    "    clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))\n",
    "    high_contrast_img = clahe.apply(gray_img) \n",
    "\n",
    "    # 4. Apply STRONG Gaussian Blur\n",
    "    # The (31, 31) kernel size is used to destroy internal texture/noise, leaving\n",
    "    # only the smooth, gross shape of the eyebrow for Canny to detect.\n",
    "    blur_kernel = (61, 61)\n",
    "    blurred_img = cv2.GaussianBlur(gray_img, blur_kernel, 0)\n",
    "    equalized_img = cv2.equalizeHist(blurred_img)\n",
    "    equalized_img = cv2.equalizeHist(equalized_img)\n",
    "\n",
    "    edges = cv2.Canny(equalized_img, 0, 40)\n",
    "    inverted_edges = cv2.bitwise_not(edges)\n",
    "    # 5. Save the preprocessed image\n",
    "    cv2.imwrite(output_path, inverted_edges)\n",
    "    print(f\"Preprocessed output saved to: {output_path}\")\n",
    "\n",
    "\n",
    "# --- Test Execution ---\n",
    "# NOTE: This file must exist after running the main line drawing script!\n",
    "INPUT_FILE = 'selective_line_drawing_eyebrows_roi.png'\n",
    "OUTPUT_FILE = 'eyebrow_preprocessed_output.png'\n",
    "\n",
    "preprocess_eyebrow_roi(INPUT_FILE, OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c19f764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting selective line drawing for: divija.png\n",
      "DEBUG: Saved ROI for eyes to: selective_line_drawing_eyes_roi.png\n",
      "DEBUG: Saved ROI for eyes to: selective_line_drawing_eyes_roi.png\n",
      "DEBUG: Saved ROI for nose to: selective_line_drawing_nose_roi.png\n",
      "DEBUG: Saved ROI for nose to: selective_line_drawing_nose_roi.png\n",
      "DEBUG: Saved ROI for mouth to: selective_line_drawing_mouth_roi.png\n",
      "DEBUG: Saved ROI for mouth to: selective_line_drawing_mouth_roi.png\n",
      "DEBUG: Saved ROI for philtrum to: selective_line_drawing_philtrum_roi.png\n",
      "DEBUG: Saved ROI for philtrum to: selective_line_drawing_philtrum_roi.png\n",
      "DEBUG: Saved ROI for eyebrows to: selective_line_drawing_eyebrows_roi.png\n",
      "DEBUG: Saved ROI for eyebrows to: selective_line_drawing_eyebrows_roi.png\n",
      "Final selective line drawing saved to: selective_line_drawing.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760851599.980152 48348156 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760851599.980956 48356431 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760851599.984671 48356432 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1760851600.014032 48348156 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760851600.014756 48356445 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "# Feature groups for distinct processing (Eyes, Nose, Mouth).\n",
    "# Each group will be processed with its own tight bounding box.\n",
    "FEATURE_GROUPS = {\n",
    "    # Indices covering both eyes, excluding the face contour\n",
    "    \"eyes\": [33, 133, 163, 144, 246, 362, 263, 390, 373, 466, 130, 160, 243, 359, 387, 463, 224], \n",
    "    \n",
    "    # INDICES UPDATED: Added points for better coverage of nostrils and the nose base.\n",
    "    \"nose\": [1, 6, 197, 2, 4, 5, 278, 275, 420, 129, 358, 64, 294, 98, 327, 118, 347], \n",
    "    \n",
    "    # Indices covering the lips and corners of the mouth\n",
    "    \"mouth\": [0, 17, 61, 291, 14, 308, 375, 40, 81, 311, 317, 415],\n",
    "    \n",
    "    # UPDATED GROUP: Simplified indices to reliably anchor the region between nose (2) and lips (13, 14).\n",
    "    # This creates a vertical bounding box for the philtrum.\n",
    "    \"philtrum\": [2, 13, 14, 172, 406, ],\n",
    "    \n",
    "    # Indices covering both left and right eyebrows\n",
    "    \"eyebrows\": [46, 53, 52, 65, 55, 70, 63, 105, 107, 276, 283, 282, 295, 285, 336, 296, 334, 293, 300],\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "# Define Canny thresholds based on feature importance/noise level\n",
    "# Lower thresholds (e.g., 50, 150) capture more detail (high sensitivity)\n",
    "# Higher thresholds (e.g., 80, 200) capture only strong, clean edges (low sensitivity/less noise)\n",
    "CANNY_THRESHOLDS = {\n",
    "    \"eyes\": (50, 180),       # High detail for eyes\n",
    "    \"nose\": (50, 150),       # High detail for nose\n",
    "    \"mouth\": (50, 180),      # High detail for lips\n",
    "    \"philtrum\": (60, 150),   # Moderate detail for philtrum/creases\n",
    "    \"eyebrows\": (10, 50),   # Low detail/Less noise for eyebrows\n",
    "}\n",
    "GAUSSIAN_BLUR_SIZES = {\n",
    "    \"eyes\": (13, 13),\n",
    "    \"nose\": (11, 11),\n",
    "    \"mouth\": (11, 11),\n",
    "    \"philtrum\": (31, 31),\n",
    "    \"eyebrows\": (31, 31),\n",
    "}\n",
    "\n",
    "def image_to_minimal_line_drawing_enhanced(image_path, output_path):\n",
    "    \"\"\"\n",
    "    Creates a highly stylized line drawing by applying CLAHE and Canny edge detection \n",
    "    independently to multiple small facial feature ROIs, then adding a minimal \n",
    "    outline for the hair and clothing.\n",
    "    \"\"\"\n",
    "    print(f\"Starting selective line drawing for: {image_path}\")\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    H, W, _ = img.shape\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Initialize the final line drawing canvas: A white background (255)\n",
    "    final_line_drawing = np.ones((H, W), dtype=np.uint8) * 255 \n",
    "    \n",
    "    # --- 1. Face Mesh for Multiple Feature ROIs and CLAHE ---\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True, \n",
    "        max_num_faces=1, \n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        \n",
    "        results_face = face_mesh.process(img_rgb)\n",
    "        \n",
    "        if results_face.multi_face_landmarks:\n",
    "            \n",
    "            # Use the first detected face\n",
    "            face_landmarks = results_face.multi_face_landmarks[0]\n",
    "            \n",
    "            # Loop over each feature group to process them individually\n",
    "            for feature_name, indices in FEATURE_GROUPS.items():\n",
    "                x_coords = []\n",
    "                y_coords = []\n",
    "                \n",
    "                # Calculate a tight bounding box *only* around the selected feature group\n",
    "                for index in indices:\n",
    "                    lm = face_landmarks.landmark[index]\n",
    "                    x_coords.append(lm.x * W)\n",
    "                    y_coords.append(lm.y * H)\n",
    "                \n",
    "                if not x_coords: continue # Skip if no landmarks are found\n",
    "                \n",
    "                min_x, max_x = int(min(x_coords)), int(max(x_coords))\n",
    "                min_y, max_y = int(min(y_coords)), int(max(y_coords))\n",
    "\n",
    "                # Use a small fixed padding for these tiny ROIs\n",
    "                padding = 15\n",
    "                if feature_name == \"nose\" or feature_name == \"philtrum\":\n",
    "                    # Increase padding for the nose and philtrum to prevent harsh cutoffs and ensure better blending\n",
    "                    padding = 25 \n",
    "\n",
    "                x1 = max(0, min_x - padding)\n",
    "                y1 = max(0, min_y - padding)\n",
    "                x2 = min(W, max_x + padding)\n",
    "                y2 = min(H, max_y + padding)\n",
    "                \n",
    "                # Ensure ROI is valid\n",
    "                if x2 <= x1 or y2 <= y1: continue\n",
    "\n",
    "                # Extract the ROI from the original image\n",
    "                face_roi_area = img[y1:y2, x1:x2].copy()\n",
    "                \n",
    "                # --- DEBUG: Save the cropped ROI image for inspection ---\n",
    "                roi_debug_path = output_path.replace('.png', f'_{feature_name}_roi.png')\n",
    "                cv2.imwrite(roi_debug_path, face_roi_area)\n",
    "                print(f\"DEBUG: Saved ROI for {feature_name} to: {roi_debug_path}\")\n",
    "                # --- END DEBUG ---\n",
    "\n",
    "                face_roi_gray = cv2.cvtColor(face_roi_area, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # --- Selective CLAHE and Canny for High-Detail Features ---\n",
    "                \n",
    "                # Apply ADAPTIVE CONTRAST (CLAHE)\n",
    "                clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))\n",
    "                high_contrast_feature = clahe.apply(face_roi_gray)\n",
    "                high_contrast_feature = cv2.equalizeHist(face_roi_gray)\n",
    "                # Apply Gaussian Blur (kernel size adjusted slightly per feature type if needed, or stick to small (5, 5))\n",
    "                blurred_feature = cv2.GaussianBlur(high_contrast_feature, GAUSSIAN_BLUR_SIZES.get(feature_name, (5, 5)), 0)\n",
    "                roi_debug_path = output_path.replace('.png', f'_{feature_name}_roi.png')\n",
    "                cv2.imwrite(roi_debug_path, blurred_feature)\n",
    "                print(f\"DEBUG: Saved ROI for {feature_name} to: {roi_debug_path}\")\n",
    "                # --- END DYNAMIC CANNY THRESHOLDS ---\n",
    "                # --- APPLY DYNAMIC CANNY THRESHOLDS ---\n",
    "                low_thresh, high_thresh = CANNY_THRESHOLDS.get(feature_name, (50, 150))\n",
    "                feature_edges = cv2.Canny(blurred_feature, low_thresh, high_thresh)\n",
    "\n",
    "                \n",
    "                \n",
    "                # Dilate the feature lines slightly\n",
    "                kernel_feature = np.ones((7, 7), np.uint8) \n",
    "                dilated_feature_edges = cv2.dilate(feature_edges, kernel_feature, iterations=1) \n",
    "                \n",
    "                # Invert the lines (black lines on white)\n",
    "                inverted_feature_edges = cv2.bitwise_not(dilated_feature_edges)\n",
    "\n",
    "                # Combine feature edges onto the final canvas (black lines (0) overwrite white (255))\n",
    "                current_canvas_roi = final_line_drawing[y1:y2, x1:x2]\n",
    "                final_line_drawing[y1:y2, x1:x2] = np.minimum(current_canvas_roi, inverted_feature_edges)\n",
    "\n",
    "    # --- 2. Selfie Segmentation for Hair and Clothing Outline (Simplified Mask Approach) ---\n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as segmentor:\n",
    "        results_segmentation = segmentor.process(img_rgb)\n",
    "        \n",
    "        if results_segmentation.segmentation_mask is not None:\n",
    "            \n",
    "            # --- SIMPLIFIED OUTER OUTLINE (Based on Mask Only) ---\n",
    "            threshold = 0.5\n",
    "            person_mask = results_segmentation.segmentation_mask > threshold\n",
    "            \n",
    "            # 1. Isolate the mask image (white person shape on black background)\n",
    "            person_outline_img = person_mask.astype(np.uint8) * 255 \n",
    "\n",
    "            # 2. Strong blur to greatly simplify the outline (captures only the gross shape)\n",
    "            blurred_outline = cv2.GaussianBlur(person_outline_img, (61, 61), 0)\n",
    "\n",
    "            # 3. Broad Canny thresholds to find the edge of the highly blurred shape\n",
    "            low_threshold_outline = 10\n",
    "            high_threshold_outline = 50 \n",
    "            \n",
    "            outline_edges = cv2.Canny(blurred_outline, low_threshold_outline, high_threshold_outline)\n",
    "            \n",
    "            # 4. Dilate to thicken and ensure continuity\n",
    "            kernel_outline = np.ones((5, 5), np.uint8) \n",
    "            dilated_outline_edges = cv2.dilate(outline_edges, kernel_outline, iterations=1) \n",
    "            \n",
    "            # 5. Invert and combine\n",
    "            inverted_outline = cv2.bitwise_not(dilated_outline_edges)\n",
    "            \n",
    "            # Combine the simple outline onto the final canvas\n",
    "            final_line_drawing = np.minimum(final_line_drawing, inverted_outline)\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(output_path, final_line_drawing)\n",
    "    print(f\"Final selective line drawing saved to: {output_path}\")\n",
    "\n",
    "# Example call for normal execution\n",
    "image_to_minimal_line_drawing_enhanced('divija.png', 'selective_line_drawing.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2076bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting selective line drawing for: divija.png\n",
      "DEBUG: Saved blurred ROI for eyes to: selective_line_drawing_eyes_blurred.png\n",
      "INFO: Using Laplacian filter output for eyes Canny input.\n",
      "DEBUG: Saved inverted edges for eyes to: selective_line_drawing_eyes_inverted_edges.png\n",
      "DEBUG: Saved blurred ROI for nose to: selective_line_drawing_nose_blurred.png\n",
      "INFO: Using Laplacian filter output for nose Canny input.\n",
      "DEBUG: Saved inverted edges for nose to: selective_line_drawing_nose_inverted_edges.png\n",
      "DEBUG: Saved blurred ROI for mouth to: selective_line_drawing_mouth_blurred.png\n",
      "INFO: Using Laplacian filter output for mouth Canny input.\n",
      "DEBUG: Saved inverted edges for mouth to: selective_line_drawing_mouth_inverted_edges.png\n",
      "DEBUG: Saved blurred ROI for philtrum to: selective_line_drawing_philtrum_blurred.png\n",
      "INFO: Using Laplacian filter output for philtrum Canny input.\n",
      "DEBUG: Saved inverted edges for philtrum to: selective_line_drawing_philtrum_inverted_edges.png\n",
      "DEBUG: Saved blurred ROI for eyebrows to: selective_line_drawing_eyebrows_blurred.png\n",
      "INFO: Using Laplacian filter output for eyebrows Canny input.\n",
      "DEBUG: Saved inverted edges for eyebrows to: selective_line_drawing_eyebrows_inverted_edges.png\n",
      "Final selective line drawing saved to: selective_line_drawing.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760851716.763782 48348156 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760851716.764531 48358285 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760851716.768074 48358284 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1760851716.787882 48348156 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760851716.788642 48358302 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "#GOOD CODE\n",
    "# Initialize MediaPipe solutions (all required imports in one place)\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "# Feature groups for distinct processing (Eyes, Nose, Mouth).\n",
    "# Each group will be processed with its own tight bounding box.\n",
    "FEATURE_GROUPS = {\n",
    "     # Indices covering both eyes, excluding the face contour\n",
    "    # EXPANDED: Added points (e.g., 159, 386, 107, 336) near the brow and temple to increase the ROI size.\n",
    "    \"eyes\": [33, 133, 163, 144, 246, 362, 263, 390, 373, 466, 130, 160, 243, 359, 387, 463, 159, 386, 224], \n",
    "    \n",
    "    # INDICES UPDATED: Added points for better coverage of nostrils and the nose base.\n",
    "    \"nose\": [1, 6, 197, 2, 4, 5, 278, 275, 420, 129, 358, 64, 294, 98, 327, 118, 347], \n",
    "    \n",
    "    # Indices covering the lips and corners of the mouth\n",
    "    # EXPANDED: Added points (e.g., 175, 404, 57, 287) near the chin and lower face to increase the ROI size.\n",
    "    \"mouth\": [0, 17, 61, 291, 14, 308, 375, 40, 81, 311, 317, 415, 199],\n",
    "\n",
    "    \n",
    "    # UPDATED GROUP: Simplified indices to reliably anchor the region between nose (2) and lips (13, 14).\n",
    "    # This creates a vertical bounding box for the philtrum.\n",
    "    \"philtrum\": [2, 13, 14, 172, 406, ],\n",
    "    \n",
    "    # Indices covering both left and right eyebrows\n",
    "    \"eyebrows\": [46, 53, 52, 65, 55, 70, 63, 105, 107, 276, 283, 282, 295, 285, 336, 296, 334, 293, 300],\n",
    "}\n",
    "\n",
    "# Define Canny thresholds based on feature importance/noise level\n",
    "CANNY_THRESHOLDS = {\n",
    "    \"eyes\": (10, 100),       # High detail for eyes\n",
    "    \"nose\": (0, 100),       # High detail for nose\n",
    "    \"mouth\": (50, 120),      # High detail for lips\n",
    "    \"philtrum\": (60, 120),   # Moderate detail for philtrum/creases\n",
    "    \"eyebrows\": (10, 60),   # Low detail/Less noise for eyebrows\n",
    "}\n",
    "GAUSSIAN_BLUR_SIZES = {\n",
    "    \"eyes\": (13, 13),\n",
    "    \"nose\": (11, 11),\n",
    "    \"mouth\": (11, 11),\n",
    "    \"philtrum\": (31, 31),\n",
    "    \"eyebrows\": (21, 21),\n",
    "}\n",
    "\n",
    "def image_to_minimal_line_drawing_enhanced(image_path, output_path):\n",
    "    \"\"\"\n",
    "    Creates a highly stylized line drawing by applying CLAHE, LAPLACIAN, and Canny edge detection \n",
    "    independently to multiple small facial feature ROIs, then adding a minimal \n",
    "    outline for the hair and clothing.\n",
    "    \"\"\"\n",
    "    print(f\"Starting selective line drawing for: {image_path}\")\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    H, W, _ = img.shape\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Initialize the final line drawing canvas: A white background (255)\n",
    "    final_line_drawing = np.ones((H, W), dtype=np.uint8) * 255 \n",
    "    \n",
    "    # --- 1. Face Mesh for Multiple Feature ROIs and CLAHE ---\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True, \n",
    "        max_num_faces=1, \n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        \n",
    "        results_face = face_mesh.process(img_rgb)\n",
    "        \n",
    "        if results_face.multi_face_landmarks:\n",
    "            \n",
    "            # Use the first detected face\n",
    "            face_landmarks = results_face.multi_face_landmarks[0]\n",
    "            \n",
    "            # Loop over each feature group to process them individually\n",
    "            for feature_name, indices in FEATURE_GROUPS.items():\n",
    "                x_coords = []\n",
    "                y_coords = []\n",
    "                \n",
    "                # Calculate a tight bounding box *only* around the selected feature group\n",
    "                for index in indices:\n",
    "                    lm = face_landmarks.landmark[index]\n",
    "                    x_coords.append(lm.x * W)\n",
    "                    y_coords.append(lm.y * H)\n",
    "                \n",
    "                if not x_coords: continue # Skip if no landmarks are found\n",
    "                \n",
    "                min_x, max_x = int(min(x_coords)), int(max(x_coords))\n",
    "                min_y, max_y = int(min(y_coords)), int(max(y_coords))\n",
    "\n",
    "                # Use a small fixed padding for these tiny ROIs\n",
    "                padding = 15\n",
    "                if feature_name == \"nose\" or feature_name == \"philtrum\":\n",
    "                    # Increase padding for the nose and philtrum to prevent harsh cutoffs and ensure better blending\n",
    "                    padding = 25 \n",
    "\n",
    "                x1 = max(0, min_x - padding)\n",
    "                y1 = max(0, min_y - padding)\n",
    "                x2 = min(W, max_x + padding)\n",
    "                y2 = min(H, max_y + padding)\n",
    "                \n",
    "                # Ensure ROI is valid\n",
    "                if x2 <= x1 or y2 <= y1: continue\n",
    "\n",
    "                # Extract the ROI from the original image\n",
    "                face_roi_area = img[y1:y2, x1:x2].copy()\n",
    "                \n",
    "               \n",
    "                face_roi_gray = cv2.cvtColor(face_roi_area, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                \n",
    "                # apply histogram equalization\n",
    "                high_contrast_feature = cv2.equalizeHist(face_roi_gray)\n",
    "                \n",
    "                # Apply Gaussian Blur (Size determined dynamically from GAUSSIAN_BLUR_SIZES)\n",
    "                blur_size = GAUSSIAN_BLUR_SIZES.get(feature_name, (5, 5))\n",
    "                blurred_feature = cv2.GaussianBlur(high_contrast_feature, blur_size, 0)\n",
    "                clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))\n",
    "                high_contrast_feature = clahe.apply(high_contrast_feature)\n",
    "                #DEBUG: Save the blurred ROI for inspection\n",
    "                roi_debug_path = output_path.replace('.png', f'_{feature_name}_blurred.png')\n",
    "                cv2.imwrite(roi_debug_path, blurred_feature)\n",
    "                print(f\"DEBUG: Saved blurred ROI for {feature_name} to: {roi_debug_path}\")\n",
    "                # --- END DEBUG ---\n",
    "                # --- UNIVERSAL LAPLACIAN FILTERING ---\n",
    "                # Apply Laplacian Filter (CV_64F for precision) to emphasize line features\n",
    "                laplacian = cv2.Laplacian(blurred_feature, cv2.CV_64F)\n",
    "                # Convert to 8-bit for Canny input (using absolute values)\n",
    "                canny_input = cv2.convertScaleAbs(laplacian)\n",
    "                print(f\"INFO: Using Laplacian filter output for {feature_name} Canny input.\")\n",
    "                # --- END UNIVERSAL LAPLACIAN FILTERING ---\n",
    "                \n",
    "                # --- APPLY DYNAMIC CANNY THRESHOLDS ---\n",
    "                low_thresh, high_thresh = CANNY_THRESHOLDS.get(feature_name, (50, 150))\n",
    "                \n",
    "                # Canny now uses the Laplacian output\n",
    "                feature_edges = cv2.Canny(blurred_feature, low_thresh, high_thresh)\n",
    "\n",
    "                \n",
    "                # Dilate the feature lines slightly (Kernel size (3,3) with iteration 1 is often cleaner than 7x7)\n",
    "                kernel_feature = np.ones((7, 7), np.uint8) \n",
    "                dilated_feature_edges = cv2.dilate(feature_edges, kernel_feature, iterations=1) \n",
    "                \n",
    "                # Invert the lines (black lines on white)\n",
    "                inverted_feature_edges = cv2.bitwise_not(dilated_feature_edges)\n",
    "                #debug: Save the inverted edges for inspection\n",
    "                roi_debug_path = output_path.replace('.png', f'_{feature_name}_inverted_edges.png')\n",
    "                cv2.imwrite(roi_debug_path, inverted_feature_edges)\n",
    "                print(f\"DEBUG: Saved inverted edges for {feature_name} to: {roi_debug_path}\")\n",
    "                # Combine feature edges onto the final canvas (black lines (0) overwrite white (255))\n",
    "                current_canvas_roi = final_line_drawing[y1:y2, x1:x2]\n",
    "                final_line_drawing[y1:y2, x1:x2] = np.minimum(current_canvas_roi, inverted_feature_edges)\n",
    "\n",
    "    # --- 2. Selfie Segmentation for Hair and Clothing Outline (Simplified Mask Approach) ---\n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as segmentor:\n",
    "        results_segmentation = segmentor.process(img_rgb)\n",
    "        \n",
    "        if results_segmentation.segmentation_mask is not None:\n",
    "            \n",
    "            # --- SIMPLIFIED OUTER OUTLINE (Based on Mask Only) ---\n",
    "            threshold = 0.5\n",
    "            person_mask = results_segmentation.segmentation_mask > threshold\n",
    "            \n",
    "            # 1. Isolate the mask image (white person shape on black background)\n",
    "            person_outline_img = person_mask.astype(np.uint8) * 255 \n",
    "\n",
    "            # 2. Strong blur to greatly simplify the outline (captures only the gross shape)\n",
    "            blurred_outline = cv2.GaussianBlur(person_outline_img, (61, 61), 0)\n",
    "\n",
    "            # 3. Broad Canny thresholds to find the edge of the highly blurred shape\n",
    "            low_threshold_outline = 10\n",
    "            high_threshold_outline = 50 \n",
    "            \n",
    "            outline_edges = cv2.Canny(blurred_outline, low_threshold_outline, high_threshold_outline)\n",
    "            \n",
    "            # 4. Dilate to thicken and ensure continuity\n",
    "            kernel_outline = np.ones((5, 5), np.uint8) \n",
    "            dilated_outline_edges = cv2.dilate(outline_edges, kernel_outline, iterations=1) \n",
    "            \n",
    "            # 5. Invert and combine\n",
    "            inverted_outline = cv2.bitwise_not(dilated_outline_edges)\n",
    "            \n",
    "            # Combine the simple outline onto the final canvas\n",
    "            final_line_drawing = np.minimum(final_line_drawing, inverted_outline)\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(output_path, final_line_drawing)\n",
    "    print(f\"Final selective line drawing saved to: {output_path}\")\n",
    "\n",
    "# Example call for normal execution\n",
    "image_to_minimal_line_drawing_enhanced('divija.png', 'selective_line_drawing.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35378d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not load image at divija.p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760851117.156375 48347869 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "[ WARN:0@0.330] global loadsave.cpp:275 findDecoder imread_('divija.p'): can't open/read file: check file path/integrity\n",
      "W0000 00:00:1760851117.157261 48347994 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760851117.160887 48347993 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 92\u001b[39m\n\u001b[32m     89\u001b[39m     exit()\n\u001b[32m     91\u001b[39m canvas = np.ones_like(image) * \u001b[32m255\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m h, w, _ = \u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\n\u001b[32m     94\u001b[39m results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m results.multi_face_landmarks:\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'shape'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Catmull-Rom Spline Interpolation Function ---\n",
    "# (Keep this function as is, it's generally useful for all smooth lines)\n",
    "def catmull_rom_spline(P0, P1, P2, P3, num_points=20):\n",
    "    P0, P1, P2, P3 = np.array(P0), np.array(P1), np.array(P2), np.array(P3)\n",
    "    \n",
    "    points = []\n",
    "    for t in np.linspace(0, 1, num_points, endpoint=False):\n",
    "        t2 = t * t\n",
    "        t3 = t2 * t\n",
    "        \n",
    "        h00 = 2 * t3 - 3 * t2 + 1\n",
    "        h10 = t3 - 2 * t2 + t\n",
    "        h01 = -2 * t3 + 3 * t2\n",
    "        h11 = t3 - t2\n",
    "        \n",
    "        M1 = 0.5 * (P2 - P0)\n",
    "        M2 = 0.5 * (P3 - P1)\n",
    "        \n",
    "        P_t = h00 * P1 + h10 * M1 + h01 * P2 + h11 * M2\n",
    "        points.append(P_t.astype(int))\n",
    "    \n",
    "    return points\n",
    "\n",
    "def draw_smooth_curve(image, points, color=(0, 0, 0), thickness=2, is_closed=False):\n",
    "    if len(points) < 2:\n",
    "        return\n",
    "\n",
    "    extended_points = list(points)\n",
    "    if is_closed:\n",
    "        extended_points.append(points[0]) \n",
    "        extended_points.insert(0, points[-1])\n",
    "        extended_points.append(points[1])\n",
    "    else:\n",
    "        extended_points.insert(0, points[0])\n",
    "        extended_points.append(points[-1])\n",
    "        \n",
    "    all_interpolated_points = []\n",
    "    \n",
    "    for i in range(1, len(extended_points) - 2):\n",
    "        P0 = extended_points[i-1]\n",
    "        P1 = extended_points[i]\n",
    "        P2 = extended_points[i+1]\n",
    "        P3 = extended_points[i+2]\n",
    "        \n",
    "        segment_points = catmull_rom_spline(P0, P1, P2, P3, num_points=10)\n",
    "        all_interpolated_points.extend(segment_points)\n",
    "\n",
    "    all_interpolated_points.append(points[-1])\n",
    "\n",
    "    for i in range(len(all_interpolated_points) - 1):\n",
    "        pt1 = tuple(all_interpolated_points[i])\n",
    "        pt2 = tuple(all_interpolated_points[i+1])\n",
    "        cv2.line(image, pt1, pt2, color, thickness, cv2.LINE_AA)\n",
    "        \n",
    "    if is_closed:\n",
    "        cv2.line(image, tuple(points[-1]), tuple(points[0]), color, thickness, cv2.LINE_AA)\n",
    "\n",
    "# --- 2. Landmark Indices for Features ---\n",
    "LEFT_EYE = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398] # Outer ring\n",
    "RIGHT_EYE = [33, 246, 161, 160, 159, 158, 157, 173, 133, 155, 154, 153, 145, 144, 163, 7] # Outer ring\n",
    "LIPS_OUTER = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 306, 375, 321, 405, 314, 17]\n",
    "LIPS_INNER = [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308, 309, 310, 311, 415, 307] \n",
    "FACE_OVAL = [10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288, 397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136, 172, 58, 132, 93, 234, 127, 162, 21, 54, 103, 67, 109]\n",
    "\n",
    "# Pupil center landmarks (Refine_landmarks=True adds these)\n",
    "LEFT_PUPIL_CENTER = 468\n",
    "RIGHT_PUPIL_CENTER = 473\n",
    "\n",
    "# --- 3. Main Processing Logic ---\n",
    "\n",
    "# IMPORTANT: Replace 'input_image.jpg' with a path to your face image\n",
    "image_path = 'divija.p' \n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True, \n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True, \n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "if image is None:\n",
    "    print(f\"Error: Could not load image at {image_path}\")\n",
    "    exit()\n",
    "\n",
    "canvas = np.ones_like(image) * 255\n",
    "h, w, _ = image.shape\n",
    "\n",
    "results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "if results.multi_face_landmarks:\n",
    "    face_landmarks = results.multi_face_landmarks[0]\n",
    "    \n",
    "    pixel_landmarks = {}\n",
    "    for i, landmark in enumerate(face_landmarks.landmark):\n",
    "        x = int(landmark.x * w)\n",
    "        y = int(landmark.y * h)\n",
    "        pixel_landmarks[i] = (x, y)\n",
    "\n",
    "    def get_contour_points(indices):\n",
    "        return [pixel_landmarks[i] for i in indices if i in pixel_landmarks]\n",
    "\n",
    "    line_color = (0, 0, 0) \n",
    "    line_thickness = 2\n",
    "    \n",
    "    # Draw Lips\n",
    "    outer_lip_points = get_contour_points(LIPS_OUTER)\n",
    "    draw_smooth_curve(canvas, outer_lip_points, line_color, line_thickness, is_closed=True)\n",
    "    inner_lip_points = get_contour_points(LIPS_INNER)\n",
    "    draw_smooth_curve(canvas, inner_lip_points, line_color, line_thickness, is_closed=True)\n",
    "    \n",
    "    # Draw Eyes\n",
    "    left_eye_points = get_contour_points(LEFT_EYE)\n",
    "    draw_smooth_curve(canvas, left_eye_points, line_color, line_thickness, is_closed=True)\n",
    "    right_eye_points = get_contour_points(RIGHT_EYE)\n",
    "    draw_smooth_curve(canvas, right_eye_points, line_color, line_thickness, is_closed=True)\n",
    "\n",
    "    # Draw Face Outline\n",
    "    face_oval_points = get_contour_points(FACE_OVAL)\n",
    "    draw_smooth_curve(canvas, face_oval_points, line_color, line_thickness, is_closed=True)\n",
    "\n",
    "    # --- MODIFIED: Draw Pupil Lines ---\n",
    "    pupil_line_thickness = 3 # Make pupil line thicker for emphasis\n",
    "    \n",
    "    # Right Eye Pupil\n",
    "    if RIGHT_PUPIL_CENTER in pixel_landmarks:\n",
    "        pupil_center_right = pixel_landmarks[RIGHT_PUPIL_CENTER]\n",
    "        \n",
    "        # Check if target landmarks exist and draw lines\n",
    "        if 145 in pixel_landmarks:\n",
    "            cv2.line(canvas, pupil_center_right, pixel_landmarks[145], line_color, pupil_line_thickness, cv2.LINE_AA)\n",
    "        if 159 in pixel_landmarks:\n",
    "            cv2.line(canvas, pupil_center_right, pixel_landmarks[159], line_color, pupil_line_thickness, cv2.LINE_AA)\n",
    "\n",
    "    # Left Eye Pupil (Using analogous points for the left eye)\n",
    "    # Based on MediaPipe's structure, 374 and 380 are analogous to 145 and 159 respectively\n",
    "    if LEFT_PUPIL_CENTER in pixel_landmarks:\n",
    "        pupil_center_left = pixel_landmarks[LEFT_PUPIL_CENTER]\n",
    "        \n",
    "        if 374 in pixel_landmarks: # Analogous to 145\n",
    "            cv2.line(canvas, pupil_center_left, pixel_landmarks[374], line_color, pupil_line_thickness, cv2.LINE_AA)\n",
    "        if 380 in pixel_landmarks: # Analogous to 159\n",
    "            cv2.line(canvas, pupil_center_left, pixel_landmarks[380], line_color, pupil_line_thickness, cv2.LINE_AA)\n",
    "\n",
    "# --- 4. Display Result ---\n",
    "cv2.imwrite('stylized_line_drawing_with_custom_pupils.png', canvas)\n",
    "print(\"Saved stylized line drawing with custom pupils to 'stylized_line_drawing_with_custom_pupils.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d638715f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting selective line drawing for: divija.png\n",
      "Processed and merged eyes edges using Lab color differences.\n",
      "Processed and merged nose edges using Lab color differences.\n",
      "Processed and merged mouth edges using Lab color differences.\n",
      "Processed and merged philtrum edges using Lab color differences.\n",
      "Processed and merged eyebrows edges using Lab color differences.\n",
      "Processed and merged outer outline edges.\n",
      "Final selective line drawing saved to: selective_line_drawing.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760851483.164439 48348156 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760851483.165217 48354507 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760851483.168703 48354506 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1760851483.259895 48348156 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760851483.260592 48354520 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe solutions (all required imports in one place)\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "# Feature groups and thresholds remain the same\n",
    "FEATURE_GROUPS = {\n",
    "    \"eyes\": [33, 133, 163, 144, 246, 362, 263, 390, 373, 466, 130, 160, 243, 359, 387, 463, 159, 386, 224], \n",
    "    \"nose\": [1, 6, 197, 2, 4, 5, 278, 275, 420, 129, 358, 64, 294, 98, 327, 118, 347], \n",
    "    \"mouth\": [0, 17, 61, 291, 14, 308, 375, 40, 81, 311, 317, 415, 199],\n",
    "    \"philtrum\": [2, 13, 14, 172, 406, ],\n",
    "    \"eyebrows\": [46, 53, 52, 65, 55, 70, 63, 105, 107, 276, 283, 282, 295, 285, 336, 296, 334, 293, 300],\n",
    "}\n",
    "CANNY_THRESHOLDS = {\n",
    "    \"eyes\": (10, 100), \"nose\": (5, 100), \"mouth\": (50, 100),\n",
    "    \"philtrum\": (60, 120), \"eyebrows\": (10, 30),\n",
    "}\n",
    "GAUSSIAN_BLUR_SIZES = {\n",
    "    \"eyes\": (13, 13), \"nose\": (11, 11), \"mouth\": (11, 11),\n",
    "    \"philtrum\": (31, 31), \"eyebrows\": (41, 41),\n",
    "}\n",
    "\n",
    "\n",
    "def image_to_minimal_line_drawing_enhanced(image_path, output_path):\n",
    "    print(f\"Starting selective line drawing for: {image_path}\")\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    H, W, _ = img.shape\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    final_line_drawing = np.ones((H, W), dtype=np.uint8) * 255 \n",
    "    \n",
    "    # --- 1. Face Mesh for Multiple Feature ROIs and Edge Detection ---\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True, \n",
    "        max_num_faces=1, \n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        \n",
    "        results_face = face_mesh.process(img_rgb)\n",
    "        \n",
    "        if results_face.multi_face_landmarks:\n",
    "            face_landmarks = results_face.multi_face_landmarks[0]\n",
    "            \n",
    "            # --- Pre-convert the full image to Lab color space once ---\n",
    "            img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "            clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))\n",
    "            \n",
    "            for feature_name, indices in FEATURE_GROUPS.items():\n",
    "                x_coords, y_coords = [], []\n",
    "                \n",
    "                # Bounding Box Calculation (omitted for brevity, assume correct)\n",
    "                for index in indices:\n",
    "                    if index < len(face_landmarks.landmark):\n",
    "                         lm = face_landmarks.landmark[index]\n",
    "                         x_coords.append(lm.x * W); y_coords.append(lm.y * H)\n",
    "                \n",
    "                if not x_coords: continue \n",
    "                min_x, max_x = int(min(x_coords)), int(max(x_coords))\n",
    "                min_y, max_y = int(min(y_coords)), int(max(y_coords))\n",
    "\n",
    "                padding = 30\n",
    "                if feature_name in [\"nose\", \"philtrum\"]: padding = 25 \n",
    "                \n",
    "                x1 = max(0, min_x - padding); y1 = max(0, min_y - padding)\n",
    "                x2 = min(W, max_x + padding); y2 = min(H, max_y + padding)\n",
    "                if x2 <= x1 or y2 <= y1: continue\n",
    "\n",
    "                # Extract the ROI from the Lab image\n",
    "                face_roi_lab = img_lab[y1:y2, x1:x2].copy()\n",
    "                \n",
    "                # --- COLOR DIFFERENCE EDGE EXTRACTION ---\n",
    "                all_channel_laplacian_edges = []\n",
    "                blur_size = GAUSSIAN_BLUR_SIZES.get(feature_name, (5, 5))\n",
    "                \n",
    "                # Iterate over the L, a, and b channels\n",
    "                for channel_index in range(3):\n",
    "                    channel = face_roi_lab[:, :, channel_index]\n",
    "                    \n",
    "                    # 1. Apply CLAHE only to the L channel (luminosity)\n",
    "                    if channel_index == 0:\n",
    "                        channel = clahe.apply(channel)\n",
    "                        \n",
    "                    # 2. Apply Gaussian Blur\n",
    "                    blurred_channel = cv2.GaussianBlur(channel, blur_size, 0)\n",
    "                    \n",
    "                    # 3. Apply Laplacian (Color Difference / Lightness Difference)\n",
    "                    laplacian = cv2.Laplacian(blurred_channel, cv2.CV_64F, ksize=3)\n",
    "                    \n",
    "                    # 4. Convert to 8-bit absolute value and normalize the edge response\n",
    "                    laplacian_abs = cv2.convertScaleAbs(laplacian)\n",
    "                    \n",
    "                    all_channel_laplacian_edges.append(laplacian_abs)\n",
    "\n",
    "                # --- Combine Edges from L, a, and b channels ---\n",
    "                # A simple weighted sum is effective: L (lightness) often needs to be balanced \n",
    "                # with a/b (color). We'll use the maximum response for robust edge capture.\n",
    "                canny_input = cv2.max(all_channel_laplacian_edges[0], all_channel_laplacian_edges[1])\n",
    "                canny_input = cv2.max(canny_input, all_channel_laplacian_edges[2])\n",
    "                \n",
    "                \n",
    "                # --- APPLY DYNAMIC CANNY THRESHOLDS ---\n",
    "                low_thresh, high_thresh = CANNY_THRESHOLDS.get(feature_name, (50, 150))\n",
    "                \n",
    "                # Canny is applied to the combined, normalized Laplacian map\n",
    "                feature_edges = cv2.Canny(canny_input, low_thresh, high_thresh)\n",
    "\n",
    "                # --- Post-processing: Dilate and Combine ---\n",
    "                kernel_feature = np.ones((3, 3), np.uint8) \n",
    "                dilated_feature_edges = cv2.dilate(feature_edges, kernel_feature, iterations=1) \n",
    "                \n",
    "                inverted_feature_edges = cv2.bitwise_not(dilated_feature_edges)\n",
    "                \n",
    "                current_canvas_roi = final_line_drawing[y1:y2, x1:x2]\n",
    "                final_line_drawing[y1:y2, x1:x2] = np.minimum(current_canvas_roi, inverted_feature_edges)\n",
    "                print(f\"Processed and merged {feature_name} edges using Lab color differences.\")\n",
    "\n",
    "    # --- 2. Selfie Segmentation for Hair and Clothing Outline (Unchanged) ---\n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as segmentor:\n",
    "        results_segmentation = segmentor.process(img_rgb)\n",
    "        \n",
    "        if results_segmentation.segmentation_mask is not None:\n",
    "            threshold = 0.5\n",
    "            person_mask = results_segmentation.segmentation_mask > threshold\n",
    "            person_outline_img = person_mask.astype(np.uint8) * 255 \n",
    "\n",
    "            blurred_outline = cv2.GaussianBlur(person_outline_img, (61, 61), 0)\n",
    "            outline_edges = cv2.Canny(blurred_outline, 10, 50)\n",
    "            \n",
    "            kernel_outline = np.ones((5, 5), np.uint8) \n",
    "            dilated_outline_edges = cv2.dilate(outline_edges, kernel_outline, iterations=1) \n",
    "            \n",
    "            inverted_outline = cv2.bitwise_not(dilated_outline_edges)\n",
    "            final_line_drawing = np.minimum(final_line_drawing, inverted_outline)\n",
    "            print(\"Processed and merged outer outline edges.\")\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(output_path, final_line_drawing)\n",
    "    print(f\"Final selective line drawing saved to: {output_path}\")\n",
    "\n",
    "# Example call for normal execution\n",
    "image_to_minimal_line_drawing_enhanced('divija.png', 'selective_line_drawing.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b969f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting single-line vector simplification for: selective_line_drawing2.png\n",
      "Applied morphological closing with kernel size: 5.\n",
      "Final paths after merging and simplification: 58.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.morphology import skeletonize \n",
    "\n",
    "def simplify_for_laser_cutter_merged(image_path, epsilon_factor=0.005, closing_kernel_size=3, min_contour_length=50, output_canvas_size=(800, 800), output_line_thickness=2):\n",
    "    \"\"\"\n",
    "    Produces a single-line drawing by applying morphological closing to merge nearby parallel lines, \n",
    "    followed by skeletonization and RDP simplification.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input line art image.\n",
    "        epsilon_factor (float): RDP simplification factor (Higher = smoother/simpler).\n",
    "        closing_kernel_size (int): Size of the kernel for the CLOSE operation. Increase this \n",
    "                                   to merge lines that are further apart. Odd numbers (e.g., 3, 5, 7) are best.\n",
    "        min_contour_length (int): Filters out short lines/noise.\n",
    "        output_canvas_size (tuple): Output image size.\n",
    "        output_line_thickness (int): Thickness for the output visualization image.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A clean, simplified, single-line drawing.\n",
    "        list: The list of simplified polygonal contours (vector paths).\n",
    "    \"\"\"\n",
    "    print(f\"Starting single-line vector simplification for: {image_path}\")\n",
    "    \n",
    "    # --- 1. Load and Binarize ---\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
    "\n",
    "    # Invert and Binarize: Lines are white (255) on black (0) for morphological ops\n",
    "    _, binary_img = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "    \n",
    "    # --- 2. Morphological Closing (MERGING STEP) ---\n",
    "    # This closes small holes and joins very close, parallel lines into a single blob.\n",
    "    kernel = np.ones((closing_kernel_size, closing_kernel_size), np.uint8)\n",
    "    # Closing = Dilate (grow) then Erode (shrink). This fills small gaps and connects nearby lines.\n",
    "    merged_img = cv2.morphologyEx(binary_img, cv2.MORPH_CLOSE, kernel)\n",
    "    print(f\"Applied morphological closing with kernel size: {closing_kernel_size}.\")\n",
    "    \n",
    "    # --- 3. Skeletonize ---\n",
    "    # Convert to boolean for skeletonize, then reduce all merged lines to 1-pixel thickness\n",
    "    merged_bool = merged_img / 255.0 > 0.5 \n",
    "    skeleton = skeletonize(merged_bool)\n",
    "    skeleton_img = (skeleton * 255).astype(np.uint8) \n",
    "    \n",
    "    # --- 4. Contour Tracing and Simplification (RDP) ---\n",
    "    contours, _ = cv2.findContours(skeleton_img, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    simplified_contours = []\n",
    "    output_canvas = np.ones((output_canvas_size[1], output_canvas_size[0], 3), dtype=np.uint8) * 255\n",
    "    \n",
    "    # Scaling and Offset calculations\n",
    "    img_h, img_w = img.shape[:2]\n",
    "    scale_factor = min(output_canvas_size[0] / img_w, output_canvas_size[1] / img_h)\n",
    "    offset_x = (output_canvas_size[0] - img_w * scale_factor) / 2\n",
    "    offset_y = (output_canvas_size[1] - img_h * scale_factor) / 2\n",
    "\n",
    "    for contour in contours:\n",
    "        if len(contour) < 2: continue\n",
    "        \n",
    "        perimeter = cv2.arcLength(contour, False) \n",
    "        if perimeter < min_contour_length: continue\n",
    "\n",
    "        # Ramer-Douglas-Peucker Simplification\n",
    "        epsilon = epsilon_factor * perimeter\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, False)\n",
    "        \n",
    "        if len(approx) < 2: continue\n",
    "            \n",
    "        # Apply Scaling and store\n",
    "        scaled_approx = np.array([[[int(p[0][0] * scale_factor + offset_x), int(p[0][1] * scale_factor + offset_y)]] for p in approx], dtype=np.int32)\n",
    "        simplified_contours.append(scaled_approx)\n",
    "        \n",
    "        # Draw the single-line path\n",
    "        cv2.drawContours(output_canvas, [scaled_approx], -1, (0, 0, 0), output_line_thickness, cv2.LINE_AA)\n",
    "        \n",
    "    print(f\"Final paths after merging and simplification: {len(simplified_contours)}.\")\n",
    "    return output_canvas, simplified_contours\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# Use the image you provided.\n",
    "input_image_path = 'selective_line_drawing2.png' \n",
    "\n",
    "try:\n",
    "    output_image, final_vectors = simplify_for_laser_cutter_merged(\n",
    "        input_image_path, \n",
    "        epsilon_factor=0.0001,          # Increased smoothing factor\n",
    "        closing_kernel_size=5,         # Increased to 5 to merge lines slightly further apart\n",
    "        min_contour_length=75,         # Filter out small artifacts\n",
    "        output_line_thickness=1\n",
    "    )\n",
    "\n",
    "    \n",
    "    cv2.imwrite('merged_single_line_output.png', output_image)\n",
    "    \n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "499afb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting selective line drawing for: divija2.png\n",
      "DEBUG: Saved blurred ROI for eyes to: selective_line_drawing2_eyes_blurred.png\n",
      "INFO: Using Laplacian filter output for eyes Canny input.\n",
      "DEBUG: Saved inverted edges for eyes to: selective_line_drawing2_eyes_inverted_edges.png\n",
      "DEBUG: Saved blurred ROI for nose to: selective_line_drawing2_nose_blurred.png\n",
      "INFO: Using Laplacian filter output for nose Canny input.\n",
      "DEBUG: Saved inverted edges for nose to: selective_line_drawing2_nose_inverted_edges.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760854637.812722 48376247 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760854637.813421 48417125 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760854637.817228 48417124 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1760854637.906366 48376247 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760854637.907291 48417137 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1760854637.953984 48376247 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760854637.955265 48417149 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Saved blurred ROI for mouth to: selective_line_drawing2_mouth_blurred.png\n",
      "INFO: Using Laplacian filter output for mouth Canny input.\n",
      "DEBUG: Saved inverted edges for mouth to: selective_line_drawing2_mouth_inverted_edges.png\n",
      "DEBUG: Saved blurred ROI for philtrum to: selective_line_drawing2_philtrum_blurred.png\n",
      "INFO: Using Laplacian filter output for philtrum Canny input.\n",
      "DEBUG: Saved inverted edges for philtrum to: selective_line_drawing2_philtrum_inverted_edges.png\n",
      "DEBUG: Saved blurred ROI for eyebrows to: selective_line_drawing2_eyebrows_blurred.png\n",
      "INFO: Using Laplacian filter output for eyebrows Canny input.\n",
      "DEBUG: Saved inverted edges for eyebrows to: selective_line_drawing2_eyebrows_inverted_edges.png\n",
      "Starting dedicated hair segmentation.\n",
      "Processed and merged dedicated hair outline.\n",
      "Final selective line drawing saved to: selective_line_drawing2.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "# Initialize MediaPipe solutions (all required imports in one place)\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "HAIR_MODEL_PATH = '/Users/divija/Divi Drive/workplace/Princeton/Sem 5/Carlab/carlab_2025/final_project/hair_segmenter.tflite'  # Path to custom hair segmentation model\n",
    "# Feature groups for distinct processing (Eyes, Nose, Mouth).\n",
    "# Each group will be processed with its own tight bounding box.\n",
    "FEATURE_GROUPS = {\n",
    "     # Indices covering both eyes, excluding the face contour\n",
    "    # EXPANDED: Added points (e.g., 159, 386, 107, 336) near the brow and temple to increase the ROI size.\n",
    "    \"eyes\": [33, 133, 163, 144, 246, 362, 263, 390, 373, 466, 130, 160, 243, 359, 387, 463, 159, 386, 224], \n",
    "    \n",
    "    # INDICES UPDATED: Added points for better coverage of nostrils and the nose base.\n",
    "    \"nose\": [1, 6, 197, 2, 4, 5, 278, 275, 420, 129, 358, 64, 294, 98, 327, 118, 347], \n",
    "    \n",
    "    # Indices covering the lips and corners of the mouth\n",
    "    # EXPANDED: Added points (e.g., 175, 404, 57, 287) near the chin and lower face to increase the ROI size.\n",
    "    \"mouth\": [0, 17, 61, 291, 14, 308, 375, 40, 81, 311, 317, 415, 199],\n",
    "\n",
    "    \n",
    "    # UPDATED GROUP: Simplified indices to reliably anchor the region between nose (2) and lips (13, 14).\n",
    "    # This creates a vertical bounding box for the philtrum.\n",
    "    \"philtrum\": [2, 13, 14, 172, 406, ],\n",
    "    \n",
    "    # Indices covering both left and right eyebrows\n",
    "    \"eyebrows\": [46, 53, 52, 65, 55, 70, 63, 105, 107, 276, 283, 282, 295, 285, 336, 296, 334, 293, 300],\n",
    "}\n",
    "\n",
    "# Define Canny thresholds based on feature importance/noise level\n",
    "CANNY_THRESHOLDS = {\n",
    "    \"eyes\": (10, 100),       # High detail for eyes\n",
    "    \"nose\": (0, 70),       # High detail for nose\n",
    "    \"mouth\": (50, 120),      # High detail for lips\n",
    "    \"philtrum\": (60, 120),   # Moderate detail for philtrum/creases\n",
    "    \"eyebrows\": (10, 60),   # Low detail/Less noise for eyebrows\n",
    "}\n",
    "GAUSSIAN_BLUR_SIZES = {\n",
    "    \"eyes\": (13, 13),\n",
    "    \"nose\": (11, 11),\n",
    "    \"mouth\": (11, 11),\n",
    "    \"philtrum\": (31, 31),\n",
    "    \"eyebrows\": (21, 21),\n",
    "}\n",
    "\n",
    "def image_to_minimal_line_drawing_enhanced(image_path, output_path):\n",
    "    \"\"\"\n",
    "    Creates a highly stylized line drawing by applying CLAHE, LAPLACIAN, and Canny edge detection \n",
    "    independently to multiple small facial feature ROIs, then adding a minimal \n",
    "    outline for the hair and clothing.\n",
    "    \"\"\"\n",
    "    print(f\"Starting selective line drawing for: {image_path}\")\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    H, W, _ = img.shape\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Initialize the final line drawing canvas: A white background (255)\n",
    "    final_line_drawing = np.ones((H, W), dtype=np.uint8) * 255 \n",
    "    \n",
    "    # --- 1. Face Mesh for Multiple Feature ROIs and CLAHE ---\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True, \n",
    "        max_num_faces=1, \n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        \n",
    "        results_face = face_mesh.process(img_rgb)\n",
    "        \n",
    "        if results_face.multi_face_landmarks:\n",
    "            \n",
    "            # Use the first detected face\n",
    "            face_landmarks = results_face.multi_face_landmarks[0]\n",
    "            \n",
    "            # Loop over each feature group to process them individually\n",
    "            for feature_name, indices in FEATURE_GROUPS.items():\n",
    "                x_coords = []\n",
    "                y_coords = []\n",
    "                \n",
    "                # Calculate a tight bounding box *only* around the selected feature group\n",
    "                for index in indices:\n",
    "                    lm = face_landmarks.landmark[index]\n",
    "                    x_coords.append(lm.x * W)\n",
    "                    y_coords.append(lm.y * H)\n",
    "                \n",
    "                if not x_coords: continue # Skip if no landmarks are found\n",
    "                \n",
    "                min_x, max_x = int(min(x_coords)), int(max(x_coords))\n",
    "                min_y, max_y = int(min(y_coords)), int(max(y_coords))\n",
    "\n",
    "                # Use a small fixed padding for these tiny ROIs\n",
    "                padding = 15\n",
    "                if feature_name == \"nose\" or feature_name == \"philtrum\":\n",
    "                    # Increase padding for the nose and philtrum to prevent harsh cutoffs and ensure better blending\n",
    "                    padding = 25 \n",
    "\n",
    "                x1 = max(0, min_x - padding)\n",
    "                y1 = max(0, min_y - padding)\n",
    "                x2 = min(W, max_x + padding)\n",
    "                y2 = min(H, max_y + padding)\n",
    "                \n",
    "                # Ensure ROI is valid\n",
    "                if x2 <= x1 or y2 <= y1: continue\n",
    "\n",
    "                # Extract the ROI from the original image\n",
    "                face_roi_area = img[y1:y2, x1:x2].copy()\n",
    "                \n",
    "                face_roi_area = cv2.medianBlur(face_roi_area, 11)\n",
    "                face_roi_gray = cv2.cvtColor(face_roi_area, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                \n",
    "                # apply histogram equalization\n",
    "                high_contrast_feature = cv2.equalizeHist(face_roi_gray)\n",
    "                \n",
    "                # Apply Gaussian Blur (Size determined dynamically from GAUSSIAN_BLUR_SIZES)\n",
    "                blur_size = GAUSSIAN_BLUR_SIZES.get(feature_name, (5, 5))\n",
    "                blurred_feature = cv2.GaussianBlur(high_contrast_feature, blur_size, 0)\n",
    "                clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))\n",
    "                high_contrast_feature = clahe.apply(high_contrast_feature)\n",
    "                #DEBUG: Save the blurred ROI for inspection\n",
    "                roi_debug_path = output_path.replace('.png', f'_{feature_name}_blurred.png')\n",
    "                cv2.imwrite(roi_debug_path, blurred_feature)\n",
    "                print(f\"DEBUG: Saved blurred ROI for {feature_name} to: {roi_debug_path}\")\n",
    "                # --- END DEBUG ---\n",
    "                # --- UNIVERSAL LAPLACIAN FILTERING ---\n",
    "                # Apply Laplacian Filter (CV_64F for precision) to emphasize line features\n",
    "                laplacian = cv2.Laplacian(blurred_feature, cv2.CV_64F)\n",
    "                # Convert to 8-bit for Canny input (using absolute values)\n",
    "                canny_input = cv2.convertScaleAbs(laplacian)\n",
    "                print(f\"INFO: Using Laplacian filter output for {feature_name} Canny input.\")\n",
    "                # --- END UNIVERSAL LAPLACIAN FILTERING ---\n",
    "                \n",
    "                # --- APPLY DYNAMIC CANNY THRESHOLDS ---\n",
    "                low_thresh, high_thresh = CANNY_THRESHOLDS.get(feature_name, (50, 150))\n",
    "                \n",
    "                # Canny now uses the Laplacian output\n",
    "                feature_edges = cv2.Canny(blurred_feature, low_thresh, high_thresh)\n",
    "\n",
    "                \n",
    "                # Dilate the feature lines slightly (Kernel size (3,3) with iteration 1 is often cleaner than 7x7)\n",
    "                kernel_feature = np.ones((7, 7), np.uint8) \n",
    "                dilated_feature_edges = cv2.dilate(feature_edges, kernel_feature, iterations=1) \n",
    "                \n",
    "                # Invert the lines (black lines on white)\n",
    "                inverted_feature_edges = cv2.bitwise_not(dilated_feature_edges)\n",
    "                #debug: Save the inverted edges for inspection\n",
    "                roi_debug_path = output_path.replace('.png', f'_{feature_name}_inverted_edges.png')\n",
    "                cv2.imwrite(roi_debug_path, inverted_feature_edges)\n",
    "                print(f\"DEBUG: Saved inverted edges for {feature_name} to: {roi_debug_path}\")\n",
    "                # Combine feature edges onto the final canvas (black lines (0) overwrite white (255))\n",
    "                current_canvas_roi = final_line_drawing[y1:y2, x1:x2]\n",
    "                final_line_drawing[y1:y2, x1:x2] = np.minimum(current_canvas_roi, inverted_feature_edges)\n",
    "\n",
    "    # --- 2. Selfie Segmentation for Hair and Clothing Outline (Simplified Mask Approach) ---\n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as segmentor:\n",
    "        results_segmentation = segmentor.process(img_rgb)\n",
    "        \n",
    "        if results_segmentation.segmentation_mask is not None:\n",
    "            \n",
    "            # --- SIMPLIFIED OUTER OUTLINE (Based on Mask Only) ---\n",
    "            threshold = 0.5\n",
    "            person_mask = results_segmentation.segmentation_mask > threshold\n",
    "            \n",
    "            # 1. Isolate the mask image (white person shape on black background)\n",
    "            person_outline_img = person_mask.astype(np.uint8) * 255 \n",
    "\n",
    "            # 2. Strong blur to greatly simplify the outline (captures only the gross shape)\n",
    "            blurred_outline = cv2.GaussianBlur(person_outline_img, (61, 61), 0)\n",
    "\n",
    "            # 3. Broad Canny thresholds to find the edge of the highly blurred shape\n",
    "            low_threshold_outline = 10\n",
    "            high_threshold_outline = 50 \n",
    "            \n",
    "            outline_edges = cv2.Canny(blurred_outline, low_threshold_outline, high_threshold_outline)\n",
    "            \n",
    "            # 4. Dilate to thicken and ensure continuity\n",
    "            kernel_outline = np.ones((5, 5), np.uint8) \n",
    "            dilated_outline_edges = cv2.dilate(outline_edges, kernel_outline, iterations=1) \n",
    "            \n",
    "            # 5. Invert and combine\n",
    "            inverted_outline = cv2.bitwise_not(dilated_outline_edges)\n",
    "            \n",
    "            # Combine the simple outline onto the final canvas\n",
    "            final_line_drawing = np.minimum(final_line_drawing, inverted_outline)\n",
    "    if HAIR_MODEL_PATH:\n",
    "        print(\"Starting dedicated hair segmentation.\")\n",
    "        try:\n",
    "            base_options = python.BaseOptions(model_asset_path=HAIR_MODEL_PATH, delegate=None)\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_rgb)\n",
    "            options = vision.ImageSegmenterOptions(\n",
    "                base_options=base_options,\n",
    "                running_mode=vision.RunningMode.IMAGE,\n",
    "                output_category_mask=True  # Hair is category 1\n",
    "            )\n",
    "            with vision.ImageSegmenter.create_from_options(options) as segmenter:\n",
    "                segmentation_result = segmenter.segment(mp_image)\n",
    "                category_mask = segmentation_result.category_mask.numpy_view()\n",
    "\n",
    "                # Hair mask (category index 1)\n",
    "                hair_mask = np.where(category_mask == 1, 255, 0).astype(np.uint8)\n",
    "                \n",
    "                # Resize the mask to the original image dimensions\n",
    "                resized_mask = cv2.resize(hair_mask, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "                \n",
    "                # 1. Blur the mask for a smoother outline\n",
    "                blurred_hair_mask = cv2.GaussianBlur(resized_mask, (21, 21), 0)\n",
    "                \n",
    "                # 2. Canny edge detection on the smoothed mask\n",
    "                hair_outline_edges = cv2.Canny(blurred_hair_mask, 10, 50)\n",
    "                \n",
    "                # 3. Dilate and Invert\n",
    "                kernel_hair = np.ones((5, 5), np.uint8) \n",
    "                dilated_hair_edges = cv2.dilate(hair_outline_edges, kernel_hair, iterations=1) \n",
    "                inverted_hair_outline = cv2.bitwise_not(dilated_hair_edges)\n",
    "                \n",
    "                # Combine the hair outline\n",
    "                final_line_drawing = np.minimum(final_line_drawing, inverted_hair_outline)\n",
    "                print(\"Processed and merged dedicated hair outline.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR in Hair Segmentation: Ensure '{HAIR_MODEL_PATH}' is correct. Falling back to Selfie Segmentation for hair/body.\")\n",
    "    # Save the result\n",
    "    cv2.imwrite(output_path, final_line_drawing)\n",
    "    print(f\"Final selective line drawing saved to: {output_path}\")\n",
    "\n",
    "# Example call for normal execution\n",
    "image_to_minimal_line_drawing_enhanced('divija2.png', 'selective_line_drawing2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a89e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carlab (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
