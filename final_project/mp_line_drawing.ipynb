{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37fda886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "FACEMESH_POINTS = mp_face_mesh.FACEMESH_TESSELATION  # all triangulated connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9d43be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: divija2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1761940475.110831 59735087 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1761940475.112650 59761977 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1761940475.117370 59761982 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1761940475.127174 59735087 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1761940475.129008 59761992 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1761940475.183036 59735087 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1761940475.186205 59762003 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved continuous single-line drawing to: divija_line.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import scipy.ndimage\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Initialize MediaPipe\n",
    "# -------------------------------------------------\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "HAIR_MODEL_PATH = '/Users/divija/Divi Drive/workplace/Princeton/Sem 5/Carlab/carlab_2025/final_project/hair_segmenter.tflite'\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Simplified Facial Feature Dictionaries\n",
    "# -------------------------------------------------\n",
    "FACIAL_FEATURES = {\n",
    "    \"left_eye\": [33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
    "                 173, 157, 158, 470, 160, 161, 246],\n",
    "    \"right_eye\": [263, 249, 390, 373, 374, 380, 381, 382,\n",
    "                  362, 398, 384, 385, 475, 387, 388, 466],\n",
    "    \"left_eyebrow\": [70, 63, 105, 66, 107, 55, 65, 52, 53, 70],\n",
    "    \"right_eyebrow\": [336, 296, 334, 293, 300, 276, 283, 282, 295, 285, 336],\n",
    "    \"outer_lips\": [61, 146, 91, 181, 84, 17, 314, 405,\n",
    "                   321, 375, 291, 308, 324, 318, 402,\n",
    "                   317, 14, 87, 178, 88, 95, 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308],\n",
    "    \"inner_lips\": [78, 95, 88, 178, 87, 14, 317, 402,\n",
    "                   318, 324, 308, 291, 375, 321, 405, 314, 17, 84],\n",
    "    \"chin\": [58, 138, 172, 136, 150, 149, 176, 148, 152, 377, 400, 378, 379],\n",
    "    \"nose_left\": [122, 196, 236, 198, 209, 49, 48, 64, 98],\n",
    "    \"nose_right\": [420, 360, 278, 294, 327, 326],\n",
    "    \"nostrils_left\": [98, 240, 75, 60, 99, 97],\n",
    "    \"nostrils_right\": [370, 354, 458, 290],\n",
    "    \"nose_center\": [238, 241, 242, 370, 354, 94],\n",
    "    \"nose_to_lips_left\": [203, 206, 216, 57],\n",
    "    \"nose_to_lips_right\": [423, 426, 436, 287],\n",
    "    \"left_eye_center\": [468],\n",
    "    \"right_eye_center\": [473],\n",
    "}\n",
    "\n",
    "EYEBALLS = {\n",
    "    \"left_iris\": [468, 469, 470, 471],\n",
    "    \"right_iris\": [473, 474, 475, 476],\n",
    "}\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Drawing helpers\n",
    "# -------------------------------------------------\n",
    "def smooth_points(points, sigma=5):\n",
    "    points = np.array(points, dtype=np.float32)\n",
    "    x_smooth = scipy.ndimage.gaussian_filter1d(points[:, 0], sigma=sigma)\n",
    "    y_smooth = scipy.ndimage.gaussian_filter1d(points[:, 1], sigma=sigma)\n",
    "    return list(zip(x_smooth.astype(int), y_smooth.astype(int)))\n",
    "\n",
    "def draw_feature(image, landmarks, indices, color=(255, 255, 255), thickness=2, smooth=True):\n",
    "    points = [(int(landmarks[i].x * image.shape[1]),\n",
    "               int(landmarks[i].y * image.shape[0])) for i in indices]\n",
    "    if smooth and len(points) >= 3:\n",
    "        points = smooth_points(points, sigma=0.1)\n",
    "    for i in range(len(points) - 1):\n",
    "        cv2.line(image, points[i], points[i + 1], color, thickness)\n",
    "\n",
    "def draw_eyeball_clipped(image, landmarks, iris_indices, top_eyelid_index, bottom_eyelid_index, color=(255, 255, 255)):\n",
    "    H, W = image.shape[:2]\n",
    "    pts = np.array([[landmarks[i].x * W, landmarks[i].y * H] for i in iris_indices])\n",
    "    center = np.mean(pts, axis=0)\n",
    "    radius = np.mean(np.linalg.norm(pts - center, axis=1))\n",
    "    upper_y = landmarks[top_eyelid_index].y * H\n",
    "    bottom_y = landmarks[bottom_eyelid_index].y * H\n",
    "    num_points = 200\n",
    "    for theta in np.linspace(0, 2*np.pi, num_points):\n",
    "        x = int(center[0] + radius * np.cos(theta))\n",
    "        y = int(center[1] + radius * np.sin(theta))\n",
    "        if upper_y <= y <= bottom_y:\n",
    "            cv2.circle(image, (x, y), 1, color, 1)\n",
    "    pupil_radius = int(radius * 0.3)\n",
    "    for y in range(int(center[1]-pupil_radius), int(center[1]+pupil_radius)):\n",
    "        for x in range(int(center[0]-pupil_radius), int(center[0]+pupil_radius)):\n",
    "            if (x-center[0])**2 + (y-center[1])**2 <= pupil_radius**2 and upper_y <= y <= bottom_y:\n",
    "                image[int(y), int(x)] = color\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Main selective line drawing\n",
    "# -------------------------------------------------\n",
    "def image_to_minimal_line_drawing_simplified(image_path, output_path):\n",
    "    print(f\"Processing: {image_path}\")\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(\"Error: Image not found.\")\n",
    "        return\n",
    "    H, W, _ = img.shape\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    final_line_drawing = np.ones((H, W), dtype=np.uint8) * 255\n",
    "\n",
    "    # --- FaceMesh landmarks ---\n",
    "    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5) as face_mesh:\n",
    "        results_face = face_mesh.process(img_rgb)\n",
    "        if results_face.multi_face_landmarks:\n",
    "            face_landmarks = results_face.multi_face_landmarks[0]\n",
    "            for name, indices in FACIAL_FEATURES.items():\n",
    "                if \"center\" not in name:\n",
    "                    draw_feature(final_line_drawing, face_landmarks.landmark, indices, color=0, thickness=2)\n",
    "\n",
    "            # Eyeballs\n",
    "            for iris_name, iris_indices in EYEBALLS.items():\n",
    "                if \"left\" in iris_name:\n",
    "                    draw_eyeball_clipped(final_line_drawing, face_landmarks.landmark, iris_indices, 160, 145, color=0)\n",
    "                else:\n",
    "                    draw_eyeball_clipped(final_line_drawing, face_landmarks.landmark, iris_indices, 385, 374, color=0)\n",
    "\n",
    "    # --- Hair/body outline ---\n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as segmentor:\n",
    "        results_seg = segmentor.process(img_rgb)\n",
    "        if results_seg.segmentation_mask is not None:\n",
    "            person_mask = (results_seg.segmentation_mask > 0.5).astype(np.uint8) * 255\n",
    "            blurred = cv2.GaussianBlur(person_mask, (61, 61), 0)\n",
    "            edges = cv2.Canny(blurred, 10, 50)\n",
    "            edges = cv2.dilate(edges, np.ones((5, 5), np.uint8), 1)\n",
    "            inverted = cv2.bitwise_not(edges)\n",
    "            final_line_drawing = np.minimum(final_line_drawing, inverted)\n",
    "\n",
    "    # --- Optional dedicated hair model ---\n",
    "    if HAIR_MODEL_PATH:\n",
    "        try:\n",
    "            base_options = python.BaseOptions(model_asset_path=HAIR_MODEL_PATH)\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_rgb)\n",
    "            options = vision.ImageSegmenterOptions(\n",
    "                base_options=base_options,\n",
    "                running_mode=vision.RunningMode.IMAGE,\n",
    "                output_category_mask=True\n",
    "            )\n",
    "            with vision.ImageSegmenter.create_from_options(options) as segmenter:\n",
    "                segmentation_result = segmenter.segment(mp_image)\n",
    "                category_mask = segmentation_result.category_mask.numpy_view()\n",
    "                hair_mask = np.where(category_mask == 1, 255, 0).astype(np.uint8)\n",
    "                hair_mask = cv2.resize(hair_mask, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "                blurred_hair = cv2.GaussianBlur(hair_mask, (41, 41), 0)\n",
    "                edges = cv2.Canny(blurred_hair, 10, 50)\n",
    "                edges = cv2.dilate(edges, np.ones((5, 5), np.uint8), 1)\n",
    "                inverted = cv2.bitwise_not(edges)\n",
    "                final_line_drawing = np.minimum(final_line_drawing, inverted)\n",
    "        except Exception as e:\n",
    "            print(f\"Hair model error: {e}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Continuous single-line simplification\n",
    "    # ---------------------------\n",
    "    binary = cv2.threshold(final_line_drawing, 127, 255, cv2.THRESH_BINARY_INV)[1]\n",
    "\n",
    "    # Merge nearby features\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n",
    "    merged = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel, iterations=3)\n",
    "    merged = cv2.morphologyEx(merged, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "\n",
    "    # Find all contours\n",
    "    contours, _ = cv2.findContours(merged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    if not contours:\n",
    "        print(\"No contours found.\")\n",
    "        cv2.imwrite(output_path, final_line_drawing)\n",
    "        return\n",
    "\n",
    "    # Pick the largest contour (main face outline)\n",
    "    contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Optional: merge all contours into one (if you want a full 'one-line' composite)\n",
    "    # contour = np.vstack(contours)\n",
    "\n",
    "    # Combine all contours to preserve internal details\n",
    "    contour = np.vstack(contours)\n",
    "\n",
    "    # Resample adaptively\n",
    "    num_points = max(4000, len(contour) // 3)\n",
    "    contour = contour[np.linspace(0, len(contour)-1, num=num_points, dtype=int)]\n",
    "\n",
    "    # Fine simplification\n",
    "    epsilon = 0.0008 * cv2.arcLength(contour.reshape(-1,1,2), True)\n",
    "    contour = cv2.approxPolyDP(contour.reshape(-1,1,2), epsilon, True)\n",
    "\n",
    "    # Light smoothing\n",
    "    from scipy.ndimage import gaussian_filter1d\n",
    "    sigma = 0.8\n",
    "    contour = contour.squeeze()\n",
    "    contour[:,0] = gaussian_filter1d(contour[:,0].astype(float), sigma=sigma)\n",
    "    contour[:,1] = gaussian_filter1d(contour[:,1].astype(float), sigma=sigma)\n",
    "    contour = contour.astype(int).reshape(-1,1,2)\n",
    "\n",
    "    # Draw the smooth single line and save output\n",
    "    single_line = np.ones_like(final_line_drawing) * 255\n",
    "    cv2.polylines(single_line, [contour], isClosed=False, color=0, thickness=2)\n",
    "\n",
    "    cv2.imwrite(output_path, single_line)\n",
    "    print(f\"✅ Saved continuous single-line drawing to: {output_path}\")\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    image_to_minimal_line_drawing_simplified('divija2.png', 'divija_line.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368f3861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760856941.733401 48421607 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760856941.734935 48456174 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760856941.739918 48456180 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize FaceMesh model (refine_landmarks=True gives iris points)\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    refine_landmarks=True,\n",
    "    max_num_faces=1,\n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread('divija2.png')\n",
    "rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Process image to get landmarks\n",
    "results = face_mesh.process(rgb_image)\n",
    "\n",
    "# ---------------------------\n",
    "# FACIAL FEATURE DICTIONARIES\n",
    "# ---------------------------\n",
    "\n",
    "FACIAL_FEATURES = {\n",
    "    # Eyes (outline of left and right eyes)\n",
    "    \"left_eye\": [33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
    "                 173, 157, 158, 470, 160, 161, 246],\n",
    "    \"right_eye\": [263, 249, 390, 373, 374, 380, 381, 382,\n",
    "                  362, 398, 384, 385, 475, 387, 388, 466],\n",
    "\n",
    "    # Eyebrows\n",
    "    \"left_eyebrow\": [70, 63, 105, 66, 107, 55, 65, 52, 53, 70],\n",
    "    \"right_eyebrow\": [336, 296, 334, 293, 300, 276, 283, 282],\n",
    "    \"top_eye\" : [414, 286, 258, 257, 259, 260, 467],  # upper eye contour\n",
    "    \"bottom_eye\" : [453, 452, 451, 450, 449, 448],  # lower eye contour\n",
    "    \"top_eye_left\": [190, 56, 28, 27, 29, 30, 247],\n",
    "    \"bottom_eye_left\": [228, 229, 230, 231, 232, 233],\n",
    "    # # Nose bridge and tip\n",
    "    # \"nose\": [1, 2, 98, 327, 168, 197, 195, 5, 4, 45],\n",
    "    # #\"nose_bridge\": [6, 197, 195, 5, 4],\n",
    "    # \"nose_tip\": [1, 2, 98, 327],\n",
    "    # Nose\n",
    "    # \"nose_bridge\": [6, 197, 195, 5, 4],\n",
    "    # \"nose_outline_left\": [98, 97, 2, 326],    # outer curve from bridge to nostril\n",
    "    # \"nose_outline_right\": [327, 326, 2, 97],  # mirror outline\n",
    "    # \"nostril_left\": [49, 50, 101, 118, 117],\n",
    "    # \"nostril_right\": [279, 278, 330, 347, 348],\n",
    "    # \"nose_bottom\": [97, 2, 326],  # base of nose\n",
    "\n",
    "    # Lips\n",
    "    \"outer_lips\": [61, 146, 91, 181, 84, 17, 314, 405,\n",
    "                   321, 375, 291, 308, 324, 318, 402,\n",
    "                   317, 14, 87, 178, 88, 95, 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308],\n",
    "\n",
    "    \"inner_lips\": [78, 95, 88, 178, 87, 14, 317, 402,\n",
    "                   318, 324, 308, 291, 375, 321, 405, 314, 17, 84],\n",
    "\n",
    "    # Upper lip ridge (natural lip contour)\n",
    "    \"upper_lip\": [61, 40, 37, 0, 267, 270, 409, 291],\n",
    "\n",
    "    # Optional lower lip (for extra definition)\n",
    "    \"lower_lip\": [61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291],\n",
    "\n",
    "    # Chin outline (jawline)\n",
    "    \"chin\": [58, 138, 172, 136, 150, 149, 176, 148, 152, 377, 400, 378, 379],\n",
    "\n",
    "    # # Ears\n",
    "    # \"left_ear\": [234, 93, 132, 58, 172, 136],\n",
    "    # \"right_ear\": [454, 323, 361, 288, 397, 365],\n",
    "\n",
    "    # # Line between nose and lips (philtrum)\n",
    "    \"nose_to_lips_left\": [203, 206, 216, 57],\n",
    "    \"nose_to_lips_right\": [423, 426, 436, 287],\n",
    "    \n",
    "\n",
    "    # Mid-eye centers (for eyeball placement)\n",
    "    \"left_eye_center\": [468],  # available if using iris model\n",
    "    \"right_eye_center\": [473],\n",
    "}\n",
    "\n",
    "# Iris (eyeball) indices (only exist when refine_landmarks=True)\n",
    "EYEBALLS = {\n",
    "    \"left_iris\": [468, 469, 470, 471],\n",
    "    \"right_iris\": [473, 474, 475, 476],\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# DRAWING HELPERS\n",
    "# ---------------------------\n",
    "\n",
    "def draw_feature(image, landmarks, indices, color=(255, 255, 255), thickness=1):\n",
    "    \"\"\"Draw line features (eyes, lips, etc.)\"\"\"\n",
    "    points = [(int(landmarks[i].x * image.shape[1]),\n",
    "               int(landmarks[i].y * image.shape[0])) for i in indices]\n",
    "    for i in range(len(points) - 1):\n",
    "        cv2.line(image, points[i], points[i + 1], color, thickness)\n",
    "\n",
    "\n",
    "def draw_eyeball_clipped(image, landmarks, iris_indices, top_eyelid_indices, bottom_eyelid_indices, color=(255, 255, 255)):\n",
    "    \"\"\"\n",
    "    Draw eyeball circle clipped at eyelids.\n",
    "    - iris_indices: points of iris\n",
    "    - top_eyelid_indices: upper eyelid contour\n",
    "    - bottom_eyelid_indices: lower eyelid contour\n",
    "    \"\"\"\n",
    "    # Iris points\n",
    "    pts = np.array([[landmarks[i].x * image.shape[1], landmarks[i].y * image.shape[0]] \n",
    "                    for i in iris_indices])\n",
    "    center = np.mean(pts, axis=0)\n",
    "    radius = np.mean(np.linalg.norm(pts - center, axis=1))\n",
    "\n",
    "    # Eyelid bounds\n",
    "    top_pts = np.array([[landmarks[i].x * image.shape[1], landmarks[i].y * image.shape[0]] \n",
    "                        for i in top_eyelid_indices])\n",
    "    bottom_pts = np.array([[landmarks[i].x * image.shape[1], landmarks[i].y * image.shape[0]] \n",
    "                           for i in bottom_eyelid_indices])\n",
    "    y_top = np.min(top_pts[:, 1])\n",
    "    y_bottom = np.max(bottom_pts[:, 1])\n",
    "\n",
    "    # Draw circle as points only within eyelid bounds\n",
    "    num_points = 100\n",
    "    for theta in np.linspace(0, 2 * np.pi, num_points):\n",
    "        x = int(center[0] + radius * np.cos(theta))\n",
    "        y = int(center[1] + radius * np.sin(theta))\n",
    "        if y_top <= y <= y_bottom:\n",
    "            cv2.circle(image, (x, y), 1, color, 1)  # small point to draw outline\n",
    "\n",
    "    # Draw pupil at iris center\n",
    "    pupil_radius = int(radius * 0.3)\n",
    "    pupil_top = max(int(center[1] - pupil_radius), int(y_top))\n",
    "    pupil_bottom = min(int(center[1] + pupil_radius), int(y_bottom))\n",
    "    for y in range(pupil_top, pupil_bottom):\n",
    "        for x in range(int(center[0] - pupil_radius), int(center[0] + pupil_radius)):\n",
    "            if (x - center[0])**2 + (y - center[1])**2 <= pupil_radius**2:\n",
    "                image[y, x] = color\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN LOGIC\n",
    "# ---------------------------\n",
    "\n",
    "if results.multi_face_landmarks:\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        # Draw facial line features\n",
    "        for name, indices in FACIAL_FEATURES.items():\n",
    "            draw_feature(image, face_landmarks.landmark, indices)\n",
    "\n",
    "        for iris_name, iris_indices in EYEBALLS.items():\n",
    "            if \"left\" in iris_name:\n",
    "                draw_eyeball_clipped(image, face_landmarks.landmark, iris_indices,\n",
    "                             FACIAL_FEATURES[\"top_eye_left\"], FACIAL_FEATURES[\"bottom_eye_left\"])\n",
    "            else:\n",
    "                draw_eyeball_clipped(image, face_landmarks.landmark, iris_indices,\n",
    "                             FACIAL_FEATURES[\"top_eye\"], FACIAL_FEATURES[\"bottom_eye\"])\n",
    "\n",
    "# Show output\n",
    "cv2.imwrite(\"face.png\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4a1db41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: niru.png\n",
      "Saved simplified line drawing to: niru_line.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762136791.370415 61411922 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1762136791.371117 61442778 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1762136791.374714 61442779 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1762136791.382858 61411922 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1762136791.383670 61442790 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1762136791.400570 61411922 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1762136791.401784 61442800 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# very very good code\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import scipy.ndimage\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "HAIR_MODEL_PATH = '/Users/divija/Divi Drive/workplace/Princeton/Sem 5/Carlab/carlab_2025/final_project/hair_segmenter.tflite'\n",
    "\n",
    "# ---------------------------\n",
    "# Simplified Facial Feature Dictionaries\n",
    "# ---------------------------\n",
    "\n",
    "FACIAL_FEATURES = {\n",
    "    # Eyes (outline of left and right eyes)\n",
    "    \"left_eye\": [33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
    "                 173, 157, 158, 470, 160, 161, 246],\n",
    "    \"right_eye\": [263, 249, 390, 373, 374, 380, 381, 382,\n",
    "                  362, 398, 384, 385, 475, 387, 388, 466],\n",
    "\n",
    "    # Eyebrows\n",
    "    \"left_eyebrow\": [70, 63, 105, 66, 107, 55, 65, 52, 53, 70],\n",
    "    \"right_eyebrow\": [336, 296, 334, 293, 300, 276, 283, 282, 295, 285, 336],\n",
    "\n",
    "    # Upper / lower eyelid contours for clipped eyeballs\n",
    "    \"top_eye\": [414, 286, 258, 257, 259, 260, 467],\n",
    "    \"bottom_eye\": [453, 452, 451, 450, 449, 448],\n",
    "    \"top_eye_left\": [190, 56, 28, 27, 29, 30, 247],\n",
    "    \"bottom_eye_left\": [228, 229, 230, 231, 232, 233],\n",
    "\n",
    "    # Lips\n",
    "    \"outer_lips\": [61, 146, 91, 181, 84, 17, 314, 405,\n",
    "                   321, 375, 291, 308, 324, 318, 402,\n",
    "                   317, 14, 87, 178, 88, 95, 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308],\n",
    "    \"inner_lips\": [78, 95, 88, 178, 87, 14, 317, 402,\n",
    "                   318, 324, 308, 291, 375, 321, 405, 314, 17, 84],\n",
    "    \"upper_lip\": [61, 40, 37, 0, 267, 270, 409, 291],\n",
    "    \"lower_lip\": [61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291],\n",
    "\n",
    "    # Chin / jawline\n",
    "    \"chin\": [58, 138, 172, 136, 150, 149, 176, 148, 152, 377, 400, 378, 379],\n",
    "    # actual nose:\n",
    "    \"nose_left\": [122, 196, 236, 198, 209, 49, 48, 64, 98],\n",
    "    \"nose_right\": [420, 360, 278, 294, 327, 326],\n",
    "    \"nostrils_left\" : [98, 240, 75, 60, 99, 97],\n",
    "    \"nostrils_right\" : [370, 354, 458, 290],\n",
    "    \"nose_center\": [238, 241,242,370, 354, 94],\n",
    "    # Nose to lips (philtrum)\n",
    "    \"nose_to_lips_left\": [203, 206, 216, 57],\n",
    "    \"nose_to_lips_right\": [423, 426, 436, 287],\n",
    "\n",
    "    # Mid-eye centers for eyeball placement\n",
    "    \"left_eye_center\": [468],\n",
    "    \"right_eye_center\": [473],\n",
    "}   \n",
    "\n",
    "# Iris (eyeball) indices\n",
    "EYEBALLS = {\n",
    "    \"left_iris\": [468, 469, 470, 471],\n",
    "    \"right_iris\": [473, 474, 475, 476],\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# Drawing helpers\n",
    "# ---------------------------\n",
    "\n",
    "def smooth_points(points, sigma=5):\n",
    "    \"\"\"\n",
    "    Smooth a list of points using Gaussian smoothing independently on x and y.\n",
    "    points: list of (x, y)\n",
    "    sigma: Gaussian kernel standard deviation\n",
    "    \"\"\"\n",
    "    \n",
    "    points = np.array(points, dtype=np.float32)\n",
    "    x_smooth = scipy.ndimage.gaussian_filter1d(points[:, 0], sigma=sigma)\n",
    "    y_smooth = scipy.ndimage.gaussian_filter1d(points[:, 1], sigma=sigma)\n",
    "    return list(zip(x_smooth.astype(int), y_smooth.astype(int)))\n",
    "def draw_feature(image, landmarks, indices, color=(255, 255, 255), thickness=2, smooth=True):\n",
    "    points = [(int(landmarks[i].x * image.shape[1]),\n",
    "               int(landmarks[i].y * image.shape[0])) for i in indices]\n",
    "    if smooth and len(points) >= 3:\n",
    "        points = smooth_points(points, sigma=0.1)\n",
    "    for i in range(len(points) - 1):\n",
    "        cv2.line(image, points[i], points[i + 1], color, thickness)\n",
    "\n",
    "def draw_eyeball_clipped(image, landmarks, iris_indices, top_eyelid_index, bottom_eyelid_index, color=(255, 255, 255)):\n",
    "    \"\"\"\n",
    "    Draw an eyeball circle clipped at the top eyelid landmark.\n",
    "    - iris_indices: list of iris landmarks\n",
    "    - top_eyelid_index: single landmark index for upper eyelid to clip at\n",
    "    - bottom_eyelid_indices: list of lower eyelid landmarks\n",
    "    \"\"\"\n",
    "    H, W = image.shape[:2]\n",
    "    \n",
    "    # Iris points\n",
    "    pts = np.array([[landmarks[i].x * W, landmarks[i].y * H] for i in iris_indices])\n",
    "    center = np.mean(pts, axis=0)\n",
    "    radius = np.mean(np.linalg.norm(pts - center, axis=1))\n",
    "    \n",
    "    # Upper eyelid landmark coordinate\n",
    "    upper_y = landmarks[top_eyelid_index].y * H\n",
    "\n",
    "    # Bottom eyelid bounds\n",
    "    bottom_y = np.array([landmarks[bottom_eyelid_index].y * H])\n",
    "    y_bottom = np.max(bottom_y)\n",
    "    \n",
    "    # Draw circle outline, skip points above upper eyelid landmark\n",
    "    num_points = 200\n",
    "    for theta in np.linspace(0, 2*np.pi, num_points):\n",
    "        x = int(center[0] + radius * np.cos(theta))\n",
    "        y = int(center[1] + radius * np.sin(theta))\n",
    "        if upper_y <= y <= y_bottom:\n",
    "            cv2.circle(image, (x, y), 1, color, 1)\n",
    "    \n",
    "    # Draw pupil clipped at the same bounds\n",
    "    pupil_radius = int(radius * 0.3)\n",
    "    pupil_top = max(int(center[1] - pupil_radius), int(upper_y))\n",
    "    pupil_bottom = min(int(center[1] + pupil_radius), int(y_bottom))\n",
    "    \n",
    "    for y in range(pupil_top, pupil_bottom):\n",
    "        for x in range(int(center[0] - pupil_radius), int(center[0] + pupil_radius)):\n",
    "            if (x - center[0])**2 + (y - center[1])**2 <= pupil_radius**2:\n",
    "                image[y, x] = color\n",
    "# ---------------------------\n",
    "# Main selective line drawing\n",
    "# ---------------------------\n",
    "\n",
    "def image_to_minimal_line_drawing_simplified(image_path, output_path):\n",
    "    print(f\"Processing: {image_path}\")\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(\"Error: Image not found.\")\n",
    "        return\n",
    "    H, W, _ = img.shape\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    final_line_drawing = np.ones((H, W), dtype=np.uint8) * 255\n",
    "\n",
    "    # FaceMesh\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True,\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        results_face = face_mesh.process(img_rgb)\n",
    "        if results_face.multi_face_landmarks:\n",
    "            face_landmarks = results_face.multi_face_landmarks[0]\n",
    "\n",
    "    # Draw simplified facial features on final_line_drawing\n",
    "    for name, indices in FACIAL_FEATURES.items():\n",
    "        if \"left_eye_center\" in name or \"right_eye_center\" in name:\n",
    "            continue\n",
    "        draw_feature(final_line_drawing, face_landmarks.landmark, indices, color=0, thickness=2)\n",
    "\n",
    "    # Draw clipped eyeballs on final_line_drawing\n",
    "    for iris_name, iris_indices in EYEBALLS.items():\n",
    "        if \"left\" in iris_name:\n",
    "            draw_eyeball_clipped(final_line_drawing, face_landmarks.landmark, iris_indices,\n",
    "                                160, 145, color=0)\n",
    "        else:\n",
    "            draw_eyeball_clipped(final_line_drawing, face_landmarks.landmark, iris_indices,\n",
    "                                385, 374, color=0)\n",
    "\n",
    "    # Selfie Segmentation for hair/body outline\n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as segmentor:\n",
    "        results_segmentation = segmentor.process(img_rgb)\n",
    "        if results_segmentation.segmentation_mask is not None:\n",
    "            person_mask = (results_segmentation.segmentation_mask > 0.5).astype(np.uint8) * 255\n",
    "            blurred_outline = cv2.GaussianBlur(person_mask, (61,61), 0)\n",
    "            edges = cv2.Canny(blurred_outline, 10, 50)\n",
    "            edges = cv2.dilate(edges, np.ones((5,5),np.uint8), iterations=1)\n",
    "            inverted = cv2.bitwise_not(edges)\n",
    "            final_line_drawing = np.minimum(final_line_drawing, inverted)\n",
    "\n",
    "    # Optional dedicated hair model\n",
    "    if HAIR_MODEL_PATH:\n",
    "        try:\n",
    "            base_options = python.BaseOptions(model_asset_path=HAIR_MODEL_PATH, delegate=None)\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_rgb)\n",
    "            options = vision.ImageSegmenterOptions(\n",
    "                base_options=base_options,\n",
    "                running_mode=vision.RunningMode.IMAGE,\n",
    "                output_category_mask=True\n",
    "            )\n",
    "            with vision.ImageSegmenter.create_from_options(options) as segmenter:\n",
    "                segmentation_result = segmenter.segment(mp_image)\n",
    "                category_mask = segmentation_result.category_mask.numpy_view()\n",
    "                hair_mask = np.where(category_mask == 1, 255, 0).astype(np.uint8)\n",
    "                hair_mask = cv2.resize(hair_mask, (W,H), interpolation=cv2.INTER_NEAREST)\n",
    "                blurred_hair = cv2.GaussianBlur(hair_mask, (41,41),0)\n",
    "                edges = cv2.Canny(blurred_hair,10,50)\n",
    "                edges = cv2.dilate(edges,np.ones((5,5),np.uint8),iterations=1)\n",
    "                inverted = cv2.bitwise_not(edges)\n",
    "                final_line_drawing = np.minimum(final_line_drawing, inverted)\n",
    "        except Exception as e:\n",
    "            print(f\"Hair model error: {e}\")\n",
    "        \n",
    "         # Save output\n",
    "        # ---------------------------\n",
    "    # Line simplification step\n",
    "    # ---------------------------\n",
    "    # Convert to binary (0/255)\n",
    "    binary = cv2.threshold(final_line_drawing, 127, 255, cv2.THRESH_BINARY_INV)[1]\n",
    "\n",
    "    # Morphological closing to merge nearby lines\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "    merged = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "    # Distance-based thinning (skeletonization)\n",
    "    dist = cv2.distanceTransform(merged, cv2.DIST_L2, 5)\n",
    "    skeleton = np.zeros_like(merged)\n",
    "    skeleton[dist > 1] = 255   # threshold controls how aggressively to keep single lines\n",
    "\n",
    "    # # Optional contour simplification (reduces jittery edges)\n",
    "    # contours, _ = cv2.findContours(skeleton, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # simplified = np.ones_like(final_line_drawing) * 255\n",
    "    # for cnt in contours:\n",
    "    #     epsilon = 0.001 * cv2.arcLength(cnt, True)\n",
    "    #     approx = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "    #     cv2.drawContours(simplified, [approx], -1, 0, 1)\n",
    "\n",
    "    # final_line_drawing = simplified\n",
    "\n",
    "    cv2.imwrite(output_path, final_line_drawing)\n",
    "    print(f\"Saved simplified line drawing to: {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "image_to_minimal_line_drawing_simplified('niru.png', 'niru_line.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5088887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved drawing animation: csv_draw_anim.mp4\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def csv_to_video(\n",
    "    csv_path,\n",
    "    out_video=\"csv_draw_anim.mp4\",\n",
    "    frame_rate=60,\n",
    "    canvas_size=(512,512),\n",
    "    line_thickness=1,\n",
    "    speed=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize draw_coords.csv as a video showing pen-down and pen-up moves.\n",
    "    - speed: how many CSV points per frame (higher = faster animation)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Load coordinates ---\n",
    "    coords = []\n",
    "    with open(csv_path, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            coords.append((int(row[\"x\"]), int(row[\"y\"]), int(row[\"pen\"])))\n",
    "\n",
    "    if not coords:\n",
    "        raise ValueError(\"CSV is empty or invalid\")\n",
    "\n",
    "    # --- Normalize / scale coordinates to fit canvas ---\n",
    "    xs, ys = [p[0] for p in coords], [p[1] for p in coords]\n",
    "    minx, maxx, miny, maxy = min(xs), max(xs), min(ys), max(ys)\n",
    "    W, H = canvas_size\n",
    "    scale = min(W / (maxx - minx + 1e-5), H / (maxy - miny + 1e-5))\n",
    "    offset_x = -minx\n",
    "    offset_y = -miny\n",
    "\n",
    "    def transform(pt):\n",
    "        x = int((pt[0] + offset_x) * scale)\n",
    "        y = int((pt[1] + offset_y) * scale)\n",
    "        return x, y\n",
    "\n",
    "    # --- Setup video ---\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    vid = cv2.VideoWriter(out_video, fourcc, frame_rate, (W, H))\n",
    "    canvas = np.ones((H, W, 3), np.uint8) * 255\n",
    "    pen_color = (0, 0, 0)\n",
    "    travel_color = (180, 180, 180)\n",
    "    pen_tip_color = (0, 0, 255)\n",
    "\n",
    "    # --- Draw progressively ---\n",
    "    prev_pt = None\n",
    "    for i in range(0, len(coords), speed):\n",
    "        x, y, pen = coords[i]\n",
    "        pt = transform((x, y))\n",
    "\n",
    "        # Draw\n",
    "        if pen == 1 and prev_pt is not None:\n",
    "            cv2.line(canvas, prev_pt, pt, pen_color, line_thickness)\n",
    "        elif pen == 0 and prev_pt is not None:\n",
    "            cv2.line(canvas, prev_pt, pt, travel_color, 1)\n",
    "\n",
    "        # Draw pen tip\n",
    "        frame = canvas.copy()\n",
    "        cv2.circle(frame, pt, 2, pen_tip_color, -1)\n",
    "        vid.write(frame)\n",
    "\n",
    "        prev_pt = pt\n",
    "\n",
    "    vid.release()\n",
    "    print(f\"✅ Saved drawing animation: {out_video}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    csv_to_video(\"draw_coords.csv\", \"csv_draw_anim.mp4\", speed=3, canvas_size=(800,800))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf0a468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 245 raw contours\n",
      "Retained 91 simplified paths\n",
      "Path 0: 6 points\n",
      "Path 1: 3 points\n",
      "Path 2: 4 points\n",
      "Path 3: 3 points\n",
      "Path 4: 4 points\n",
      "Path 5: 5 points\n",
      "Path 6: 3 points\n",
      "Path 7: 3 points\n",
      "Path 8: 5 points\n",
      "Path 9: 3 points\n",
      "Path 10: 10 points\n",
      "Path 11: 4 points\n",
      "Path 12: 3 points\n",
      "Path 13: 3 points\n",
      "Path 14: 18 points\n",
      "Path 15: 5 points\n",
      "Path 16: 3 points\n",
      "Path 17: 3 points\n",
      "Path 18: 3 points\n",
      "Path 19: 3 points\n",
      "Path 20: 3 points\n",
      "Path 21: 14 points\n",
      "Path 22: 3 points\n",
      "Path 23: 3 points\n",
      "Path 24: 3 points\n",
      "Path 25: 3 points\n",
      "Path 26: 3 points\n",
      "Path 27: 3 points\n",
      "Path 28: 3 points\n",
      "Path 29: 3 points\n",
      "Path 30: 3 points\n",
      "Path 31: 3 points\n",
      "Path 32: 3 points\n",
      "Path 33: 3 points\n",
      "Path 34: 3 points\n",
      "Path 35: 3 points\n",
      "Path 36: 3 points\n",
      "Path 37: 3 points\n",
      "Path 38: 3 points\n",
      "Path 39: 3 points\n",
      "Path 40: 3 points\n",
      "Path 41: 3 points\n",
      "Path 42: 3 points\n",
      "Path 43: 3 points\n",
      "Path 44: 3 points\n",
      "Path 45: 3 points\n",
      "Path 46: 3 points\n",
      "Path 47: 3 points\n",
      "Path 48: 3 points\n",
      "Path 49: 3 points\n",
      "Path 50: 3 points\n",
      "Path 51: 3 points\n",
      "Path 52: 3 points\n",
      "Path 53: 3 points\n",
      "Path 54: 3 points\n",
      "Path 55: 3 points\n",
      "Path 56: 3 points\n",
      "Path 57: 4 points\n",
      "Path 58: 3 points\n",
      "Path 59: 3 points\n",
      "Path 60: 3 points\n",
      "Path 61: 3 points\n",
      "Path 62: 3 points\n",
      "Path 63: 3 points\n",
      "Path 64: 3 points\n",
      "Path 65: 3 points\n",
      "Path 66: 3 points\n",
      "Path 67: 3 points\n",
      "Path 68: 3 points\n",
      "Path 69: 3 points\n",
      "Path 70: 3 points\n",
      "Path 71: 3 points\n",
      "Path 72: 3 points\n",
      "Path 73: 3 points\n",
      "Path 74: 3 points\n",
      "Path 75: 3 points\n",
      "Path 76: 3 points\n",
      "Path 77: 3 points\n",
      "Path 78: 3 points\n",
      "Path 79: 3 points\n",
      "Path 80: 3 points\n",
      "Path 81: 3 points\n",
      "Path 82: 3 points\n",
      "Path 83: 3 points\n",
      "Path 84: 3 points\n",
      "Path 85: 3 points\n",
      "Path 86: 3 points\n",
      "Path 87: 5 points\n",
      "Path 88: 3 points\n",
      "Path 89: 3 points\n",
      "Path 90: 460 points\n",
      "Saved coordinates to drawing_path.csv\n",
      "Saved extracted path visualization as extracted_paths.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------------------------\n",
    "INPUT_PATH = \"divija_line.png\"\n",
    "OUTPUT_CSV = \"drawing_path.csv\"\n",
    "OUTPUT_VIS = \"extracted_paths.png\"\n",
    "\n",
    "# parameters\n",
    "THRESH = 200\n",
    "EPSILON = 1.5\n",
    "DOWNSAMPLE_STEP = 3\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Load and preprocess\n",
    "# -------------------------------------------------\n",
    "img = cv2.imread(INPUT_PATH, cv2.IMREAD_GRAYSCALE)\n",
    "if img is None:\n",
    "    raise FileNotFoundError(f\"Could not read image at {INPUT_PATH}\")\n",
    "\n",
    "_, binary = cv2.threshold(img, THRESH, 255, cv2.THRESH_BINARY_INV)\n",
    "binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, np.ones((3, 3), np.uint8))\n",
    "\n",
    "# thin lines to 1-pixel width\n",
    "skeleton = cv2.ximgproc.thinning(binary)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Extract contours (continuous strokes)\n",
    "# -------------------------------------------------\n",
    "contours, _ = cv2.findContours(skeleton, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "print(f\"Found {len(contours)} raw contours\")\n",
    "\n",
    "paths = []\n",
    "for cnt in contours:\n",
    "    approx = cv2.approxPolyDP(cnt, EPSILON, False)\n",
    "    pts = approx[:, 0, :]\n",
    "    if len(pts) > DOWNSAMPLE_STEP:\n",
    "        pts = pts[::DOWNSAMPLE_STEP]\n",
    "    if len(pts) > 2:\n",
    "        paths.append(pts)\n",
    "\n",
    "print(f\"Retained {len(paths)} simplified paths\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Print and save extracted paths\n",
    "# -------------------------------------------------\n",
    "for i, path in enumerate(paths):\n",
    "    print(f\"Path {i}: {len(path)} points\")\n",
    "\n",
    "# Save combined coordinates for downstream use\n",
    "all_coords = np.concatenate(paths, axis=0)\n",
    "np.savetxt(OUTPUT_CSV, all_coords, fmt=\"%.1f\", delimiter=\",\", header=\"x,y\")\n",
    "print(f\"Saved coordinates to {OUTPUT_CSV}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Visualize on blank canvas\n",
    "# -------------------------------------------------\n",
    "# Create a white canvas same size as input\n",
    "canvas = np.ones_like(img) * 255\n",
    "canvas_color = cv2.cvtColor(canvas, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "color_map = plt.cm.jet(np.linspace(0, 1, len(paths)))\n",
    "\n",
    "for i, pts in enumerate(paths):\n",
    "    color = tuple(int(c * 255) for c in color_map[i][:3])\n",
    "    for j in range(len(pts) - 1):\n",
    "        cv2.line(canvas_color, tuple(pts[j]), tuple(pts[j+1]), color, 1)\n",
    "\n",
    "cv2.imwrite(OUTPUT_VIS, canvas_color)\n",
    "print(f\"Saved extracted path visualization as {OUTPUT_VIS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e91744c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: woman.jpeg\n",
      "✅ Saved single-pixel line drawing to: woman_line.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762137891.345272 61411922 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1762137891.346666 61472432 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1762137891.350605 61472430 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1762137891.361280 61411922 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1762137891.362746 61472443 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1762137891.385742 61411922 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1762137891.387352 61472462 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# very very very good code oolala so sexy\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import scipy.ndimage\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Initialize MediaPipe\n",
    "# -------------------------------------------------\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "HAIR_MODEL_PATH = '/Users/divija/Divi Drive/workplace/Princeton/Sem 5/Carlab/carlab_2025/final_project/hair_segmenter.tflite'\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Facial feature landmark groups\n",
    "# -------------------------------------------------\n",
    "FACIAL_FEATURES = {\n",
    "    # Eyes (outline of left and right eyes)\n",
    "    \"left_eye\": [33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
    "                 173, 157, 158, 470, 160, 161, 246],\n",
    "    \"right_eye\": [263, 249, 390, 373, 374, 380, 381, 382,\n",
    "                  362, 398, 384, 385, 475, 387, 388, 466],\n",
    "\n",
    "    # Eyebrows\n",
    "    \"left_eyebrow\": [70, 63, 105, 66, 107, 55, 65, 52, 53, 70],\n",
    "    \"right_eyebrow\": [336, 296, 334, 293, 300, 276, 283, 282, 295, 285, 336],\n",
    "\n",
    "    # Upper / lower eyelid contours for clipped eyeballs\n",
    "    \"top_eye\": [414, 286, 258, 257, 259, 260, 467],\n",
    "    \"bottom_eye\": [453, 452, 451, 450, 449, 448],\n",
    "    \"top_eye_left\": [190, 56, 28, 27, 29, 30, 247],\n",
    "    \"bottom_eye_left\": [228, 229, 230, 231, 232, 233],\n",
    "\n",
    "    # Lips\n",
    "    \"outer_lips\": [61, 146, 91, 181, 84, 17, 314, 405,\n",
    "                   321, 375, 291, 308, 324, 318, 402,\n",
    "                   317, 14, 87, 178, 88, 95, 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308],\n",
    "    \"inner_lips\": [78, 95, 88, 178, 87, 14, 317, 402,\n",
    "                   318, 324, 308, 291, 375, 321, 405, 314, 17, 84],\n",
    "    \"upper_lip\": [61, 40, 37, 0, 267, 270, 409, 291],\n",
    "    \"lower_lip\": [61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291],\n",
    "\n",
    "    # Chin / jawline\n",
    "    \"chin\": [58, 138, 172, 136, 150, 149, 176, 148, 152, 377, 400, 378, 379],\n",
    "    # actual nose:\n",
    "    \"nose_left\": [122, 196, 236, 198, 209, 49, 48, 64, 98],\n",
    "    \"nose_right\": [420, 360, 278, 294, 327, 326],\n",
    "    \"nostrils_left\" : [98, 240, 75, 60, 99, 97],\n",
    "    \"nostrils_right\" : [370, 354, 458, 290],\n",
    "    \"nose_center\": [238, 241,242,370, 354, 94],\n",
    "    # Nose to lips (philtrum)\n",
    "    \"nose_to_lips_left\": [203, 206, 216, 57],\n",
    "    \"nose_to_lips_right\": [423, 426, 436, 287],\n",
    "\n",
    "    # Mid-eye centers for eyeball placement\n",
    "    \"left_eye_center\": [468],\n",
    "    \"right_eye_center\": [473],\n",
    "}   \n",
    "\n",
    "\n",
    "EYEBALLS = {\n",
    "    \"left_iris\": [468, 469, 470, 471],\n",
    "    \"right_iris\": [473, 474, 475, 476],\n",
    "}\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Helpers\n",
    "# -------------------------------------------------\n",
    "def draw_feature(image, landmarks, indices, color=0, thickness=1):\n",
    "    pts = [(int(landmarks[i].x * image.shape[1]),\n",
    "            int(landmarks[i].y * image.shape[0])) for i in indices]\n",
    "    for i in range(len(pts) - 1):\n",
    "        cv2.line(image, pts[i], pts[i + 1], color, thickness)\n",
    "\n",
    "def draw_eyeball_clipped(image, landmarks, iris_indices, top_eyelid_index, bottom_eyelid_index, color=0):\n",
    "    H, W = image.shape[:2]\n",
    "    pts = np.array([[landmarks[i].x * W, landmarks[i].y * H] for i in iris_indices])\n",
    "    center = np.mean(pts, axis=0)\n",
    "    radius = np.mean(np.linalg.norm(pts - center, axis=1))\n",
    "    upper_y = landmarks[top_eyelid_index].y * H\n",
    "    bottom_y = landmarks[bottom_eyelid_index].y * H\n",
    "    for theta in np.linspace(0, 2*np.pi, 200):\n",
    "        x = int(center[0] + radius * np.cos(theta))\n",
    "        y = int(center[1] + radius * np.sin(theta))\n",
    "        if upper_y <= y <= bottom_y and 0 <= y < H and 0 <= x < W:\n",
    "            image[y, x] = color\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Main\n",
    "# -------------------------------------------------\n",
    "def image_to_minimal_line_drawing_simplified(image_path, output_path):\n",
    "    print(f\"Processing: {image_path}\")\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(\"Error: Image not found.\")\n",
    "        return\n",
    "    H, W, _ = img.shape\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    final_line_drawing = np.ones((H, W), dtype=np.uint8) * 255\n",
    "\n",
    "    # ---- FaceMesh ----\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True, max_num_faces=1,\n",
    "        refine_landmarks=True, min_detection_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        res = face_mesh.process(img_rgb)\n",
    "        if not res.multi_face_landmarks:\n",
    "            print(\"No face found.\"); return\n",
    "        face_landmarks = res.multi_face_landmarks[0]\n",
    "\n",
    "    # ---- Facial features ----\n",
    "    for name, idxs in FACIAL_FEATURES.items():\n",
    "        if \"center\" in name: continue\n",
    "        draw_feature(final_line_drawing, face_landmarks.landmark, idxs)\n",
    "    for iris_name, iris_indices in EYEBALLS.items():\n",
    "        if \"left\" in iris_name:\n",
    "            draw_eyeball_clipped(final_line_drawing, face_landmarks.landmark, iris_indices, 160, 145)\n",
    "        else:\n",
    "            draw_eyeball_clipped(final_line_drawing, face_landmarks.landmark, iris_indices, 385, 374)\n",
    "\n",
    "    # ---- Hair segmentation ----\n",
    "    hair_outline = np.ones((H, W), dtype=np.uint8) * 255\n",
    "    hair_mask = None\n",
    "    try:\n",
    "        base_opts = python.BaseOptions(model_asset_path=HAIR_MODEL_PATH)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=img_rgb)\n",
    "        opts = vision.ImageSegmenterOptions(\n",
    "            base_options=base_opts,\n",
    "            running_mode=vision.RunningMode.IMAGE,\n",
    "            output_category_mask=True\n",
    "        )\n",
    "        with vision.ImageSegmenter.create_from_options(opts) as segmenter:\n",
    "            seg = segmenter.segment(mp_image)\n",
    "            cat_mask = seg.category_mask.numpy_view()\n",
    "            hair_mask = np.where(cat_mask == 1, 255, 0).astype(np.uint8)\n",
    "            hair_mask = cv2.resize(hair_mask, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "            cs, _ = cv2.findContours(hair_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "            for c in cs:\n",
    "                eps = 0.002 * cv2.arcLength(c, True)\n",
    "                approx = cv2.approxPolyDP(c, eps, True)\n",
    "                cv2.drawContours(hair_outline, [approx], -1, 0, 1)\n",
    "            hair_outline = cv2.ximgproc.thinning(255 - hair_outline)\n",
    "            hair_outline = 255 - hair_outline\n",
    "            final_line_drawing = np.minimum(final_line_drawing, hair_outline)\n",
    "    except Exception as e:\n",
    "        print(f\"Hair model error: {e}\")\n",
    "\n",
    "    # ---- Selfie segmentation ----\n",
    "    outline = np.ones((H, W), dtype=np.uint8) * 255\n",
    "    with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as seg:\n",
    "        res = seg.process(img_rgb)\n",
    "        if res.segmentation_mask is not None:\n",
    "            body_mask = (res.segmentation_mask > 0.5).astype(np.uint8) * 255\n",
    "            cs, _ = cv2.findContours(body_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "            for c in cs:\n",
    "                eps = 0.0015 * cv2.arcLength(c, True)\n",
    "                approx = cv2.approxPolyDP(c, eps, True)\n",
    "                cv2.drawContours(outline, [approx], -1, 0, 1)\n",
    "\n",
    "    # ---- Hair-priority merge ----\n",
    "    if hair_mask is not None:\n",
    "        hair_bin = cv2.threshold(hair_outline, 127, 255, cv2.THRESH_BINARY_INV)[1]\n",
    "        body_bin = cv2.threshold(outline, 127, 255, cv2.THRESH_BINARY_INV)[1]\n",
    "        dist_to_hair = cv2.distanceTransform(255 - hair_bin, cv2.DIST_L2, 5)\n",
    "        proximity_thresh = 50  # px; tune this\n",
    "        body_bin[dist_to_hair < proximity_thresh] = 0\n",
    "        outline = 255 - body_bin\n",
    "\n",
    "    # ---- Combine & finalize ----\n",
    "    final_line_drawing = np.minimum(final_line_drawing, outline)\n",
    "    binary = cv2.threshold(final_line_drawing, 127, 255, cv2.THRESH_BINARY_INV)[1]\n",
    "    skeleton = cv2.ximgproc.thinning(binary)\n",
    "    final_line_drawing = 255 - skeleton\n",
    "\n",
    "    cv2.imwrite(output_path, final_line_drawing)\n",
    "    print(f\"✅ Saved single-pixel line drawing to: {output_path}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Example usage\n",
    "# -------------------------------------------------\n",
    "image_to_minimal_line_drawing_simplified('woman.jpeg', 'woman_line.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1317c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------\n",
    "# Camera + face capture parameters\n",
    "# -------------------------------------------------\n",
    "CAPTURE_PATH = \"captured_face.png\"\n",
    "CENTER_TOLERANCE = 0.1  # 10% of frame width/height\n",
    "FACE_DETECT_CONF = 0.6\n",
    "\n",
    "mp_face = mp.solutions.face_detection\n",
    "\n",
    "def capture_centered_face(output_path=CAPTURE_PATH):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    detector = mp_face.FaceDetection(model_selection=0, min_detection_confidence=FACE_DETECT_CONF)\n",
    "    print(\"📷 Starting camera. Center your face to capture automatically...\")\n",
    "\n",
    "    captured = False\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Camera read failed.\")\n",
    "            break\n",
    "\n",
    "        H, W, _ = frame.shape\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = detector.process(rgb)\n",
    "\n",
    "        if results.detections:\n",
    "            for det in results.detections:\n",
    "                bbox = det.location_data.relative_bounding_box\n",
    "                cx = bbox.xmin + bbox.width / 2\n",
    "                cy = bbox.ymin + bbox.height / 2\n",
    "                dx = abs(cx - 0.5)\n",
    "                dy = abs(cy - 0.5)\n",
    "\n",
    "                # Visual guide\n",
    "                cv2.rectangle(frame, \n",
    "                              (int((0.5 - CENTER_TOLERANCE)*W), int((0.5 - CENTER_TOLERANCE)*H)),\n",
    "                              (int((0.5 + CENTER_TOLERANCE)*W), int((0.5 + CENTER_TOLERANCE)*H)),\n",
    "                              (0,255,0), 2)\n",
    "                cv2.putText(frame, \"Align your face in green box\", (30,40),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
    "\n",
    "                if dx < CENTER_TOLERANCE and dy < CENTER_TOLERANCE:\n",
    "                    cv2.putText(frame, \"✅ Face centered - capturing...\", (30,70),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
    "                    cv2.imwrite(output_path, frame)\n",
    "                    captured = True\n",
    "                    break\n",
    "\n",
    "        cv2.imshow(\"Align Your Face\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q') or captured:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    detector.close()\n",
    "\n",
    "    if captured:\n",
    "        print(f\"✅ Captured image saved to {output_path}\")\n",
    "        return output_path\n",
    "    else:\n",
    "        print(\"❌ No capture made.\")\n",
    "        return None\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Main combined flow\n",
    "# -------------------------------------------------\n",
    "def capture_and_generate():\n",
    "    captured_path = capture_centered_face()\n",
    "    if captured_path and os.path.exists(captured_path):\n",
    "        # Run the full pipeline on captured image\n",
    "        line_path = \"captured_line.png\"\n",
    "        csv_path = \"captured_coords.csv\"\n",
    "        video_path = \"captured_draw.mp4\"\n",
    "        image_to_minimal_line_drawing_simplified(captured_path, line_path, csv_path, video_path)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Run\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    capture_and_generate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carlab (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
